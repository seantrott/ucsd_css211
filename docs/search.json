[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "CSS 211 Schedule",
    "section": "",
    "text": "This is a tentative schedule of which topics I plan to cover. Note that while all lectures will have an interactive component, Fridays are reserved for more hands-on labs—typically involving a CSS-relevant dataset—with active participation encouraged.\nReadings are option; “ISLR” refers to “Introduction to Statistical Learning in R” (James et al.), which can be downloaded here.\n\n\n\nWeek\nDay\nDate\nTopic(s)\nAssignments Due\nSuggested readings\n\n\n\n\n0\nF\n09-26\nCourse Introduction\n\n\n\n\n1\nM\n09-30\nPhilosophy of Science in CSS\n\nIntroduction to R for Data Science\n\n\n1\nW\n10-01\nIntroduction to R and RStudio\n\n\n\n\n1\nF\n10-03\nR basics [HANDS-ON]\nLab 1\n\n\n\n2\nM\n10-06\nData wrangling\n\nTidy data (Wickham, 2011)\n\n\n2\nW\n10-08\nData wrangling\n\n\n\n\n2\nF\n10-10\nHands-on data cleaning [HANDS-ON]\nConcept quiz\n\n\n\n3\nM\n10-13\nData visualization (principles)\n\nIntroduction to Data Visualization in R\n\n\n3\nW\n10-15\nData visualization (ggplot)\n\n\n\n\n3\nF\n10-17\nExploratory data analysis [HANDS-ON]\nLab 2\n\n\n\n4\nM\n10-20\nLinear regression\n\nISLR, 3.1\n\n\n4\nW\n10-22\nLinear regression\n\n\n\n\n4\nF\n10-24\nHands-on regression [HANDS-ON]\nConcept quiz\n\n\n\n5\nM\n10-27\nMultiple regression\n\nISLR, 3.2\n\n\n5\nW\n10-29\nRegression: issues\n\n\n\n\n5\nF\n10-31\nBuilding complex models [HANDS-ON]\nLab 3\n\n\n\n6\nM\n11-03\nLogistic regression\n\nISLR, 4.3\n\n\n6\nW\n11-05\nLogistic regression\n\nISLR, 6.2\n\n\n6\nF\n11-07\nInterpreting logistic models [HANDS-ON]\nConcept quiz\n\n\n\n7\nM\n11-10\nMidterm review\n\n\n\n\n7\nW\n11-12\nMixed effects models (Pre-recorded lecture)\n\n\n\n\n7\nF\n11-14\nIn-class midterm\nMidterm!\n\n\n\n8\nM\n11-17\nFinal project check-in\n\nWinter, 2013\n\n\n8\nW\n11-19\nMixed effects models\nLab 4\n\n\n\n8\nF\n11-21\nMixed effects models\nConcept quiz\n\n\n\n9\nM\n11-24\nModel selection [HANDS-ON]\n\n\n\n\n9\nW\n11-26\nHOLIDAY (Thanksgiving)\n\n\n\n\n9\nF\n11-28\nHOLIDAY (Thanksgiving)\n\n\n\n\n10\nM\n12-01\nResampling, best practices, wrap-up\n\n\n\n\n10\nW\n12-03\nFinal project presentations (pt. 1)\n\n\n\n\n10\nF\n12-05\nFinal project presentations (pt. 2)\n\n\n\n\n11\nW\n12-10\n\nFinal report due"
  },
  {
    "objectID": "lectures/week6-logistic.html",
    "href": "lectures/week6-logistic.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Classification: dealing with categorical outcomes.\nLogistic regression.\n\nWhy not linear regression?\nGeneralized linear models (GLMs).\nOdds and log-odds.\nThe logistic function.\n\nBuilding and interpreting logistic models with glm."
  },
  {
    "objectID": "lectures/week6-logistic.html#goals-of-the-lecture",
    "href": "lectures/week6-logistic.html#goals-of-the-lecture",
    "title": "Logistic Regression",
    "section": "",
    "text": "Classification: dealing with categorical outcomes.\nLogistic regression.\n\nWhy not linear regression?\nGeneralized linear models (GLMs).\nOdds and log-odds.\nThe logistic function.\n\nBuilding and interpreting logistic models with glm."
  },
  {
    "objectID": "lectures/week6-logistic.html#what-is-classification",
    "href": "lectures/week6-logistic.html#what-is-classification",
    "title": "Logistic Regression",
    "section": "What is classification?",
    "text": "What is classification?\n\n“To classify is human…We sort dirty dishes from clean, white laundry from colorfast, important email to be answered from e-junk…. Any part of the home, school, or workplace reveals some such system of classification.”\n— Bowker & Star, 2000\n\n\n\nClassification = predicting a categorical response variable using features\nCommon examples:\n\nIs an email spam or not spam?\nIs a cell mass cancerous or not cancerous?\nWill this customer buy or not buy?\nIs a credit card transaction fraudulent?\nIs this image a cat, dog, person, or other?"
  },
  {
    "objectID": "lectures/week6-logistic.html#binary-vs.-multi-class-classification",
    "href": "lectures/week6-logistic.html#binary-vs.-multi-class-classification",
    "title": "Logistic Regression",
    "section": "Binary vs. multi-class classification",
    "text": "Binary vs. multi-class classification\n\n\nBinary classification: sorting inputs into one of two labels\n\nE.g., spam vs. not spam\n\nMulti-class classification: more than two labels\n\nE.g., face recognition with n possible identities\nE.g., image classification (cat, dog, person, other)\n\nToday: focus on binary classification with logistic regression"
  },
  {
    "objectID": "lectures/week6-logistic.html#part-1-foundations-of-logistic-regression",
    "href": "lectures/week6-logistic.html#part-1-foundations-of-logistic-regression",
    "title": "Logistic Regression",
    "section": "Part 1: Foundations of logistic regression",
    "text": "Part 1: Foundations of logistic regression\nMotivation, log-odds, and the logistic function."
  },
  {
    "objectID": "lectures/week6-logistic.html#example-dataset-email-spam",
    "href": "lectures/week6-logistic.html#example-dataset-email-spam",
    "title": "Logistic Regression",
    "section": "Example dataset: Email spam",
    "text": "Example dataset: Email spam\n\n### Loading the tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf_spam = read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/logistic/spam.csv\")\n\nRows: 3921 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (2): winner, number\ndbl  (18): spam, to_multiple, from, cc, sent_email, image, attach, dollar, i...\ndttm  (1): time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnrow(df_spam)\n\n[1] 3921\n\n\n\nDataset contains information about emails and whether they were spam, along with various features like number of characters, whether they contain certain words, etc."
  },
  {
    "objectID": "lectures/week6-logistic.html#why-not-linear-regression",
    "href": "lectures/week6-logistic.html#why-not-linear-regression",
    "title": "Logistic Regression",
    "section": "Why not linear regression?",
    "text": "Why not linear regression?\n\n\nspam is coded as 0 (no) or 1 (yes)\nCould we treat this as continuous and use linear regression?\nInterpret prediction \\(\\hat{y}\\) as probability of outcome?\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWhat issues might arise here?"
  },
  {
    "objectID": "lectures/week6-logistic.html#the-problem-predictions-beyond-01",
    "href": "lectures/week6-logistic.html#the-problem-predictions-beyond-01",
    "title": "Logistic Regression",
    "section": "The problem: predictions beyond [0,1]",
    "text": "The problem: predictions beyond [0,1]\n\nggplot(df_spam, aes(x = num_char, y = spam)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Number of characters\",\n       y = \"Spam (0 = no, 1 = yes)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLinear model generates predictions outside [0,1], but probability must be bounded!"
  },
  {
    "objectID": "lectures/week6-logistic.html#framing-the-problem-probabilistically",
    "href": "lectures/week6-logistic.html#framing-the-problem-probabilistically",
    "title": "Logistic Regression",
    "section": "Framing the problem probabilistically",
    "text": "Framing the problem probabilistically\n\n\nTreat each outcome as Bernoulli trials: “success” (spam) vs. “failure” (not spam)\nEach observation has independent probability of success: \\(p\\)\nOn its own: \\(p\\) = proportion of spam emails\nGoal: model \\(p\\) conditioned on other variables, i.e., \\(P(Y = 1 | X)\\)\n\n\n\nQuestion: What does \\(p\\) on its own remind you of from linear regression?\n\n\nAnswer: The intercept-only model (the mean of \\(Y\\))"
  },
  {
    "objectID": "lectures/week6-logistic.html#generalized-linear-models-glms",
    "href": "lectures/week6-logistic.html#generalized-linear-models-glms",
    "title": "Logistic Regression",
    "section": "Generalized linear models (GLMs)",
    "text": "Generalized linear models (GLMs)\n\nGeneralized linear models (GLMs) are generalizations of linear regression.\n\nEach GLM has:\n\n\nA probability distribution for the outcome variable\nA linear model: \\(\\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k\\)\nA link function relating the linear model to the outcome\n\n\n\nWe need a function that links our linear model to a probability score bounded at [0, 1]."
  },
  {
    "objectID": "lectures/week6-logistic.html#common-glms",
    "href": "lectures/week6-logistic.html#common-glms",
    "title": "Logistic Regression",
    "section": "Common GLMs",
    "text": "Common GLMs\n\n\n\n\n\n\n\n\n\n\nModel name\nDistribution\nLink function\nUse cases\nExample\n\n\n\n\nLinear regression\nNormal\nIdentity\nContinuous response\nHeight, price\n\n\nLogistic regression\nBernoulli/Binomial\nLogit\nBinary response\nSpam, fraud\n\n\nPoisson regression\nPoisson\nLog\nCount data\n# words, # visitors\n\n\n\nToday: logistic regression"
  },
  {
    "objectID": "lectures/week6-logistic.html#the-logit-link-function",
    "href": "lectures/week6-logistic.html#the-logit-link-function",
    "title": "Logistic Regression",
    "section": "The logit link function",
    "text": "The logit link function\nLogistic regression uses the logit link function:\n\\[\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)\\]\n\n\nWhere \\(p\\) is the probability of some outcome\nTakes a value between \\([0, 1]\\) and maps it to \\((-\\infty, \\infty)\\)\nAlso called the log-odds"
  },
  {
    "objectID": "lectures/week6-logistic.html#introducing-the-odds",
    "href": "lectures/week6-logistic.html#introducing-the-odds",
    "title": "Logistic Regression",
    "section": "Introducing the odds",
    "text": "Introducing the odds\n\nThe odds of an event are the ratio of the probability of an event occuring (\\(p\\)) and the probability of event not occurring (\\(1-p\\)).\n\n\\[\\text{Odds}(Y) = \\frac{p}{1-p}\\]\n\n\nUnlike \\(p\\), odds are bounded at \\([0, \\infty)\\)\nOdds of 1 means 50/50 chance\nOdds &gt; 1 means more likely to occur than not\nOdds &lt; 1 means less likely to occur than not"
  },
  {
    "objectID": "lectures/week6-logistic.html#visualizing-odds",
    "href": "lectures/week6-logistic.html#visualizing-odds",
    "title": "Logistic Regression",
    "section": "Visualizing odds",
    "text": "Visualizing odds\n\np &lt;- seq(0.01, 0.99, 0.01)\nodds &lt;- p / (1 - p)\n\nggplot(data.frame(p, odds), aes(x = p, y = odds)) +\n  geom_point() +\n  labs(x = \"P(Y)\", y = \"Odds(Y)\", \n       title = \"Odds(Y) vs. P(Y)\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week6-logistic.html#introducing-the-log-odds-logit",
    "href": "lectures/week6-logistic.html#introducing-the-log-odds-logit",
    "title": "Logistic Regression",
    "section": "Introducing the log-odds (logit)",
    "text": "Introducing the log-odds (logit)\n\nThe log-odds is the log of the odds (the logit function).\n\n\\[\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)\\]\n\n\nUnlike \\(p\\), log-odds are bounded at \\((-\\infty, \\infty)\\)\nThis is what we’ll model linearly!"
  },
  {
    "objectID": "lectures/week6-logistic.html#visualizing-log-odds",
    "href": "lectures/week6-logistic.html#visualizing-log-odds",
    "title": "Logistic Regression",
    "section": "Visualizing log-odds",
    "text": "Visualizing log-odds\n\nlog_odds &lt;- log(p / (1 - p))\n\nggplot(data.frame(p, log_odds), aes(x = p, y = log_odds)) +\n  geom_point() +\n  labs(x = \"P(Y)\", y = \"Log-odds(Y)\", \n       title = \"Log-odds(Y) vs. P(Y)\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week6-logistic.html#interpreting-the-sign-of-log-odds",
    "href": "lectures/week6-logistic.html#interpreting-the-sign-of-log-odds",
    "title": "Logistic Regression",
    "section": "Interpreting the sign of log-odds",
    "text": "Interpreting the sign of log-odds\n\nggplot(data.frame(p, log_odds), aes(x = p, y = log_odds)) +\n  geom_point() +\n  geom_vline(xintercept = 0.5, linetype = \"dotted\", color = \"red\") +\n  geom_hline(yintercept = 0, linetype = \"dotted\", color = \"red\") +\n  labs(x = \"P(Y)\", y = \"Log-odds(Y)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nPositive log-odds → \\(p &gt; 0.5\\)\nNegative log-odds → \\(p &lt; 0.5\\)"
  },
  {
    "objectID": "lectures/week6-logistic.html#is-log-odds-linearly-related-to-p",
    "href": "lectures/week6-logistic.html#is-log-odds-linearly-related-to-p",
    "title": "Logistic Regression",
    "section": "Is log-odds linearly related to p?",
    "text": "Is log-odds linearly related to p?\nNo! The log-odds of \\(Y\\) is non-linearly related to \\(P(Y)\\).\n\n\nThis means we cannot interpret linear changes in log-odds as linear changes in probability\nThis will be very important when interpreting logistic regression models"
  },
  {
    "objectID": "lectures/week6-logistic.html#the-logistic-function",
    "href": "lectures/week6-logistic.html#the-logistic-function",
    "title": "Logistic Regression",
    "section": "The logistic function",
    "text": "The logistic function\n\nThe logistic function is the inverse of the logit function.\n\n\\[P(Y) = \\frac{e^{\\text{logit}(p)}}{1 + e^{\\text{logit}(p)}}\\]\n\n\nConverts log-odds back to probability\nMaps \\((-\\infty, \\infty)\\) to \\([0, 1]\\)\nAlso called the sigmoid function"
  },
  {
    "objectID": "lectures/week6-logistic.html#mapping-from-log-odds-to-probability",
    "href": "lectures/week6-logistic.html#mapping-from-log-odds-to-probability",
    "title": "Logistic Regression",
    "section": "Mapping from log-odds to probability",
    "text": "Mapping from log-odds to probability\n\nlog_odds_range &lt;- seq(-10, 10, 0.1)\np_range &lt;- exp(log_odds_range) / (1 + exp(log_odds_range))\nggplot(data.frame(log_odds_range, p_range), \n       aes(x = log_odds_range, y = p_range)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Log-odds(Y)\", y = \"P(Y)\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week6-logistic.html#where-does-regression-come-in",
    "href": "lectures/week6-logistic.html#where-does-regression-come-in",
    "title": "Logistic Regression",
    "section": "Where does regression come in?",
    "text": "Where does regression come in?\nWith logistic regression, we learn parameters \\(\\beta\\) for:\n\\[\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k\\]\n\n\nOur “dependent variable” is the log-odds (logit) of \\(p\\)\nWe learn a linear relationship between \\(X\\) and the log-odds of our outcome\nNOT a linear relationship with probability itself!"
  },
  {
    "objectID": "lectures/week6-logistic.html#interpreting-β-log-odds-x-is-linear",
    "href": "lectures/week6-logistic.html#interpreting-β-log-odds-x-is-linear",
    "title": "Logistic Regression",
    "section": "Interpreting β: log-odds ~ X is linear",
    "text": "Interpreting β: log-odds ~ X is linear\n\\[\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X\\]\n\n\nIf \\(\\beta_1 &gt; 0\\): For each 1-unit increase in \\(X\\), log-odds increase by \\(\\beta_1\\)\nIf \\(\\beta_1 &lt; 0\\): For each 1-unit increase in \\(X\\), log-odds decrease by \\(|\\beta_1|\\)\nStraightforward linear interpretation for log-odds"
  },
  {
    "objectID": "lectures/week6-logistic.html#interpreting-β-py-x-is-not-linear",
    "href": "lectures/week6-logistic.html#interpreting-β-py-x-is-not-linear",
    "title": "Logistic Regression",
    "section": "Interpreting β: P(Y) ~ X is NOT linear",
    "text": "Interpreting β: P(Y) ~ X is NOT linear\nThe mapping between log-odds and \\(P(Y)\\) is not linear.\n\n\n\n\n\n\n\n\n\nWe cannot interpret coefficients linearly with respect to \\(P(Y)\\)!"
  },
  {
    "objectID": "lectures/week6-logistic.html#part-2-logistic-regression-in-r",
    "href": "lectures/week6-logistic.html#part-2-logistic-regression-in-r",
    "title": "Logistic Regression",
    "section": "Part 2: Logistic regression in R",
    "text": "Part 2: Logistic regression in R\nUsing glm, interpreting logistic models."
  },
  {
    "objectID": "lectures/week6-logistic.html#logistic-regression-in-r",
    "href": "lectures/week6-logistic.html#logistic-regression-in-r",
    "title": "Logistic Regression",
    "section": "Logistic regression in R",
    "text": "Logistic regression in R\nUse the glm() function with family = binomial:\n\nmodel &lt;- glm(y ~ x,  ### formula\n             data = df_name,  ## dataframe name\n             family = binomial(link = \"logit\")) ## using logit link\n\n\n\nfamily = binomial: specifies we’re modeling binary outcomes\nlink = \"logit\": specifies the logit link function (default for binomial)\nFitting is straightforward—interpreting is the harder part!"
  },
  {
    "objectID": "lectures/week6-logistic.html#fitting-a-simple-model",
    "href": "lectures/week6-logistic.html#fitting-a-simple-model",
    "title": "Logistic Regression",
    "section": "Fitting a simple model",
    "text": "Fitting a simple model\nLet’s predict spam from num_char (message length):\n\nmod_len &lt;- glm(spam ~ num_char, \n               data = df_spam, \n               family = binomial)\nsummary(mod_len)\n\n\nCall:\nglm(formula = spam ~ num_char, family = binomial, data = df_spam)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.798738   0.071562 -25.135  &lt; 2e-16 ***\nnum_char    -0.062071   0.008014  -7.746  9.5e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2437.2  on 3920  degrees of freedom\nResidual deviance: 2346.4  on 3919  degrees of freedom\nAIC: 2350.4\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "lectures/week6-logistic.html#interpreting-the-coefficients",
    "href": "lectures/week6-logistic.html#interpreting-the-coefficients",
    "title": "Logistic Regression",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\n\ncoef(mod_len)\n\n(Intercept)    num_char \n-1.79873764 -0.06207116 \n\n\n\n\nIntercept (-1.80): log-odds of spam when num_char = 0\nnum_char (-0.06): for every 1-unit increase in num_char, log-odds of spam decrease by 0.06\nNegative coefficient → longer emails are less likely to be spam"
  },
  {
    "objectID": "lectures/week6-logistic.html#converting-log-odds-to-probability",
    "href": "lectures/week6-logistic.html#converting-log-odds-to-probability",
    "title": "Logistic Regression",
    "section": "Converting log-odds to probability",
    "text": "Converting log-odds to probability\nWhat’s \\(P(\\text{spam})\\) when num_char = 0?\n\n# Log-odds (just the intercept)\nlog_odds &lt;- coef(mod_len)[1]\nlog_odds\n\n(Intercept) \n  -1.798738 \n\n# Convert to probability\np &lt;- exp(log_odds) / (1 + exp(log_odds))\np\n\n(Intercept) \n  0.1420048 \n\n\nAbout 14% chance of spam for a message with 0 characters."
  },
  {
    "objectID": "lectures/week6-logistic.html#example-num_char-100",
    "href": "lectures/week6-logistic.html#example-num_char-100",
    "title": "Logistic Regression",
    "section": "Example: num_char = 100",
    "text": "Example: num_char = 100\nWhat’s \\(P(\\text{spam})\\) when num_char = 100?\n\n# Calculate log-odds\nlog_odds &lt;- coef(mod_len)[1] + coef(mod_len)[2] * 100\nlog_odds\n\n(Intercept) \n  -8.005853 \n\n# Convert to probability\np &lt;- exp(log_odds) / (1 + exp(log_odds))\np\n\n (Intercept) \n0.0003333936 \n\n\nLess than 0.1% chance—very unlikely to be spam!"
  },
  {
    "objectID": "lectures/week6-logistic.html#visualizing-log-odds-vs.-probability",
    "href": "lectures/week6-logistic.html#visualizing-log-odds-vs.-probability",
    "title": "Logistic Regression",
    "section": "Visualizing: log-odds vs. probability",
    "text": "Visualizing: log-odds vs. probability\n\nX &lt;- df_spam$num_char\nlo &lt;- coef(mod_len)[1] + coef(mod_len)[2] * X\np &lt;- exp(lo) / (1 + exp(lo))\n\npar(mfrow = c(1, 2))\nplot(X, lo, xlab = \"# Characters\", ylab = \"Log-odds(spam)\")\nplot(X, p, xlab = \"# Characters\", ylab = \"P(spam)\")\n\n\n\n\n\n\n\n\nLinear on log-odds scale, non-linear on probability scale!"
  },
  {
    "objectID": "lectures/week6-logistic.html#categorical-predictors",
    "href": "lectures/week6-logistic.html#categorical-predictors",
    "title": "Logistic Regression",
    "section": "Categorical predictors",
    "text": "Categorical predictors\nLet’s use winner (whether email contains the word “winner”):\n\nmod_winner &lt;- glm(spam ~ winner, \n                  data = df_spam, \n                  family = binomial)\nsummary(mod_winner)\n\n\nCall:\nglm(formula = spam ~ winner, family = binomial, data = df_spam)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.31405    0.05627 -41.121  &lt; 2e-16 ***\nwinneryes    1.52559    0.27549   5.538 3.06e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2437.2  on 3920  degrees of freedom\nResidual deviance: 2412.7  on 3919  degrees of freedom\nAIC: 2416.7\n\nNumber of Fisher Scoring iterations: 5"
  },
  {
    "objectID": "lectures/week6-logistic.html#interpreting-categorical-predictors",
    "href": "lectures/week6-logistic.html#interpreting-categorical-predictors",
    "title": "Logistic Regression",
    "section": "Interpreting categorical predictors",
    "text": "Interpreting categorical predictors\n\ncoef(mod_winner)\n\n(Intercept)   winneryes \n  -2.314047    1.525589 \n\n\n\n\nIntercept (-2.31): log-odds of spam when winner = \"no\"\nwinneryes (1.53): change in log-odds (relative to intercept) when winner = \"yes\"\nJust like linear regression: categorical predictors are relative to reference level"
  },
  {
    "objectID": "lectures/week6-logistic.html#probability-for-winner-no",
    "href": "lectures/week6-logistic.html#probability-for-winner-no",
    "title": "Logistic Regression",
    "section": "Probability for winner = “no”",
    "text": "Probability for winner = “no”\n\n# Log-odds (just intercept)\nlo &lt;- coef(mod_winner)[1]\n\n# Convert to probability\np &lt;- exp(lo) / (1 + exp(lo))\np\n\n(Intercept) \n  0.0899663 \n\n\nAbout 9% chance of spam without “winner”"
  },
  {
    "objectID": "lectures/week6-logistic.html#probability-for-winner-yes",
    "href": "lectures/week6-logistic.html#probability-for-winner-yes",
    "title": "Logistic Regression",
    "section": "Probability for winner = “yes”",
    "text": "Probability for winner = “yes”\n\n# Log-odds (intercept + coefficient)\nlo &lt;- coef(mod_winner)[1] + coef(mod_winner)[2]\n\n# Convert to probability\np &lt;- exp(lo) / (1 + exp(lo))\np\n\n(Intercept) \n     0.3125 \n\n\nAbout 31% chance of spam with “winner”—much higher!"
  },
  {
    "objectID": "lectures/week6-logistic.html#generating-predictions",
    "href": "lectures/week6-logistic.html#generating-predictions",
    "title": "Logistic Regression",
    "section": "Generating predictions",
    "text": "Generating predictions\nThe predict() function with type = \"response\" gives predicted \\(P(Y)\\):\n\npredictions &lt;- predict(mod_len, type = \"response\")\n\nggplot(df_spam, aes(x = num_char, y = predictions)) +\n  geom_point(alpha = 0.3) +\n  labs(x = \"Number of characters\",\n       y = \"Predicted P(spam)\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week6-logistic.html#summary",
    "href": "lectures/week6-logistic.html#summary",
    "title": "Logistic Regression",
    "section": "Summary",
    "text": "Summary\n\n\nMany statistical modeling problems involve categorical response variables\nLogistic regression for binary classification tasks\nIt’s a generalized linear model (GLM)\n\nPredicts log-odds of \\(P(Y)\\) as a linear function of \\(X\\)\nLog-odds converted to \\(P(Y)\\) using the logistic function\n\nInterpretation:\n\nLinear relationship with log-odds\nNon-linear relationship with probability"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#goals-of-the-lecture",
    "href": "lectures/week6-r-logistic-slides.html#goals-of-the-lecture",
    "title": "Logistic Regression",
    "section": "Goals of the lecture",
    "text": "Goals of the lecture\n\nClassification: dealing with categorical outcomes.\nLogistic regression.\n\nWhy not linear regression?\nGeneralized linear models (GLMs).\nOdds and log-odds.\nThe logistic function.\n\nBuilding and interpreting logistic models with glm."
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#what-is-classification",
    "href": "lectures/week6-r-logistic-slides.html#what-is-classification",
    "title": "Logistic Regression",
    "section": "What is classification?",
    "text": "What is classification?\n\n“To classify is human…We sort dirty dishes from clean, white laundry from colorfast, important email to be answered from e-junk…. Any part of the home, school, or workplace reveals some such system of classification.”\n— Bowker & Star, 2000\n\n\nClassification = predicting a categorical response variable using features\nCommon examples:\n\nIs an email spam or not spam?\nIs a cell mass cancerous or not cancerous?\nWill this customer buy or not buy?\nIs a credit card transaction fraudulent?\nIs this image a cat, dog, person, or other?"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#binary-vs.-multi-class-classification",
    "href": "lectures/week6-r-logistic-slides.html#binary-vs.-multi-class-classification",
    "title": "Logistic Regression",
    "section": "Binary vs. multi-class classification",
    "text": "Binary vs. multi-class classification\n\nBinary classification: sorting inputs into one of two labels\n\nE.g., spam vs. not spam\n\nMulti-class classification: more than two labels\n\nE.g., face recognition with n possible identities\nE.g., image classification (cat, dog, person, other)\n\nToday: focus on binary classification with logistic regression"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#part-1-foundations-of-logistic-regression",
    "href": "lectures/week6-r-logistic-slides.html#part-1-foundations-of-logistic-regression",
    "title": "Logistic Regression",
    "section": "Part 1: Foundations of logistic regression",
    "text": "Part 1: Foundations of logistic regression\nMotivation, log-odds, and the logistic function."
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#example-dataset-email-spam",
    "href": "lectures/week6-r-logistic-slides.html#example-dataset-email-spam",
    "title": "Logistic Regression",
    "section": "Example dataset: Email spam",
    "text": "Example dataset: Email spam\n\n### Loading the tidyverse\nlibrary(tidyverse)\n\ndf_spam = read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/logistic/spam.csv\")\nnrow(df_spam)\n\n\n\n[1] 3921\n\n\n\nDataset contains information about emails and whether they were spam, along with various features like number of characters, whether they contain certain words, etc."
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#why-not-linear-regression",
    "href": "lectures/week6-r-logistic-slides.html#why-not-linear-regression",
    "title": "Logistic Regression",
    "section": "Why not linear regression?",
    "text": "Why not linear regression?\n\nspam is coded as 0 (no) or 1 (yes)\nCould we treat this as continuous and use linear regression?\nInterpret prediction \\(\\hat{y}\\) as probability of outcome?\n\n\n\n\n\n💭 Check-in\n\n\nWhat issues might arise here?"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#the-problem-predictions-beyond-01",
    "href": "lectures/week6-r-logistic-slides.html#the-problem-predictions-beyond-01",
    "title": "Logistic Regression",
    "section": "The problem: predictions beyond [0,1]",
    "text": "The problem: predictions beyond [0,1]\n\nggplot(df_spam, aes(x = num_char, y = spam)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Number of characters\",\n       y = \"Spam (0 = no, 1 = yes)\") +\n  theme_minimal()\n\n\nLinear model generates predictions outside [0,1], but probability must be bounded!"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#framing-the-problem-probabilistically",
    "href": "lectures/week6-r-logistic-slides.html#framing-the-problem-probabilistically",
    "title": "Logistic Regression",
    "section": "Framing the problem probabilistically",
    "text": "Framing the problem probabilistically\n\nTreat each outcome as Bernoulli trials: “success” (spam) vs. “failure” (not spam)\nEach observation has independent probability of success: \\(p\\)\nOn its own: \\(p\\) = proportion of spam emails\nGoal: model \\(p\\) conditioned on other variables, i.e., \\(P(Y = 1 | X)\\)\n\n\nQuestion: What does \\(p\\) on its own remind you of from linear regression?\n\n\nAnswer: The intercept-only model (the mean of \\(Y\\))"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#generalized-linear-models-glms",
    "href": "lectures/week6-r-logistic-slides.html#generalized-linear-models-glms",
    "title": "Logistic Regression",
    "section": "Generalized linear models (GLMs)",
    "text": "Generalized linear models (GLMs)\n\nGeneralized linear models (GLMs) are generalizations of linear regression.\n\nEach GLM has:\n\nA probability distribution for the outcome variable\nA linear model: \\(\\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k\\)\nA link function relating the linear model to the outcome\n\n\nWe need a function that links our linear model to a probability score bounded at [0, 1]."
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#common-glms",
    "href": "lectures/week6-r-logistic-slides.html#common-glms",
    "title": "Logistic Regression",
    "section": "Common GLMs",
    "text": "Common GLMs\n\n\n\n\n\n\n\n\n\n\nModel name\nDistribution\nLink function\nUse cases\nExample\n\n\n\n\nLinear regression\nNormal\nIdentity\nContinuous response\nHeight, price\n\n\nLogistic regression\nBernoulli/Binomial\nLogit\nBinary response\nSpam, fraud\n\n\nPoisson regression\nPoisson\nLog\nCount data\n# words, # visitors\n\n\n\nToday: logistic regression"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#the-logit-link-function",
    "href": "lectures/week6-r-logistic-slides.html#the-logit-link-function",
    "title": "Logistic Regression",
    "section": "The logit link function",
    "text": "The logit link function\nLogistic regression uses the logit link function:\n\\[\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)\\]\n\nWhere \\(p\\) is the probability of some outcome\nTakes a value between \\([0, 1]\\) and maps it to \\((-\\infty, \\infty)\\)\nAlso called the log-odds"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#introducing-the-odds",
    "href": "lectures/week6-r-logistic-slides.html#introducing-the-odds",
    "title": "Logistic Regression",
    "section": "Introducing the odds",
    "text": "Introducing the odds\n\nThe odds of an event are the ratio of the probability of an event occuring (\\(p\\)) and the probability of event not occurring (\\(1-p\\)).\n\n\\[\\text{Odds}(Y) = \\frac{p}{1-p}\\]\n\nUnlike \\(p\\), odds are bounded at \\([0, \\infty)\\)\nOdds of 1 means 50/50 chance\nOdds &gt; 1 means more likely to occur than not\nOdds &lt; 1 means less likely to occur than not"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#visualizing-odds",
    "href": "lectures/week6-r-logistic-slides.html#visualizing-odds",
    "title": "Logistic Regression",
    "section": "Visualizing odds",
    "text": "Visualizing odds\n\np &lt;- seq(0.01, 0.99, 0.01)\nodds &lt;- p / (1 - p)\n\nggplot(data.frame(p, odds), aes(x = p, y = odds)) +\n  geom_point() +\n  labs(x = \"P(Y)\", y = \"Odds(Y)\", \n       title = \"Odds(Y) vs. P(Y)\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#introducing-the-log-odds-logit",
    "href": "lectures/week6-r-logistic-slides.html#introducing-the-log-odds-logit",
    "title": "Logistic Regression",
    "section": "Introducing the log-odds (logit)",
    "text": "Introducing the log-odds (logit)\n\nThe log-odds is the log of the odds (the logit function).\n\n\\[\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)\\]\n\nUnlike \\(p\\), log-odds are bounded at \\((-\\infty, \\infty)\\)\nThis is what we’ll model linearly!"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#visualizing-log-odds",
    "href": "lectures/week6-r-logistic-slides.html#visualizing-log-odds",
    "title": "Logistic Regression",
    "section": "Visualizing log-odds",
    "text": "Visualizing log-odds\n\nlog_odds &lt;- log(p / (1 - p))\n\nggplot(data.frame(p, log_odds), aes(x = p, y = log_odds)) +\n  geom_point() +\n  labs(x = \"P(Y)\", y = \"Log-odds(Y)\", \n       title = \"Log-odds(Y) vs. P(Y)\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#interpreting-the-sign-of-log-odds",
    "href": "lectures/week6-r-logistic-slides.html#interpreting-the-sign-of-log-odds",
    "title": "Logistic Regression",
    "section": "Interpreting the sign of log-odds",
    "text": "Interpreting the sign of log-odds\n\nggplot(data.frame(p, log_odds), aes(x = p, y = log_odds)) +\n  geom_point() +\n  geom_vline(xintercept = 0.5, linetype = \"dotted\", color = \"red\") +\n  geom_hline(yintercept = 0, linetype = \"dotted\", color = \"red\") +\n  labs(x = \"P(Y)\", y = \"Log-odds(Y)\") +\n  theme_minimal()\n\n\n\nPositive log-odds → \\(p &gt; 0.5\\)\nNegative log-odds → \\(p &lt; 0.5\\)"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#is-log-odds-linearly-related-to-p",
    "href": "lectures/week6-r-logistic-slides.html#is-log-odds-linearly-related-to-p",
    "title": "Logistic Regression",
    "section": "Is log-odds linearly related to p?",
    "text": "Is log-odds linearly related to p?\nNo! The log-odds of \\(Y\\) is non-linearly related to \\(P(Y)\\).\n\nThis means we cannot interpret linear changes in log-odds as linear changes in probability\nThis will be very important when interpreting logistic regression models"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#the-logistic-function",
    "href": "lectures/week6-r-logistic-slides.html#the-logistic-function",
    "title": "Logistic Regression",
    "section": "The logistic function",
    "text": "The logistic function\n\nThe logistic function is the inverse of the logit function.\n\n\\[P(Y) = \\frac{e^{\\text{logit}(p)}}{1 + e^{\\text{logit}(p)}}\\]\n\nConverts log-odds back to probability\nMaps \\((-\\infty, \\infty)\\) to \\([0, 1]\\)\nAlso called the sigmoid function"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#mapping-from-log-odds-to-probability",
    "href": "lectures/week6-r-logistic-slides.html#mapping-from-log-odds-to-probability",
    "title": "Logistic Regression",
    "section": "Mapping from log-odds to probability",
    "text": "Mapping from log-odds to probability\n\nlog_odds_range &lt;- seq(-10, 10, 0.1)\np_range &lt;- exp(log_odds_range) / (1 + exp(log_odds_range))\nggplot(data.frame(log_odds_range, p_range), \n       aes(x = log_odds_range, y = p_range)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Log-odds(Y)\", y = \"P(Y)\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#where-does-regression-come-in",
    "href": "lectures/week6-r-logistic-slides.html#where-does-regression-come-in",
    "title": "Logistic Regression",
    "section": "Where does regression come in?",
    "text": "Where does regression come in?\nWith logistic regression, we learn parameters \\(\\beta\\) for:\n\\[\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k\\]\n\nOur “dependent variable” is the log-odds (logit) of \\(p\\)\nWe learn a linear relationship between \\(X\\) and the log-odds of our outcome\nNOT a linear relationship with probability itself!"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#interpreting-β-log-odds-x-is-linear",
    "href": "lectures/week6-r-logistic-slides.html#interpreting-β-log-odds-x-is-linear",
    "title": "Logistic Regression",
    "section": "Interpreting β: log-odds ~ X is linear",
    "text": "Interpreting β: log-odds ~ X is linear\n\\[\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X\\]\n\nIf \\(\\beta_1 &gt; 0\\): For each 1-unit increase in \\(X\\), log-odds increase by \\(\\beta_1\\)\nIf \\(\\beta_1 &lt; 0\\): For each 1-unit increase in \\(X\\), log-odds decrease by \\(|\\beta_1|\\)\nStraightforward linear interpretation for log-odds"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#interpreting-β-py-x-is-not-linear",
    "href": "lectures/week6-r-logistic-slides.html#interpreting-β-py-x-is-not-linear",
    "title": "Logistic Regression",
    "section": "Interpreting β: P(Y) ~ X is NOT linear",
    "text": "Interpreting β: P(Y) ~ X is NOT linear\nThe mapping between log-odds and \\(P(Y)\\) is not linear.\n\nWe cannot interpret coefficients linearly with respect to \\(P(Y)\\)!"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#part-2-logistic-regression-in-r",
    "href": "lectures/week6-r-logistic-slides.html#part-2-logistic-regression-in-r",
    "title": "Logistic Regression",
    "section": "Part 2: Logistic regression in R",
    "text": "Part 2: Logistic regression in R\nUsing glm, interpreting logistic models."
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#logistic-regression-in-r",
    "href": "lectures/week6-r-logistic-slides.html#logistic-regression-in-r",
    "title": "Logistic Regression",
    "section": "Logistic regression in R",
    "text": "Logistic regression in R\nUse the glm() function with family = binomial:\n\nmodel &lt;- glm(y ~ x,  ### formula\n             data = df_name,  ## dataframe name\n             family = binomial(link = \"logit\")) ## using logit link\n\n\nfamily = binomial: specifies we’re modeling binary outcomes\nlink = \"logit\": specifies the logit link function (default for binomial)\nFitting is straightforward—interpreting is the harder part!"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#fitting-a-simple-model",
    "href": "lectures/week6-r-logistic-slides.html#fitting-a-simple-model",
    "title": "Logistic Regression",
    "section": "Fitting a simple model",
    "text": "Fitting a simple model\nLet’s predict spam from num_char (message length):\n\nmod_len &lt;- glm(spam ~ num_char, \n               data = df_spam, \n               family = binomial)\nsummary(mod_len)\n\n\nCall:\nglm(formula = spam ~ num_char, family = binomial, data = df_spam)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.798738   0.071562 -25.135  &lt; 2e-16 ***\nnum_char    -0.062071   0.008014  -7.746  9.5e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2437.2  on 3920  degrees of freedom\nResidual deviance: 2346.4  on 3919  degrees of freedom\nAIC: 2350.4\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#interpreting-the-coefficients",
    "href": "lectures/week6-r-logistic-slides.html#interpreting-the-coefficients",
    "title": "Logistic Regression",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\n\ncoef(mod_len)\n\n(Intercept)    num_char \n-1.79873764 -0.06207116 \n\n\n\nIntercept (-1.80): log-odds of spam when num_char = 0\nnum_char (-0.06): for every 1-unit increase in num_char, log-odds of spam decrease by 0.06\nNegative coefficient → longer emails are less likely to be spam"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#converting-log-odds-to-probability",
    "href": "lectures/week6-r-logistic-slides.html#converting-log-odds-to-probability",
    "title": "Logistic Regression",
    "section": "Converting log-odds to probability",
    "text": "Converting log-odds to probability\nWhat’s \\(P(\\text{spam})\\) when num_char = 0?\n\n# Log-odds (just the intercept)\nlog_odds &lt;- coef(mod_len)[1]\nlog_odds\n\n(Intercept) \n  -1.798738 \n\n# Convert to probability\np &lt;- exp(log_odds) / (1 + exp(log_odds))\np\n\n(Intercept) \n  0.1420048 \n\n\nAbout 14% chance of spam for a message with 0 characters."
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#example-num_char-100",
    "href": "lectures/week6-r-logistic-slides.html#example-num_char-100",
    "title": "Logistic Regression",
    "section": "Example: num_char = 100",
    "text": "Example: num_char = 100\nWhat’s \\(P(\\text{spam})\\) when num_char = 100?\n\n# Calculate log-odds\nlog_odds &lt;- coef(mod_len)[1] + coef(mod_len)[2] * 100\nlog_odds\n\n(Intercept) \n  -8.005853 \n\n# Convert to probability\np &lt;- exp(log_odds) / (1 + exp(log_odds))\np\n\n (Intercept) \n0.0003333936 \n\n\nLess than 0.1% chance—very unlikely to be spam!"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#visualizing-log-odds-vs.-probability",
    "href": "lectures/week6-r-logistic-slides.html#visualizing-log-odds-vs.-probability",
    "title": "Logistic Regression",
    "section": "Visualizing: log-odds vs. probability",
    "text": "Visualizing: log-odds vs. probability\n\nX &lt;- df_spam$num_char\nlo &lt;- coef(mod_len)[1] + coef(mod_len)[2] * X\np &lt;- exp(lo) / (1 + exp(lo))\n\npar(mfrow = c(1, 2))\nplot(X, lo, xlab = \"# Characters\", ylab = \"Log-odds(spam)\")\nplot(X, p, xlab = \"# Characters\", ylab = \"P(spam)\")\n\n\nLinear on log-odds scale, non-linear on probability scale!"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#categorical-predictors",
    "href": "lectures/week6-r-logistic-slides.html#categorical-predictors",
    "title": "Logistic Regression",
    "section": "Categorical predictors",
    "text": "Categorical predictors\nLet’s use winner (whether email contains the word “winner”):\n\nmod_winner &lt;- glm(spam ~ winner, \n                  data = df_spam, \n                  family = binomial)\nsummary(mod_winner)\n\n\nCall:\nglm(formula = spam ~ winner, family = binomial, data = df_spam)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.31405    0.05627 -41.121  &lt; 2e-16 ***\nwinneryes    1.52559    0.27549   5.538 3.06e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2437.2  on 3920  degrees of freedom\nResidual deviance: 2412.7  on 3919  degrees of freedom\nAIC: 2416.7\n\nNumber of Fisher Scoring iterations: 5"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#interpreting-categorical-predictors",
    "href": "lectures/week6-r-logistic-slides.html#interpreting-categorical-predictors",
    "title": "Logistic Regression",
    "section": "Interpreting categorical predictors",
    "text": "Interpreting categorical predictors\n\ncoef(mod_winner)\n\n(Intercept)   winneryes \n  -2.314047    1.525589 \n\n\n\nIntercept (-2.31): log-odds of spam when winner = \"no\"\nwinneryes (1.53): change in log-odds (relative to intercept) when winner = \"yes\"\nJust like linear regression: categorical predictors are relative to reference level"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#probability-for-winner-no",
    "href": "lectures/week6-r-logistic-slides.html#probability-for-winner-no",
    "title": "Logistic Regression",
    "section": "Probability for winner = “no”",
    "text": "Probability for winner = “no”\n\n# Log-odds (just intercept)\nlo &lt;- coef(mod_winner)[1]\n\n# Convert to probability\np &lt;- exp(lo) / (1 + exp(lo))\np\n\n(Intercept) \n  0.0899663 \n\n\nAbout 9% chance of spam without “winner”"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#probability-for-winner-yes",
    "href": "lectures/week6-r-logistic-slides.html#probability-for-winner-yes",
    "title": "Logistic Regression",
    "section": "Probability for winner = “yes”",
    "text": "Probability for winner = “yes”\n\n# Log-odds (intercept + coefficient)\nlo &lt;- coef(mod_winner)[1] + coef(mod_winner)[2]\n\n# Convert to probability\np &lt;- exp(lo) / (1 + exp(lo))\np\n\n(Intercept) \n     0.3125 \n\n\nAbout 31% chance of spam with “winner”—much higher!"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#generating-predictions",
    "href": "lectures/week6-r-logistic-slides.html#generating-predictions",
    "title": "Logistic Regression",
    "section": "Generating predictions",
    "text": "Generating predictions\nThe predict() function with type = \"response\" gives predicted \\(P(Y)\\):\n\npredictions &lt;- predict(mod_len, type = \"response\")\n\nggplot(df_spam, aes(x = num_char, y = predictions)) +\n  geom_point(alpha = 0.3) +\n  labs(x = \"Number of characters\",\n       y = \"Predicted P(spam)\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week6-r-logistic-slides.html#summary",
    "href": "lectures/week6-r-logistic-slides.html#summary",
    "title": "Logistic Regression",
    "section": "Summary",
    "text": "Summary\n\nMany statistical modeling problems involve categorical response variables\nLogistic regression for binary classification tasks\nIt’s a generalized linear model (GLM)\n\nPredicts log-odds of \\(P(Y)\\) as a linear function of \\(X\\)\nLog-odds converted to \\(P(Y)\\) using the logistic function\n\nInterpretation:\n\nLinear relationship with log-odds\nNon-linear relationship with probability"
  },
  {
    "objectID": "lectures/week4-regression.html",
    "href": "lectures/week4-regression.html",
    "title": "Linear regression in R",
    "section": "",
    "text": "Foundations of statistical modeling.\nBasic goals and assumptions of univariate linear regression.\n\nMeasuring error.\n\nLinear regression in R with lm.\n\nInterpreting lm model outputs: coefficients, p-values, and \\(R^2\\).\n\n\n\n\n### Loading the tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)"
  },
  {
    "objectID": "lectures/week4-regression.html#goals-of-the-lecture",
    "href": "lectures/week4-regression.html#goals-of-the-lecture",
    "title": "Linear regression in R",
    "section": "",
    "text": "Foundations of statistical modeling.\nBasic goals and assumptions of univariate linear regression.\n\nMeasuring error.\n\nLinear regression in R with lm.\n\nInterpreting lm model outputs: coefficients, p-values, and \\(R^2\\).\n\n\n\n\n### Loading the tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)"
  },
  {
    "objectID": "lectures/week4-regression.html#part-1-modeling",
    "href": "lectures/week4-regression.html#part-1-modeling",
    "title": "Linear regression in R",
    "section": "Part 1: Modeling",
    "text": "Part 1: Modeling\nWhat are “models”, and why should we build them?"
  },
  {
    "objectID": "lectures/week4-regression.html#from-descriptions-to-models",
    "href": "lectures/week4-regression.html#from-descriptions-to-models",
    "title": "Linear regression in R",
    "section": "From descriptions to models",
    "text": "From descriptions to models\n\nA statistical model is a mathematical model representing a data-generating process.\n\n\n\nSo far, we’ve focused on describing and visualizing data.\nBut often, we want to model our data.\n\nTypically involves learning some function mapping \\(X\\) to \\(Y\\).\n\nAll models are wrong, but some are useful."
  },
  {
    "objectID": "lectures/week4-regression.html#why-models",
    "href": "lectures/week4-regression.html#why-models",
    "title": "Linear regression in R",
    "section": "Why models?",
    "text": "Why models?\n\nStatistical models help us understand our data, and also predict new data.\n\n\n\nPrediction: Try to predict/estimate unseen values of \\(Y\\), given \\(X\\).\nInference: Try to understand how and why \\(X\\) relates to \\(Y\\), test hypotheses, and more.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBoth serve a useful function, and both are compatible! In general, models can help you think more clearly about your data."
  },
  {
    "objectID": "lectures/week4-regression.html#models-encode-functions",
    "href": "lectures/week4-regression.html#models-encode-functions",
    "title": "Linear regression in R",
    "section": "Models encode functions",
    "text": "Models encode functions\n\nA statistical model often represents a function mapping from \\(X\\) (inputs) to \\(Y\\) (outputs).\n\n\\(Y = \\beta X + \\epsilon\\)\n\n\n\\(Y\\): What we want to predict.\n\\(X\\): The features we’re using to predict \\(Y\\).\n\\(\\beta\\): The coefficients (or weights) mapping \\(X\\) to \\(Y\\).\n\\(\\epsilon\\): Residual variance or “error”.\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nThink of a research question from your domain. What would the \\(X\\) and \\(Y\\) be, if it was “translated” into a statistical model?"
  },
  {
    "objectID": "lectures/week4-regression.html#models-arent-perfect",
    "href": "lectures/week4-regression.html#models-arent-perfect",
    "title": "Linear regression in R",
    "section": "Models aren’t perfect",
    "text": "Models aren’t perfect\n\nNo model is perfect; all models have some amount of prediction error, typically called residuals or error."
  },
  {
    "objectID": "lectures/week4-regression.html#models-have-trade-offs",
    "href": "lectures/week4-regression.html#models-have-trade-offs",
    "title": "Linear regression in R",
    "section": "Models have trade-offs",
    "text": "Models have trade-offs\n\nIn general, there is often a trade-off between the flexibility of a model and the interpretability of that model.\n\n\n\nMore flexible models can learn more complex functions/relationships, but they are often harder to interpret.\n\nThey’re also more likely to **overfitt*.\n\nLess flexible models (e.g., linear regression) have higher bias, but are often easier to interpret, and less likely to overfit."
  },
  {
    "objectID": "lectures/week4-regression.html#part-2-univariate-linear-regression",
    "href": "lectures/week4-regression.html#part-2-univariate-linear-regression",
    "title": "Linear regression in R",
    "section": "Part 2: Univariate linear regression",
    "text": "Part 2: Univariate linear regression\nLinear equation, basic premise, key assumptions."
  },
  {
    "objectID": "lectures/week4-regression.html#linear-regression-basics",
    "href": "lectures/week4-regression.html#linear-regression-basics",
    "title": "Linear regression in R",
    "section": "Linear regression: basics",
    "text": "Linear regression: basics\n\nThe goal of linear regression is to find the line of best fit between some variable(s) \\(X\\) and the continuous dependent variable \\(Y\\).\n\n\n\nAssuming a linear relationship between \\(X\\) and \\(Y\\)…\n…find parameters \\(\\beta\\) that minimize prediction error.\nAllows for many predictors, but we’ll start with univariate regression: a *single predictor (variable)."
  },
  {
    "objectID": "lectures/week4-regression.html#the-line-of-best-fit",
    "href": "lectures/week4-regression.html#the-line-of-best-fit",
    "title": "Linear regression in R",
    "section": "The line of best fit",
    "text": "The line of best fit\nGiven some bivariate data, there are many possible lines we could draw. Each line is defined by the linear equation:\n\\(Y = \\beta_1 X_1 + \\beta_0\\)\n\n\n\\(\\beta_0\\): Intercept.\n\\(\\beta_1\\): Slope for \\(X_1\\)."
  },
  {
    "objectID": "lectures/week4-regression.html#the-best-fitting-line-pt.-1",
    "href": "lectures/week4-regression.html#the-best-fitting-line-pt.-1",
    "title": "Linear regression in R",
    "section": "The best-fitting line (pt. 1)",
    "text": "The best-fitting line (pt. 1)\nTo illustrate this, let’s simulate some data:\n\nset.seed(123)\nn &lt;- 50\nx &lt;- rnorm(n, mean = 10, sd = 2)\ny &lt;- 3 + 0.5 * x + rnorm(n, mean = 0, sd = 1.5)\ndf &lt;- data.frame(x = x, y = y)\n# Create the plot\nggplot(df, aes(x = x, y = y)) +\n  # Add data points\n  geom_point(color = \"steelblue\", size = 3, alpha = 0.7) +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week4-regression.html#the-best-fitting-line-pt.-2",
    "href": "lectures/week4-regression.html#the-best-fitting-line-pt.-2",
    "title": "Linear regression in R",
    "section": "The best-fitting line (pt. 2)",
    "text": "The best-fitting line (pt. 2)\nNow let’s plot different lines with the same slope but different intercepts.\n\n# Create the plot\nggplot(df, aes(x = x, y = y)) +\n  # Add several \"possible\" lines\n  geom_abline(intercept = 2, slope = 0.5, color = \"gray60\", linetype = \"dashed\", linewidth = 1) +\n  geom_abline(intercept = 3, slope = 0.5, color = \"gray60\", linetype = \"dashed\", linewidth = 1) +\n  geom_abline(intercept = 4, slope = 0.5, color = \"gray60\", linetype = \"dashed\", linewidth = 1) +\n  # Add data points\n  geom_point(color = \"steelblue\", size = 3, alpha = 0.7) +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week4-regression.html#the-best-fitting-line-pt.-3",
    "href": "lectures/week4-regression.html#the-best-fitting-line-pt.-3",
    "title": "Linear regression in R",
    "section": "The best-fitting line (pt. 3)",
    "text": "The best-fitting line (pt. 3)\nWe can also try the same intercept but different slopes.\n\n# Create the plot\nggplot(df, aes(x = x, y = y)) +\n  # Add several \"possible\" lines\n  geom_abline(intercept = 3, slope = 0.75, color = \"gray60\", linetype = \"dashed\", linewidth = 1) +\n  geom_abline(intercept = 3, slope = 0.5, color = \"gray60\", linetype = \"dashed\", linewidth = 1) +\n  geom_abline(intercept = 3, slope = 0.25, color = \"gray60\", linetype = \"dashed\", linewidth = 1) +\n  # Add data points\n  geom_point(color = \"steelblue\", size = 3, alpha = 0.7) +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week4-regression.html#some-lines-are-better-than-others",
    "href": "lectures/week4-regression.html#some-lines-are-better-than-others",
    "title": "Linear regression in R",
    "section": "Some lines are better than others",
    "text": "Some lines are better than others\n\nThe line of best fit minimizes the residual error, i.e., the difference between the predictions (the line) and the actual values.\n\n\\(RSS = \\sum_{i=1}^{N} (\\hat{y_i} - y_i^2)\\)\n\n\nFor each data point \\(i\\), subtract the actual value \\(y_i\\) from the predicted value \\(\\hat{y_i}\\).\nThen, square that difference.\nThen, sum all those squared differences.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIntuition: A “better” line is one that has smaller differences between the predicted and actual values."
  },
  {
    "objectID": "lectures/week4-regression.html#mse-mean-squared-error",
    "href": "lectures/week4-regression.html#mse-mean-squared-error",
    "title": "Linear regression in R",
    "section": "MSE: Mean-squared error",
    "text": "MSE: Mean-squared error\n\nThe mean-squared error (MSE) is the average squared error (as opposed to the sum).\n\n\n\nCalculate \\(RSS\\).\nThen, divide by \\(N\\).\nNote that RMSE is the root of MSE."
  },
  {
    "objectID": "lectures/week4-regression.html#calculating-mse-for-a-line",
    "href": "lectures/week4-regression.html#calculating-mse-for-a-line",
    "title": "Linear regression in R",
    "section": "Calculating MSE for a line",
    "text": "Calculating MSE for a line\nWe can compare the MSE for two different lines for the same data.\n\ny_pred1 = x * 1.5 + 2 ### Predictions from slope = 1.5 and intercept = 2\ny_pred2 = x * .5 + 3 ### Predictions from slope = .5 and intercept = 3\n\nmse_1 = sum((y_pred1 - y)**2)/length(y_pred1)\nmse_2 = sum((y_pred2 - y)**2)/length(y_pred2)\nprint(mse_1)\n\n[1] 83.65316\n\nprint(mse_2)\n\n[1] 1.855965"
  },
  {
    "objectID": "lectures/week4-regression.html#standard-error-of-the-estimate",
    "href": "lectures/week4-regression.html#standard-error-of-the-estimate",
    "title": "Linear regression in R",
    "section": "Standard error of the estimate",
    "text": "Standard error of the estimate\n\nThe standard error of the estimate is a measure of the expected prediction error, i.e., how much your predictions are “wrong” on average.\n\n\\(S_{Y|X} = \\sqrt{\\frac{RSS}{n-2}}\\)\n\n\nHow much, on average, do we expect \\(\\hat{Y}\\) to deviate from \\(Y\\)?\nA smaller number means a better fit."
  },
  {
    "objectID": "lectures/week4-regression.html#calculating-standard-error-of-the-estimate",
    "href": "lectures/week4-regression.html#calculating-standard-error-of-the-estimate",
    "title": "Linear regression in R",
    "section": "Calculating standard error of the estimate",
    "text": "Calculating standard error of the estimate\nWe can calculate standard error of the estimate:\n\nsy_1 = sqrt(sum((y_pred1 - y)**2)/(length(y_pred1)-2))\nsy_2 = sqrt(sum((y_pred2 - y)**2)/(length(y_pred2)-2))\nprint(sy_1)\n\n[1] 9.334812\n\nprint(sy_2)\n\n[1] 1.39043"
  },
  {
    "objectID": "lectures/week4-regression.html#evaluating-with-r-squared",
    "href": "lectures/week4-regression.html#evaluating-with-r-squared",
    "title": "Linear regression in R",
    "section": "Evaluating with R-squared",
    "text": "Evaluating with R-squared\n\nThe \\(R^2\\), or coefficient of determination, measures the proportion of variance in \\(Y\\) explained by the model.\n\n\\(R^2 = 1 - \\frac{RSS}{SS_Y}\\)\nWhere \\(SS_Y\\) is the sum of squared error in \\(Y\\).\n\n\n\n\n\n\n\nTip💭 Check-in\n\n\n\nWhat does this formula mean and why does it measure the proportion of variance in \\(Y\\) explained by the model?"
  },
  {
    "objectID": "lectures/week4-regression.html#decomposing-r2",
    "href": "lectures/week4-regression.html#decomposing-r2",
    "title": "Linear regression in R",
    "section": "Decomposing \\(R^2\\)",
    "text": "Decomposing \\(R^2\\)\n\\(R^2 = 1 - \\frac{RSS}{SS_Y}\\)\n\n\n\\(RSS\\) refers to the unexplained (residual) variance in \\(Y\\) from the model.\n\\(SS_Y\\) is the total variance in \\(Y\\) (i.e., before even fitting the model).\nThus, \\(\\frac{RSS}{SS_Y}\\) captures the proportion of unexplained variance by the model.\n\nIf \\(\\frac{RSS}{SS_Y} = 1\\), the model has explained no variance.\nIf \\(\\frac{RSS}{SS_Y} = 0\\), the model has explained all variance.\n\nAnd \\(1 - \\frac{RSS}{SS_Y}\\) captures the proportion of explained variance."
  },
  {
    "objectID": "lectures/week4-regression.html#key-assumptions-of-linear-regression",
    "href": "lectures/week4-regression.html#key-assumptions-of-linear-regression",
    "title": "Linear regression in R",
    "section": "Key assumptions of linear regression",
    "text": "Key assumptions of linear regression\nOrdinary least squares (OLS) regression has a few key assumptions.\n\n\n\n\n\n\n\n\nAssumption\nWhat it means\nWhy it matters\n\n\n\n\nLinearity\nThe relationship between \\(X\\) and \\(Y\\) is linear\nOLS fits a straight line, so if the true relationship is curved, predictions will be systematically biased\n\n\nIndependence\nThe observations are independent of each other\nDependent observations (e.g., repeated measures, time series) violate the assumption that errors are uncorrelated, leading to underestimated standard errors and invalid p-values\n\n\nHomoscedasticity\nThe variance of residuals is constant across all levels of \\(X\\) (equal spread)\nIf variance changes with \\(X\\) (heteroscedasticity), standard errors will be incorrect: some coefficients appear more/less significant than they truly are\n\n\nNormality of residuals\nThe errors are approximately normally distributed\nNeeded for valid confidence intervals and hypothesis tests (p-values). Less critical with large samples due to the Central Limit Theorem."
  },
  {
    "objectID": "lectures/week4-regression.html#interim-summary",
    "href": "lectures/week4-regression.html#interim-summary",
    "title": "Linear regression in R",
    "section": "Interim summary",
    "text": "Interim summary\n\n\nIn statistical modeling, we aim to construct models of our data.\nLinear regression is a specific (high-bias) model.\nThe goal of linear regression is to identify the best-fitting line for our data, i.e., to reduce the residual sum of squares (RSS).\nLinear regression rests on a few assumptions about the data (more on this in an upcoming lecture)."
  },
  {
    "objectID": "lectures/week4-regression.html#part-3-linear-regression-in-r",
    "href": "lectures/week4-regression.html#part-3-linear-regression-in-r",
    "title": "Linear regression in R",
    "section": "Part 3: Linear regression in R",
    "text": "Part 3: Linear regression in R\nUsing and interpreting fit lm models, using broom."
  },
  {
    "objectID": "lectures/week4-regression.html#the-lm-function",
    "href": "lectures/week4-regression.html#the-lm-function",
    "title": "Linear regression in R",
    "section": "The lm function",
    "text": "The lm function\n\nA linear model can be fit using the lm function.\n\n\n\nSupply a formula (i.e., y ~ x).\nSupply the data (i.e., a dataframe).\nUsage: lm(data = df_name, y ~ x).\n\nWhere y and x are columns in df_name."
  },
  {
    "objectID": "lectures/week4-regression.html#loading-a-dataset",
    "href": "lectures/week4-regression.html#loading-a-dataset",
    "title": "Linear regression in R",
    "section": "Loading a dataset",
    "text": "Loading a dataset\nTo illustrate linear regression in R, we’ll work with a sample dataset.\n\ndf_income &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/regression/income.csv\")\n\nRows: 30 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): Education, Seniority, Income\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_income %&gt;%\n  ggplot(aes(x = Education, y = Income)) +\n  geom_point(alpha = .5) +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week4-regression.html#visualizing-the-model-fit",
    "href": "lectures/week4-regression.html#visualizing-the-model-fit",
    "title": "Linear regression in R",
    "section": "Visualizing the model fit",
    "text": "Visualizing the model fit\nAs we discussed before, geom_smooth(method = \"lm\") can be used to plot a regression line over your data.\n\ndf_income %&gt;%\n  ggplot(aes(x = Education, y = Income)) +\n  geom_point(alpha = .5) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nBut to actually fit a model, we need to use lm."
  },
  {
    "objectID": "lectures/week4-regression.html#fitting-an-lm-model",
    "href": "lectures/week4-regression.html#fitting-an-lm-model",
    "title": "Linear regression in R",
    "section": "Fitting an lm model",
    "text": "Fitting an lm model\nCalling summary on a fit lm model object returns information about the coefficients and the overall model fit.\n\nmod = lm(data = df_income, Income ~ Education)\nsummary(mod)\n\n\nCall:\nlm(formula = Income ~ Education, data = df_income)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.568  -8.012   1.474   5.754  23.701 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -41.9166     9.7689  -4.291 0.000192 ***\nEducation     6.3872     0.5812  10.990 1.15e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.93 on 28 degrees of freedom\nMultiple R-squared:  0.8118,    Adjusted R-squared:  0.8051 \nF-statistic: 120.8 on 1 and 28 DF,  p-value: 1.151e-11"
  },
  {
    "objectID": "lectures/week4-regression.html#understanding-summary-output",
    "href": "lectures/week4-regression.html#understanding-summary-output",
    "title": "Linear regression in R",
    "section": "Understanding summary output",
    "text": "Understanding summary output\nCalling summary returns information about the coefficients of our model, as well as indicators of model fit.\n\nsumm = summary(mod) \nsumm$coefficients\n\n              Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -41.916612  9.7689490 -4.290801 1.918257e-04\nEducation     6.387161  0.5811716 10.990148 1.150567e-11\n\nsumm$r.squared\n\n[1] 0.8118069\n\n\n\n\nEstimate: fit intercept and slope coefficients.\nStd. Error: estimated standard error for those coefficients.\nt value: the t-statistic for those coefficients (slope / SE).\np-value: the probability of obtaining a t-statistic that large assuming the null hypothesis.\nMultiple R-squared: proportion of variance in y explained by x.\nResidual standard error: The standard error of the estimate."
  },
  {
    "objectID": "lectures/week4-regression.html#interpreting-coefficients",
    "href": "lectures/week4-regression.html#interpreting-coefficients",
    "title": "Linear regression in R",
    "section": "Interpreting coefficients",
    "text": "Interpreting coefficients\nThere are a few relevant things to note about coefficients:\n\n\nThe estimate tells you the direction (sign) and degree (magnitude) of the relationship.\nThe p-value tells you whether a relationship of this size would be expected assuming there was no effect (i.e., the null hypothesis).\n\nMore on this in an upcoming lecture!\n\n\n\n\n\n\n\n\n\n\nTip💭 Check-in\n\n\n\nHow would you report and interpret the intercept and slope we obtained for Income ~ Education? (As a reminder, \\(\\beta_0 = -41.9\\) and \\(\\beta_1 = 6.4\\).)"
  },
  {
    "objectID": "lectures/week4-regression.html#the-broom-package",
    "href": "lectures/week4-regression.html#the-broom-package",
    "title": "Linear regression in R",
    "section": "The broom package",
    "text": "The broom package\nThe broom package is also an easy way to quickly (and tidily) extract coefficient estimates.\n\ndf_coef = broom::tidy(mod)\ndf_coef\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -41.9      9.77      -4.29 1.92e- 4\n2 Education       6.39     0.581     11.0  1.15e-11"
  },
  {
    "objectID": "lectures/week4-regression.html#plotting-coefficients",
    "href": "lectures/week4-regression.html#plotting-coefficients",
    "title": "Linear regression in R",
    "section": "Plotting coefficients",
    "text": "Plotting coefficients\nOnce coefficients are in a dataframe, we can plot them using ggplot: a great way to visualize model fits!\n\ndf_coef %&gt;%\n  ggplot(aes(x = term, y = estimate)) +\n  geom_point() +\n  geom_errorbar(aes(x = term, y = estimate, \n                    ymin = estimate - std.error, \n                    ymax = estimate + std.error)) +\n  coord_flip() +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week4-regression.html#overall-model-fit-with-glance",
    "href": "lectures/week4-regression.html#overall-model-fit-with-glance",
    "title": "Linear regression in R",
    "section": "Overall model fit with glance",
    "text": "Overall model fit with glance\nbroom::glance() provides a tidy summary of overall model statistics.\n\nbroom::glance(mod)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.812         0.805  11.9      121. 1.15e-11     1  -116.  238.  242.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\nr.squared: proportion of variance explained\nadj.r.squared: adjusted for number of predictors\nsigma: residual standard error\np.value: p-value for the F-statistic"
  },
  {
    "objectID": "lectures/week4-regression.html#using-lm-with",
    "href": "lectures/week4-regression.html#using-lm-with",
    "title": "Linear regression in R",
    "section": "Using lm with %>%",
    "text": "Using lm with %&gt;%\nIf you like the %&gt;% syntax, you can integrate lm into a series of pipe operations.\n\ndf_income %&gt;%\n  ## fit model\n  lm(Income ~ Education, data = .) %&gt;%\n  ## tidy into dataframe\n  broom::tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -41.9      9.77      -4.29 1.92e- 4\n2 Education       6.39     0.581     11.0  1.15e-11"
  },
  {
    "objectID": "lectures/week4-regression.html#making-predictions",
    "href": "lectures/week4-regression.html#making-predictions",
    "title": "Linear regression in R",
    "section": "Making predictions",
    "text": "Making predictions\nOnce we’ve fit a model, we can use it to make predictions for new data!\n\n# Predict income for someone with 16 years of education\nnew_data &lt;- data.frame(Education = 16)\npredict(mod, newdata = new_data)\n\n       1 \n60.27797 \n\n# Or for multiple values\nnew_data &lt;- data.frame(Education = c(12, 16, 20))\npredict(mod, newdata = new_data, interval = \"confidence\")\n\n       fit      lwr      upr\n1 34.72932 27.86206 41.59659\n2 60.27797 55.79413 64.76181\n3 85.82661 79.62969 92.02353"
  },
  {
    "objectID": "lectures/week4-regression.html#visualizing-residuals",
    "href": "lectures/week4-regression.html#visualizing-residuals",
    "title": "Linear regression in R",
    "section": "Visualizing residuals",
    "text": "Visualizing residuals\nWe can also assess the residuals.\n\ndf_income %&gt;%\n  mutate(resid = residuals(mod)) %&gt;%\n  ggplot(aes(x = Income, y = resid)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_point() +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week4-regression.html#what-about-categorical-predictors",
    "href": "lectures/week4-regression.html#what-about-categorical-predictors",
    "title": "Linear regression in R",
    "section": "What about categorical predictors?",
    "text": "What about categorical predictors?\n\nA categorical (or qualitative) variable takes on one of several discrete values.\n\n\ndf_stroop &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/regression/stroop.csv\")\n\nRows: 48 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Condition\ndbl (1): RT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(df_stroop, 3)\n\n# A tibble: 3 × 2\n  Condition    RT\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Congruent 12.1 \n2 Congruent 16.8 \n3 Congruent  9.56\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nHow might you model and interpret the effect of a categorical variable?"
  },
  {
    "objectID": "lectures/week4-regression.html#contrast-coding",
    "href": "lectures/week4-regression.html#contrast-coding",
    "title": "Linear regression in R",
    "section": "Contrast coding",
    "text": "Contrast coding\nA common approach is to use the mean of one level (e.g., Congruent) as the intercept; the slope then represents the difference in means across those levels.\n\n### Actual means\ndf_stroop %&gt;%\n  group_by(Condition) %&gt;%\n  summarise(mean_RT = mean(RT))\n\n# A tibble: 2 × 2\n  Condition   mean_RT\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Congruent      14.1\n2 Incongruent    22.0\n\n### Fit coefficients\nlm(data = df_stroop, RT ~ Condition)$coefficients\n\n         (Intercept) ConditionIncongruent \n           14.051125             7.964792"
  },
  {
    "objectID": "lectures/week4-regression.html#conclusion",
    "href": "lectures/week4-regression.html#conclusion",
    "title": "Linear regression in R",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nLinear regression is a foundational tool in your modeling toolbox.\nR simplifies fitting and interpreting regressions:\n\nlm: fit the model.\nsummary/broom::tidy/broom::glance: interpret the model coefficients and \\(R^2\\).\npredict: get predictions from the model.\n\nNext time, we’ll discuss advanced issues, like multiple predictors."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#goals-of-the-lecture",
    "href": "lectures/week4-r-regression-slides.html#goals-of-the-lecture",
    "title": "Linear regression in R",
    "section": "Goals of the lecture",
    "text": "Goals of the lecture\n\nFoundations of statistical modeling.\nBasic goals and assumptions of univariate linear regression.\n\nMeasuring error.\n\nLinear regression in R with lm.\n\nInterpreting lm model outputs: coefficients, p-values, and \\(R^2\\)."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#part-1-modeling",
    "href": "lectures/week4-r-regression-slides.html#part-1-modeling",
    "title": "Linear regression in R",
    "section": "Part 1: Modeling",
    "text": "Part 1: Modeling\nWhat are “models”, and why should we build them?"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#from-descriptions-to-models",
    "href": "lectures/week4-r-regression-slides.html#from-descriptions-to-models",
    "title": "Linear regression in R",
    "section": "From descriptions to models",
    "text": "From descriptions to models\n\nA statistical model is a mathematical model representing a data-generating process.\n\n\nSo far, we’ve focused on describing and visualizing data.\nBut often, we want to model our data.\n\nTypically involves learning some function mapping \\(X\\) to \\(Y\\).\n\nAll models are wrong, but some are useful."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#why-models",
    "href": "lectures/week4-r-regression-slides.html#why-models",
    "title": "Linear regression in R",
    "section": "Why models?",
    "text": "Why models?\n\nStatistical models help us understand our data, and also predict new data.\n\n\nPrediction: Try to predict/estimate unseen values of \\(Y\\), given \\(X\\).\nInference: Try to understand how and why \\(X\\) relates to \\(Y\\), test hypotheses, and more.\n\n\n\n\n\nNote\n\n\nBoth serve a useful function, and both are compatible! In general, models can help you think more clearly about your data."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#models-encode-functions",
    "href": "lectures/week4-r-regression-slides.html#models-encode-functions",
    "title": "Linear regression in R",
    "section": "Models encode functions",
    "text": "Models encode functions\n\nA statistical model often represents a function mapping from \\(X\\) (inputs) to \\(Y\\) (outputs).\n\n\\(Y = \\beta X + \\epsilon\\)\n\n\\(Y\\): What we want to predict.\n\\(X\\): The features we’re using to predict \\(Y\\).\n\\(\\beta\\): The coefficients (or weights) mapping \\(X\\) to \\(Y\\).\n\\(\\epsilon\\): Residual variance or “error”.\n\n\n\n\n\n💭 Check-in\n\n\nThink of a research question from your domain. What would the \\(X\\) and \\(Y\\) be, if it was “translated” into a statistical model?"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#models-arent-perfect",
    "href": "lectures/week4-r-regression-slides.html#models-arent-perfect",
    "title": "Linear regression in R",
    "section": "Models aren’t perfect",
    "text": "Models aren’t perfect\n\nNo model is perfect; all models have some amount of prediction error, typically called residuals or error."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#models-have-trade-offs",
    "href": "lectures/week4-r-regression-slides.html#models-have-trade-offs",
    "title": "Linear regression in R",
    "section": "Models have trade-offs",
    "text": "Models have trade-offs\n\nIn general, there is often a trade-off between the flexibility of a model and the interpretability of that model.\n\n\nMore flexible models can learn more complex functions/relationships, but they are often harder to interpret.\n\nThey’re also more likely to **overfitt*.\n\nLess flexible models (e.g., linear regression) have higher bias, but are often easier to interpret, and less likely to overfit."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#part-2-univariate-linear-regression",
    "href": "lectures/week4-r-regression-slides.html#part-2-univariate-linear-regression",
    "title": "Linear regression in R",
    "section": "Part 2: Univariate linear regression",
    "text": "Part 2: Univariate linear regression\nLinear equation, basic premise, key assumptions."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#linear-regression-basics",
    "href": "lectures/week4-r-regression-slides.html#linear-regression-basics",
    "title": "Linear regression in R",
    "section": "Linear regression: basics",
    "text": "Linear regression: basics\n\nThe goal of linear regression is to find the line of best fit between some variable(s) \\(X\\) and the continuous dependent variable \\(Y\\).\n\n\nAssuming a linear relationship between \\(X\\) and \\(Y\\)…\n…find parameters \\(\\beta\\) that minimize prediction error.\nAllows for many predictors, but we’ll start with univariate regression: a *single predictor (variable)."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#the-line-of-best-fit",
    "href": "lectures/week4-r-regression-slides.html#the-line-of-best-fit",
    "title": "Linear regression in R",
    "section": "The line of best fit",
    "text": "The line of best fit\nGiven some bivariate data, there are many possible lines we could draw. Each line is defined by the linear equation:\n\\(Y = \\beta_1 X_1 + \\beta_0\\)\n\n\\(\\beta_0\\): Intercept.\n\\(\\beta_1\\): Slope for \\(X_1\\)."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#the-best-fitting-line-pt.-1",
    "href": "lectures/week4-r-regression-slides.html#the-best-fitting-line-pt.-1",
    "title": "Linear regression in R",
    "section": "The best-fitting line (pt. 1)",
    "text": "The best-fitting line (pt. 1)\nTo illustrate this, let’s simulate some data:\n\nset.seed(123)\nn &lt;- 50\nx &lt;- rnorm(n, mean = 10, sd = 2)\ny &lt;- 3 + 0.5 * x + rnorm(n, mean = 0, sd = 1.5)\ndf &lt;- data.frame(x = x, y = y)\n# Create the plot\nggplot(df, aes(x = x, y = y)) +\n  # Add data points\n  geom_point(color = \"steelblue\", size = 3, alpha = 0.7) +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#the-best-fitting-line-pt.-2",
    "href": "lectures/week4-r-regression-slides.html#the-best-fitting-line-pt.-2",
    "title": "Linear regression in R",
    "section": "The best-fitting line (pt. 2)",
    "text": "The best-fitting line (pt. 2)\nNow let’s plot different lines with the same slope but different intercepts.\n\n# Create the plot\nggplot(df, aes(x = x, y = y)) +\n  # Add several \"possible\" lines\n  geom_abline(intercept = 2, slope = 0.5, color = \"gray60\", linetype = \"dashed\", linewidth = 1) +\n  geom_abline(intercept = 3, slope = 0.5, color = \"gray60\", linetype = \"dashed\", linewidth = 1) +\n  geom_abline(intercept = 4, slope = 0.5, color = \"gray60\", linetype = \"dashed\", linewidth = 1) +\n  # Add data points\n  geom_point(color = \"steelblue\", size = 3, alpha = 0.7) +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#the-best-fitting-line-pt.-3",
    "href": "lectures/week4-r-regression-slides.html#the-best-fitting-line-pt.-3",
    "title": "Linear regression in R",
    "section": "The best-fitting line (pt. 3)",
    "text": "The best-fitting line (pt. 3)\nWe can also try the same intercept but different slopes.\n\n# Create the plot\nggplot(df, aes(x = x, y = y)) +\n  # Add several \"possible\" lines\n  geom_abline(intercept = 3, slope = 0.75, color = \"gray60\", linetype = \"dashed\", linewidth = 1) +\n  geom_abline(intercept = 3, slope = 0.5, color = \"gray60\", linetype = \"dashed\", linewidth = 1) +\n  geom_abline(intercept = 3, slope = 0.25, color = \"gray60\", linetype = \"dashed\", linewidth = 1) +\n  # Add data points\n  geom_point(color = \"steelblue\", size = 3, alpha = 0.7) +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#some-lines-are-better-than-others",
    "href": "lectures/week4-r-regression-slides.html#some-lines-are-better-than-others",
    "title": "Linear regression in R",
    "section": "Some lines are better than others",
    "text": "Some lines are better than others\n\nThe line of best fit minimizes the residual error, i.e., the difference between the predictions (the line) and the actual values.\n\n\\(RSS = \\sum_{i=1}^{N} (\\hat{y_i} - y_i^2)\\)\n\nFor each data point \\(i\\), subtract the actual value \\(y_i\\) from the predicted value \\(\\hat{y_i}\\).\nThen, square that difference.\nThen, sum all those squared differences.\n\n\n\n\n\nTip\n\n\nIntuition: A “better” line is one that has smaller differences between the predicted and actual values."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#mse-mean-squared-error",
    "href": "lectures/week4-r-regression-slides.html#mse-mean-squared-error",
    "title": "Linear regression in R",
    "section": "MSE: Mean-squared error",
    "text": "MSE: Mean-squared error\n\nThe mean-squared error (MSE) is the average squared error (as opposed to the sum).\n\n\nCalculate \\(RSS\\).\nThen, divide by \\(N\\).\nNote that RMSE is the root of MSE."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#calculating-mse-for-a-line",
    "href": "lectures/week4-r-regression-slides.html#calculating-mse-for-a-line",
    "title": "Linear regression in R",
    "section": "Calculating MSE for a line",
    "text": "Calculating MSE for a line\nWe can compare the MSE for two different lines for the same data.\n\ny_pred1 = x * 1.5 + 2 ### Predictions from slope = 1.5 and intercept = 2\ny_pred2 = x * .5 + 3 ### Predictions from slope = .5 and intercept = 3\n\nmse_1 = sum((y_pred1 - y)**2)/length(y_pred1)\nmse_2 = sum((y_pred2 - y)**2)/length(y_pred2)\nprint(mse_1)\n\n\n\n[1] 83.65316\n\nprint(mse_2)\n\n[1] 1.855965"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#standard-error-of-the-estimate",
    "href": "lectures/week4-r-regression-slides.html#standard-error-of-the-estimate",
    "title": "Linear regression in R",
    "section": "Standard error of the estimate",
    "text": "Standard error of the estimate\n\nThe standard error of the estimate is a measure of the expected prediction error, i.e., how much your predictions are “wrong” on average.\n\n\\(S_{Y|X} = \\sqrt{\\frac{RSS}{n-2}}\\)\n\nHow much, on average, do we expect \\(\\hat{Y}\\) to deviate from \\(Y\\)?\nA smaller number means a better fit."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#calculating-standard-error-of-the-estimate",
    "href": "lectures/week4-r-regression-slides.html#calculating-standard-error-of-the-estimate",
    "title": "Linear regression in R",
    "section": "Calculating standard error of the estimate",
    "text": "Calculating standard error of the estimate\nWe can calculate standard error of the estimate:\n\nsy_1 = sqrt(sum((y_pred1 - y)**2)/(length(y_pred1)-2))\nsy_2 = sqrt(sum((y_pred2 - y)**2)/(length(y_pred2)-2))\nprint(sy_1)\n\n\n\n[1] 9.334812\n\nprint(sy_2)\n\n[1] 1.39043"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#evaluating-with-r-squared",
    "href": "lectures/week4-r-regression-slides.html#evaluating-with-r-squared",
    "title": "Linear regression in R",
    "section": "Evaluating with R-squared",
    "text": "Evaluating with R-squared\n\nThe \\(R^2\\), or coefficient of determination, measures the proportion of variance in \\(Y\\) explained by the model.\n\n\\(R^2 = 1 - \\frac{RSS}{SS_Y}\\)\nWhere \\(SS_Y\\) is the sum of squared error in \\(Y\\).\n\n\n\n\n💭 Check-in\n\n\nWhat does this formula mean and why does it measure the proportion of variance in \\(Y\\) explained by the model?"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#decomposing-r2",
    "href": "lectures/week4-r-regression-slides.html#decomposing-r2",
    "title": "Linear regression in R",
    "section": "Decomposing \\(R^2\\)",
    "text": "Decomposing \\(R^2\\)\n\\(R^2 = 1 - \\frac{RSS}{SS_Y}\\)\n\n\\(RSS\\) refers to the unexplained (residual) variance in \\(Y\\) from the model.\n\\(SS_Y\\) is the total variance in \\(Y\\) (i.e., before even fitting the model).\nThus, \\(\\frac{RSS}{SS_Y}\\) captures the proportion of unexplained variance by the model.\n\nIf \\(\\frac{RSS}{SS_Y} = 1\\), the model has explained no variance.\nIf \\(\\frac{RSS}{SS_Y} = 0\\), the model has explained all variance.\n\nAnd \\(1 - \\frac{RSS}{SS_Y}\\) captures the proportion of explained variance."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#key-assumptions-of-linear-regression",
    "href": "lectures/week4-r-regression-slides.html#key-assumptions-of-linear-regression",
    "title": "Linear regression in R",
    "section": "Key assumptions of linear regression",
    "text": "Key assumptions of linear regression\nOrdinary least squares (OLS) regression has a few key assumptions.\n\n\n\n\n\n\n\n\nAssumption\nWhat it means\nWhy it matters\n\n\n\n\nLinearity\nThe relationship between \\(X\\) and \\(Y\\) is linear\nOLS fits a straight line, so if the true relationship is curved, predictions will be systematically biased\n\n\nIndependence\nThe observations are independent of each other\nDependent observations (e.g., repeated measures, time series) violate the assumption that errors are uncorrelated, leading to underestimated standard errors and invalid p-values\n\n\nHomoscedasticity\nThe variance of residuals is constant across all levels of \\(X\\) (equal spread)\nIf variance changes with \\(X\\) (heteroscedasticity), standard errors will be incorrect: some coefficients appear more/less significant than they truly are\n\n\nNormality of residuals\nThe errors are approximately normally distributed\nNeeded for valid confidence intervals and hypothesis tests (p-values). Less critical with large samples due to the Central Limit Theorem."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#interim-summary",
    "href": "lectures/week4-r-regression-slides.html#interim-summary",
    "title": "Linear regression in R",
    "section": "Interim summary",
    "text": "Interim summary\n\nIn statistical modeling, we aim to construct models of our data.\nLinear regression is a specific (high-bias) model.\nThe goal of linear regression is to identify the best-fitting line for our data, i.e., to reduce the residual sum of squares (RSS).\nLinear regression rests on a few assumptions about the data (more on this in an upcoming lecture)."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#part-3-linear-regression-in-r",
    "href": "lectures/week4-r-regression-slides.html#part-3-linear-regression-in-r",
    "title": "Linear regression in R",
    "section": "Part 3: Linear regression in R",
    "text": "Part 3: Linear regression in R\nUsing and interpreting fit lm models, using broom."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#the-lm-function",
    "href": "lectures/week4-r-regression-slides.html#the-lm-function",
    "title": "Linear regression in R",
    "section": "The lm function",
    "text": "The lm function\n\nA linear model can be fit using the lm function.\n\n\nSupply a formula (i.e., y ~ x).\nSupply the data (i.e., a dataframe).\nUsage: lm(data = df_name, y ~ x).\n\nWhere y and x are columns in df_name."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#loading-a-dataset",
    "href": "lectures/week4-r-regression-slides.html#loading-a-dataset",
    "title": "Linear regression in R",
    "section": "Loading a dataset",
    "text": "Loading a dataset\nTo illustrate linear regression in R, we’ll work with a sample dataset.\n\ndf_income &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/regression/income.csv\")\ndf_income %&gt;%\n  ggplot(aes(x = Education, y = Income)) +\n  geom_point(alpha = .5) +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#visualizing-the-model-fit",
    "href": "lectures/week4-r-regression-slides.html#visualizing-the-model-fit",
    "title": "Linear regression in R",
    "section": "Visualizing the model fit",
    "text": "Visualizing the model fit\nAs we discussed before, geom_smooth(method = \"lm\") can be used to plot a regression line over your data.\n\ndf_income %&gt;%\n  ggplot(aes(x = Education, y = Income)) +\n  geom_point(alpha = .5) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nBut to actually fit a model, we need to use lm."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#fitting-an-lm-model",
    "href": "lectures/week4-r-regression-slides.html#fitting-an-lm-model",
    "title": "Linear regression in R",
    "section": "Fitting an lm model",
    "text": "Fitting an lm model\nCalling summary on a fit lm model object returns information about the coefficients and the overall model fit.\n\nmod = lm(data = df_income, Income ~ Education)\nsummary(mod)\n\n\n\n\nCall:\nlm(formula = Income ~ Education, data = df_income)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.568  -8.012   1.474   5.754  23.701 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -41.9166     9.7689  -4.291 0.000192 ***\nEducation     6.3872     0.5812  10.990 1.15e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.93 on 28 degrees of freedom\nMultiple R-squared:  0.8118,    Adjusted R-squared:  0.8051 \nF-statistic: 120.8 on 1 and 28 DF,  p-value: 1.151e-11"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#understanding-summary-output",
    "href": "lectures/week4-r-regression-slides.html#understanding-summary-output",
    "title": "Linear regression in R",
    "section": "Understanding summary output",
    "text": "Understanding summary output\nCalling summary returns information about the coefficients of our model, as well as indicators of model fit.\n\nsumm = summary(mod) \nsumm$coefficients\n\n\n\n              Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -41.916612  9.7689490 -4.290801 1.918257e-04\nEducation     6.387161  0.5811716 10.990148 1.150567e-11\n\nsumm$r.squared\n\n[1] 0.8118069\n\n\n\nEstimate: fit intercept and slope coefficients.\nStd. Error: estimated standard error for those coefficients.\nt value: the t-statistic for those coefficients (slope / SE).\np-value: the probability of obtaining a t-statistic that large assuming the null hypothesis.\nMultiple R-squared: proportion of variance in y explained by x.\nResidual standard error: The standard error of the estimate."
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#interpreting-coefficients",
    "href": "lectures/week4-r-regression-slides.html#interpreting-coefficients",
    "title": "Linear regression in R",
    "section": "Interpreting coefficients",
    "text": "Interpreting coefficients\nThere are a few relevant things to note about coefficients:\n\nThe estimate tells you the direction (sign) and degree (magnitude) of the relationship.\nThe p-value tells you whether a relationship of this size would be expected assuming there was no effect (i.e., the null hypothesis).\n\nMore on this in an upcoming lecture!\n\n\n\n\n\n\n💭 Check-in\n\n\nHow would you report and interpret the intercept and slope we obtained for Income ~ Education? (As a reminder, \\(\\beta_0 = -41.9\\) and \\(\\beta_1 = 6.4\\).)"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#the-broom-package",
    "href": "lectures/week4-r-regression-slides.html#the-broom-package",
    "title": "Linear regression in R",
    "section": "The broom package",
    "text": "The broom package\nThe broom package is also an easy way to quickly (and tidily) extract coefficient estimates.\n\ndf_coef = broom::tidy(mod)\ndf_coef\n\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -41.9      9.77      -4.29 1.92e- 4\n2 Education       6.39     0.581     11.0  1.15e-11"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#plotting-coefficients",
    "href": "lectures/week4-r-regression-slides.html#plotting-coefficients",
    "title": "Linear regression in R",
    "section": "Plotting coefficients",
    "text": "Plotting coefficients\nOnce coefficients are in a dataframe, we can plot them using ggplot: a great way to visualize model fits!"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#overall-model-fit-with-glance",
    "href": "lectures/week4-r-regression-slides.html#overall-model-fit-with-glance",
    "title": "Linear regression in R",
    "section": "Overall model fit with glance",
    "text": "Overall model fit with glance\nbroom::glance() provides a tidy summary of overall model statistics.\n\nbroom::glance(mod)\n\n\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.812         0.805  11.9      121. 1.15e-11     1  -116.  238.  242.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\nr.squared: proportion of variance explained\nadj.r.squared: adjusted for number of predictors\nsigma: residual standard error\np.value: p-value for the F-statistic"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#using-lm-with",
    "href": "lectures/week4-r-regression-slides.html#using-lm-with",
    "title": "Linear regression in R",
    "section": "Using lm with %>%",
    "text": "Using lm with %&gt;%\nIf you like the %&gt;% syntax, you can integrate lm into a series of pipe operations.\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -41.9      9.77      -4.29 1.92e- 4\n2 Education       6.39     0.581     11.0  1.15e-11"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#making-predictions",
    "href": "lectures/week4-r-regression-slides.html#making-predictions",
    "title": "Linear regression in R",
    "section": "Making predictions",
    "text": "Making predictions\nOnce we’ve fit a model, we can use it to make predictions for new data!\n\n# Predict income for someone with 16 years of education\nnew_data &lt;- data.frame(Education = 16)\npredict(mod, newdata = new_data)\n\n\n\n       1 \n60.27797 \n\n# Or for multiple values\nnew_data &lt;- data.frame(Education = c(12, 16, 20))\npredict(mod, newdata = new_data, interval = \"confidence\")\n\n       fit      lwr      upr\n1 34.72932 27.86206 41.59659\n2 60.27797 55.79413 64.76181\n3 85.82661 79.62969 92.02353"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#visualizing-residuals",
    "href": "lectures/week4-r-regression-slides.html#visualizing-residuals",
    "title": "Linear regression in R",
    "section": "Visualizing residuals",
    "text": "Visualizing residuals\nWe can also assess the residuals.\n\ndf_income %&gt;%\n  mutate(resid = residuals(mod)) %&gt;%\n  ggplot(aes(x = Income, y = resid)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_point() +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#what-about-categorical-predictors",
    "href": "lectures/week4-r-regression-slides.html#what-about-categorical-predictors",
    "title": "Linear regression in R",
    "section": "What about categorical predictors?",
    "text": "What about categorical predictors?\n\nA categorical (or qualitative) variable takes on one of several discrete values.\n\n\ndf_stroop &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/regression/stroop.csv\")\nhead(df_stroop, 3)\n\n\n\n# A tibble: 3 × 2\n  Condition    RT\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Congruent 12.1 \n2 Congruent 16.8 \n3 Congruent  9.56\n\n\n\n\n\n\n💭 Check-in\n\n\nHow might you model and interpret the effect of a categorical variable?"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#contrast-coding",
    "href": "lectures/week4-r-regression-slides.html#contrast-coding",
    "title": "Linear regression in R",
    "section": "Contrast coding",
    "text": "Contrast coding\nA common approach is to use the mean of one level (e.g., Congruent) as the intercept; the slope then represents the difference in means across those levels.\n\n### Actual means\ndf_stroop %&gt;%\n  group_by(Condition) %&gt;%\n  summarise(mean_RT = mean(RT))\n\n\n\n# A tibble: 2 × 2\n  Condition   mean_RT\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Congruent      14.1\n2 Incongruent    22.0\n\n### Fit coefficients\nlm(data = df_stroop, RT ~ Condition)$coefficients\n\n         (Intercept) ConditionIncongruent \n           14.051125             7.964792"
  },
  {
    "objectID": "lectures/week4-r-regression-slides.html#conclusion",
    "href": "lectures/week4-r-regression-slides.html#conclusion",
    "title": "Linear regression in R",
    "section": "Conclusion",
    "text": "Conclusion\n\nLinear regression is a foundational tool in your modeling toolbox.\nR simplifies fitting and interpreting regressions:\n\nlm: fit the model.\nsummary/broom::tidy/broom::glance: interpret the model coefficients and \\(R^2\\).\npredict: get predictions from the model.\n\nNext time, we’ll discuss advanced issues, like multiple predictors."
  },
  {
    "objectID": "lectures/week1-r-intro.html",
    "href": "lectures/week1-r-intro.html",
    "title": "Introduction to R",
    "section": "",
    "text": "Brief tooling.\nWhy R?\nIntroduction to “base R”.\nBrief preview of the tidyverse.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that today’s lecture will extend to Friday’s in-class lab slot. Depending on whether we have time on Friday, we can also work on the take-hom lab in Friday’s class."
  },
  {
    "objectID": "lectures/week1-r-intro.html#goals-of-the-lecture",
    "href": "lectures/week1-r-intro.html#goals-of-the-lecture",
    "title": "Introduction to R",
    "section": "",
    "text": "Brief tooling.\nWhy R?\nIntroduction to “base R”.\nBrief preview of the tidyverse.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that today’s lecture will extend to Friday’s in-class lab slot. Depending on whether we have time on Friday, we can also work on the take-hom lab in Friday’s class."
  },
  {
    "objectID": "lectures/week1-r-intro.html#tooling-briefly",
    "href": "lectures/week1-r-intro.html#tooling-briefly",
    "title": "Introduction to R",
    "section": "Tooling (briefly)",
    "text": "Tooling (briefly)\n\nOne of the most frustrating parts of programming is tooling: getting your computer set up to actually do the stuff you want to learn about.\n\nIn this class, we’ll be working with the R programming language using a desktop IDE called RStudio.\n\n\nLinks to download and install RStudio can be found here.\nFollow the instructions: will include downloading and installing R.\n\nTo avoid other tooling headaches, we’ll just be using Canvas for course management.\nWe won’t be relying on GitHub, but it’s also very useful and important!"
  },
  {
    "objectID": "lectures/week1-r-intro.html#why-r",
    "href": "lectures/week1-r-intro.html#why-r",
    "title": "Introduction to R",
    "section": "Why R?",
    "text": "Why R?\nThere are many different programming languages. Why use R?\n\n\nR was specifically designed for data analysis.\n\nR supports a number of open-source packages to make data analysis easier.\n\nggplot, dplyr, lme4.\nWe’ll be learning all about these in the course!\n\nOther CSS-relevant languages include Python and SQL.\n\nLearning R is not incompatible with learning these!"
  },
  {
    "objectID": "lectures/week1-r-intro.html#introduction-to-base-r",
    "href": "lectures/week1-r-intro.html#introduction-to-base-r",
    "title": "Introduction to R",
    "section": "Introduction to “base” R",
    "text": "Introduction to “base” R\n\n“Base” R just refers to the set of functions and tools available “out of the box”, without using additional packages like tidyverse.\n\nBase R includes (but is not limited to):\n\n\nBasic mechanics like variable assignment.\nSimple functions like plot, as well as core types like vectors.\nStatistical methods like lm and anova (which we’ll discuss later)."
  },
  {
    "objectID": "lectures/week1-r-intro.html#variable-assignment",
    "href": "lectures/week1-r-intro.html#variable-assignment",
    "title": "Introduction to R",
    "section": "Variable assignment",
    "text": "Variable assignment\n\nVariables allow us to store information (values, vectors, etc.) so we can use it again later.\n\nHere, we create a variable called account, so we can add to it.\n\n# Our first R variable\naccount &lt;- 20 ### assign value to variable\naccount + 25 ### add to variable\n\n[1] 45\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can also use = for assignment, but &lt;- is the R convention. (For most purposes in this course, it shouldn’t matter which you use, and I sometimes mix them up!)\n\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nTry adding different numbers to account. What do you think will happen if you add a string like 'CSS'?"
  },
  {
    "objectID": "lectures/week1-r-intro.html#basic-variable-types",
    "href": "lectures/week1-r-intro.html#basic-variable-types",
    "title": "Introduction to R",
    "section": "Basic variable types",
    "text": "Basic variable types\n\nEach variable has a certain type or class.\n\nYou can do different things with different types of variables. For instance, you can’t calculate the mean of multiple characters, but you can for numeric types.\n\n\n\n\n\n\n\n\nType\nWhat it is\nExample\n\n\n\n\nnumeric\nNumbers (integers & decimals)\nage &lt;- 25, gpa &lt;- 3.7\n\n\ncharacter\nText strings\nname &lt;- \"Alice\"\n\n\nlogical\nTRUE/FALSE values\npassed &lt;- TRUE\n\n\ninteger\nWhole numbers only\ncount &lt;- 5L\n\n\nfactor\nCategorical data\ngrade &lt;- factor(\"A\")\n\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nUse the typeof() or class() functions to check the type of different variables or values."
  },
  {
    "objectID": "lectures/week1-r-intro.html#basic-operations-with-numeric-variables",
    "href": "lectures/week1-r-intro.html#basic-operations-with-numeric-variables",
    "title": "Introduction to R",
    "section": "Basic Operations with Numeric Variables",
    "text": "Basic Operations with Numeric Variables\nnumeric variables allow for a number of arithmetic operations (like a calculator).\n\n# Creating numeric variables\nmy_var &lt;- 5\n\n\nmy_var + 1 ### Addition\n\n[1] 6\n\nmy_var * 2 ### Multiplication\n\n[1] 10\n\nmy_var / 2 ### Division\n\n[1] 2.5\n\nmy_var ** 2 ### Exponentiation\n\n[1] 25"
  },
  {
    "objectID": "lectures/week1-r-intro.html#vectors-building-blocks-of-r",
    "href": "lectures/week1-r-intro.html#vectors-building-blocks-of-r",
    "title": "Introduction to R",
    "section": "Vectors: Building Blocks of R",
    "text": "Vectors: Building Blocks of R\n\nA vector is a collection of elements with the same class.\n\nVectors can be created with the c(...) function.\n\n# Creating vector\nmy_vector &lt;- c(25, 30, 32)\nprint(my_vector)\n\n[1] 25 30 32\n\nprint(my_vector[1])\n\n[1] 25\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nCreate your own vector, this time with character types in it. Try indexing into different parts of the vector."
  },
  {
    "objectID": "lectures/week1-r-intro.html#working-with-vectors",
    "href": "lectures/week1-r-intro.html#working-with-vectors",
    "title": "Introduction to R",
    "section": "Working with vectors",
    "text": "Working with vectors\nLike scalars, numeric vectors can be manipulated mathematically.\n\nmy_vector + 1 ### add 1 to each element\n\n[1] 26 31 33\n\nmy_vector * 5 ### Multiply each element by 5\n\n[1] 125 150 160\n\nmy_vector + c(1, 2, 3) ### Add vector to another vector\n\n[1] 26 32 35\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWhat do you think will happen if you try to add two vectors of different lengths?"
  },
  {
    "objectID": "lectures/week1-r-intro.html#functions",
    "href": "lectures/week1-r-intro.html#functions",
    "title": "Introduction to R",
    "section": "Functions",
    "text": "Functions\n\nA function implements some operation; you can think of it as a verb applied to some input.\n\nIn CSS, you’ll often be using functions to summarize your data (like a vector).\n\n# Creating vector\nheights &lt;- c(60, 65, 62, 70, 72, 73)\n\nmean(heights)\n\n[1] 67\n\nmedian(heights)\n\n[1] 67.5\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nCreate a vector containing possible incomes; then create the mean and median of this vector."
  },
  {
    "objectID": "lectures/week1-r-intro.html#creating-vectors-from-distributions-pt.-1",
    "href": "lectures/week1-r-intro.html#creating-vectors-from-distributions-pt.-1",
    "title": "Introduction to R",
    "section": "Creating vectors from distributions (pt. 1)",
    "text": "Creating vectors from distributions (pt. 1)\nIn addition to creating vectors by hand, we can use functions to create random vectors by sampling from some distribution, e.g., a normal distribution (rnorm(x, mean, sd)).\n\n# Creating vector\nv_norm = rnorm(100, mean = 50, sd = 2)\nhist(v_norm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nTry changing the different parameters of rnorm and then plotting the resulting vector again using hist. What do you notice about changing the mean or sd?"
  },
  {
    "objectID": "lectures/week1-r-intro.html#creating-vectors-from-distributions-pt.-2",
    "href": "lectures/week1-r-intro.html#creating-vectors-from-distributions-pt.-2",
    "title": "Introduction to R",
    "section": "Creating vectors from distributions (pt. 2)",
    "text": "Creating vectors from distributions (pt. 2)\nThere are also many types of distributions beyond normal distributions.\n\n\nUniform distributions: use runif.\nBinomial distributions: use rbinom.\nPoisson distributions: use rpois.\nSampling from these distributions (and visualizing them) is a helpful way to learn about different statistical distributions.\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nCreate a uniform distribution with runif with 100 values ranging from 2 to 3. If you’re not sure how to do this, use ?runif to learn more about the function."
  },
  {
    "objectID": "lectures/week1-r-intro.html#interim-summary",
    "href": "lectures/week1-r-intro.html#interim-summary",
    "title": "Introduction to R",
    "section": "Interim summary",
    "text": "Interim summary\nSo far, we’ve covered a number of core topics in base R.\n\n\nAssigning and working with variables.\nDifferent types of variables.\nApplying functions to variables.\nCreating vectors and visualizing them with hist.\nSampling from statistical distribution.\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nAny questions before we move on to creating dataframes and other kinds of plots?"
  },
  {
    "objectID": "lectures/week1-r-intro.html#dataframes",
    "href": "lectures/week1-r-intro.html#dataframes",
    "title": "Introduction to R",
    "section": "Dataframes",
    "text": "Dataframes\n\nThe data.frame class is a “tightly coupled collection of variables”; it’s also a fundamental data structure in R.\n\n\n\nLike a matrix, but with labeled columns of the same length.\nEach column corresponds to a vector of values (numbers, characters, etc.).\nSupports many useful operations.\nAnalogous to pandas.DataFrame in Python!\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that once we move to the tidyverse, we’ll be working with tibbles, which are basically like a data.frame."
  },
  {
    "objectID": "lectures/week1-r-intro.html#creating-a-data.frame",
    "href": "lectures/week1-r-intro.html#creating-a-data.frame",
    "title": "Introduction to R",
    "section": "Creating a data.frame",
    "text": "Creating a data.frame\n\n\nA data.frame can be created using the data.frame function.\nPass in labeled vectors of the same length.\n\n\n\ndf_example = data.frame(hours_studied = c(0, 2, 2, 3, 5, 8), test_score = c(70, 85, 89, 89, 94, 95))\nhead(df_example, 2)\n\n  hours_studied test_score\n1             0         70\n2             2         85\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nTry creating your own data.frame with custom columns. For example, one column could be movie_title and another could be your rating of that movie. Make sure the columns are the same length!"
  },
  {
    "objectID": "lectures/week1-r-intro.html#exploring-a-data.frame",
    "href": "lectures/week1-r-intro.html#exploring-a-data.frame",
    "title": "Introduction to R",
    "section": "Exploring a data.frame",
    "text": "Exploring a data.frame\nWe can use functions like nrow, head, and colnames to learn about our data.frame.\n\nprint(nrow(df_example)) ### How many rows?\n\n[1] 6\n\nprint(colnames(df_example)) ### Column names\n\n[1] \"hours_studied\" \"test_score\"   \n\nprint(head(df_example, 2)) ### First two rows\n\n  hours_studied test_score\n1             0         70\n2             2         85\n\nprint(str(df_example)) ### Structure of data\n\n'data.frame':   6 obs. of  2 variables:\n $ hours_studied: num  0 2 2 3 5 8\n $ test_score   : num  70 85 89 89 94 95\nNULL\n\nprint(summary(df_example)) ### Summary of each column\n\n hours_studied     test_score   \n Min.   :0.000   Min.   :70.00  \n 1st Qu.:2.000   1st Qu.:86.00  \n Median :2.500   Median :89.00  \n Mean   :3.333   Mean   :87.00  \n 3rd Qu.:4.500   3rd Qu.:92.75  \n Max.   :8.000   Max.   :95.00"
  },
  {
    "objectID": "lectures/week1-r-intro.html#accessing-individual-columns",
    "href": "lectures/week1-r-intro.html#accessing-individual-columns",
    "title": "Introduction to R",
    "section": "Accessing individual columns",
    "text": "Accessing individual columns\nYou can access individual columns using the dataframe$column_name syntax.\n\ndf_example$hours_studied ### Get vector\n\n[1] 0 2 2 3 5 8\n\nsummary(df_example$hours_studied) ### Get summary of vector\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   2.000   2.500   3.333   4.500   8.000"
  },
  {
    "objectID": "lectures/week1-r-intro.html#filtering-a-data.frame",
    "href": "lectures/week1-r-intro.html#filtering-a-data.frame",
    "title": "Introduction to R",
    "section": "Filtering a data.frame",
    "text": "Filtering a data.frame\nIn base R, you can filter a data.frame using the df[CONDITION] syntax, where CONDITION corresponds to a logical statement.\n\ndf_example[df_example$hours_studied &gt; 2, ]\n\n  hours_studied test_score\n4             3         89\n5             5         94\n6             8         95\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the tidyverse, we can use the handy filter function."
  },
  {
    "objectID": "lectures/week1-r-intro.html#simple-bivariate-plots",
    "href": "lectures/week1-r-intro.html#simple-bivariate-plots",
    "title": "Introduction to R",
    "section": "Simple bivariate plots",
    "text": "Simple bivariate plots\nOnce you have multiple vectors, you can plot the relationship between them, e.g., using a simple scatterplot.\n\nplot(df_example$hours_studied, \n     df_example$test_score,\n     xlab = \"Hours Studied\", \n     ylab = \"Test Score\",\n     pch = 16, # Filled circles\n     col = \"blue\")"
  },
  {
    "objectID": "lectures/week1-r-intro.html#calculating-correlations",
    "href": "lectures/week1-r-intro.html#calculating-correlations",
    "title": "Introduction to R",
    "section": "Calculating correlations",
    "text": "Calculating correlations\nYou can also quantify the relationship between variables, e.g., using a Pearson’s r correlation coefficient.\n\ncor.test(df_example$hours_studied, df_example$test_score)\n\n\n    Pearson's product-moment correlation\n\ndata:  df_example$hours_studied and df_example$test_score\nt = 2.8958, df = 4, p-value = 0.0443\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.03391115 0.97998161\nsample estimates:\n      cor \n0.8228274"
  },
  {
    "objectID": "lectures/week1-r-intro.html#working-with-missing-data",
    "href": "lectures/week1-r-intro.html#working-with-missing-data",
    "title": "Introduction to R",
    "section": "Working with missing data",
    "text": "Working with missing data\nReal data often contains missing values. R represents these as NA (Not Available). We’ll discuss these in more detail next week, but here’s a preview:\n\n# Vector with missing data\nsurvey_responses &lt;- c(85, 92, NA, 78, NA, 88)\nmean(survey_responses)                    # Returns NA!\n\n[1] NA\n\nmean(survey_responses, na.rm = TRUE)      # Remove NAs first: 85.75\n\n[1] 85.75"
  },
  {
    "objectID": "lectures/week1-r-intro.html#working-with-missing-data-pt.-2",
    "href": "lectures/week1-r-intro.html#working-with-missing-data-pt.-2",
    "title": "Introduction to R",
    "section": "Working with missing data (pt. 2)",
    "text": "Working with missing data (pt. 2)\nYou can remove missing data by filtering the data.frame, using the syntax below and the is.na condition.\n\n# Vector with missing data\nsurvey_responses[is.na(survey_responses) == FALSE]\n\n[1] 85 92 78 88"
  },
  {
    "objectID": "lectures/week1-r-intro.html#putting-it-together-simulating-data",
    "href": "lectures/week1-r-intro.html#putting-it-together-simulating-data",
    "title": "Introduction to R",
    "section": "Putting it together: simulating data",
    "text": "Putting it together: simulating data\nSo far, we’ve discussed a number of useful concepts in R:\n\n\nWorking with vectors.\nSimulating random distributions using rnorm.\nCreating data.frame objects and plotting or analyzing them.\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nNow, let’s simulate data.\n\nFirst, use rnorm to create a random normal distribution of parent_heights (use parameters that seem reasonable to you).\nThen, create a second variable called child_heights that’s related to parent_heights, ideally with some random error added. (Hint: Think about how a regression line works.).\nPut these variables in a data.frame.\nFinally, plot the relationship between those variables and calculate the correlation."
  },
  {
    "objectID": "lectures/week1-r-intro.html#simulating-data",
    "href": "lectures/week1-r-intro.html#simulating-data",
    "title": "Introduction to R",
    "section": "Simulating data",
    "text": "Simulating data\n\nparent_heights = rnorm(100, 65, 3)\nchild_heights = parent_heights + rnorm(100, 0, 2)\ndf_heights = data.frame(parent_heights, child_heights)\n\nplot(df_heights$parent_heights, \n     df_heights$child_heights,\n     xlab = \"Parent Height\", \n     ylab = \"Child Height\",\n     pch = 16, # Filled circles\n     col = \"blue\")\n\n\n\n\n\n\n\ncor(df_heights$parent_heights, df_heights$child_heights)\n\n[1] 0.8498317"
  },
  {
    "objectID": "lectures/week1-r-intro.html#a-conceptual-preview-of-the-tidyverse",
    "href": "lectures/week1-r-intro.html#a-conceptual-preview-of-the-tidyverse",
    "title": "Introduction to R",
    "section": "A conceptual preview of the tidyverse",
    "text": "A conceptual preview of the tidyverse\nNext week, we’ll discuss the tidyverse: a set of packages and functions developed to make data analysis and visualization in R easier.\nThis includes (but is not limited to):\n\n\nFunctions for transforming data, e.g., filter or mutate.\nFunctions for merging data, like left_join or inner_join.\nFunctions for visualizing data, like ggplot."
  },
  {
    "objectID": "lectures/week1-r-intro.html#lecture-wrap-up",
    "href": "lectures/week1-r-intro.html#lecture-wrap-up",
    "title": "Introduction to R",
    "section": "Lecture wrap-up",
    "text": "Lecture wrap-up\nThis course is not primarily about programming in R, but programming in R is a foundational skill for other parts of this course.\nThis lecture (and accompanying lab) is intended to give you more comfort with the following concepts:\n\n\nWorking with variables and different types of data.\nCreating and working with vectors.\n\nSimple plotting.\nWorking with data.frame objects."
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#goals-of-the-lecture",
    "href": "lectures/week1-r-intro-slides.html#goals-of-the-lecture",
    "title": "Introduction to R",
    "section": "Goals of the lecture",
    "text": "Goals of the lecture\n\nBrief tooling.\nWhy R?\nIntroduction to “base R”.\nBrief preview of the tidyverse.\n\n\n\n\n\nNote\n\n\nNote that today’s lecture will extend to Friday’s in-class lab slot. Depending on whether we have time on Friday, we can also work on the take-hom lab in Friday’s class."
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#tooling-briefly",
    "href": "lectures/week1-r-intro-slides.html#tooling-briefly",
    "title": "Introduction to R",
    "section": "Tooling (briefly)",
    "text": "Tooling (briefly)\n\nOne of the most frustrating parts of programming is tooling: getting your computer set up to actually do the stuff you want to learn about.\n\nIn this class, we’ll be working with the R programming language using a desktop IDE called RStudio.\n\nLinks to download and install RStudio can be found here.\nFollow the instructions: will include downloading and installing R.\n\nTo avoid other tooling headaches, we’ll just be using Canvas for course management.\nWe won’t be relying on GitHub, but it’s also very useful and important!"
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#why-r",
    "href": "lectures/week1-r-intro-slides.html#why-r",
    "title": "Introduction to R",
    "section": "Why R?",
    "text": "Why R?\nThere are many different programming languages. Why use R?\n\nR was specifically designed for data analysis.\n\nR supports a number of open-source packages to make data analysis easier.\n\nggplot, dplyr, lme4.\nWe’ll be learning all about these in the course!\n\nOther CSS-relevant languages include Python and SQL.\n\nLearning R is not incompatible with learning these!"
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#introduction-to-base-r",
    "href": "lectures/week1-r-intro-slides.html#introduction-to-base-r",
    "title": "Introduction to R",
    "section": "Introduction to “base” R",
    "text": "Introduction to “base” R\n\n“Base” R just refers to the set of functions and tools available “out of the box”, without using additional packages like tidyverse.\n\nBase R includes (but is not limited to):\n\nBasic mechanics like variable assignment.\nSimple functions like plot, as well as core types like vectors.\nStatistical methods like lm and anova (which we’ll discuss later)."
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#variable-assignment",
    "href": "lectures/week1-r-intro-slides.html#variable-assignment",
    "title": "Introduction to R",
    "section": "Variable assignment",
    "text": "Variable assignment\n\nVariables allow us to store information (values, vectors, etc.) so we can use it again later.\n\nHere, we create a variable called account, so we can add to it.\n\n# Our first R variable\naccount &lt;- 20 ### assign value to variable\naccount + 25 ### add to variable\n\n\n\n[1] 45\n\n\n\n\n\n\nNote\n\n\nYou can also use = for assignment, but &lt;- is the R convention. (For most purposes in this course, it shouldn’t matter which you use, and I sometimes mix them up!)\n\n\n\n\n\n\n\n\n💭 Check-in\n\n\nTry adding different numbers to account. What do you think will happen if you add a string like 'CSS'?"
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#basic-variable-types",
    "href": "lectures/week1-r-intro-slides.html#basic-variable-types",
    "title": "Introduction to R",
    "section": "Basic variable types",
    "text": "Basic variable types\n\nEach variable has a certain type or class.\n\nYou can do different things with different types of variables. For instance, you can’t calculate the mean of multiple characters, but you can for numeric types.\n\n\n\n\n\n\n\n\nType\nWhat it is\nExample\n\n\n\n\nnumeric\nNumbers (integers & decimals)\nage &lt;- 25, gpa &lt;- 3.7\n\n\ncharacter\nText strings\nname &lt;- \"Alice\"\n\n\nlogical\nTRUE/FALSE values\npassed &lt;- TRUE\n\n\ninteger\nWhole numbers only\ncount &lt;- 5L\n\n\nfactor\nCategorical data\ngrade &lt;- factor(\"A\")\n\n\n\n\n\n\n\n💭 Check-in\n\n\nUse the typeof() or class() functions to check the type of different variables or values."
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#basic-operations-with-numeric-variables",
    "href": "lectures/week1-r-intro-slides.html#basic-operations-with-numeric-variables",
    "title": "Introduction to R",
    "section": "Basic Operations with Numeric Variables",
    "text": "Basic Operations with Numeric Variables\nnumeric variables allow for a number of arithmetic operations (like a calculator).\n\n# Creating numeric variables\nmy_var &lt;- 5\n\n\nmy_var + 1 ### Addition\n\n\n\n[1] 6\n\nmy_var * 2 ### Multiplication\n\n[1] 10\n\nmy_var / 2 ### Division\n\n[1] 2.5\n\nmy_var ** 2 ### Exponentiation\n\n[1] 25"
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#vectors-building-blocks-of-r",
    "href": "lectures/week1-r-intro-slides.html#vectors-building-blocks-of-r",
    "title": "Introduction to R",
    "section": "Vectors: Building Blocks of R",
    "text": "Vectors: Building Blocks of R\n\nA vector is a collection of elements with the same class.\n\nVectors can be created with the c(...) function.\n\n# Creating vector\nmy_vector &lt;- c(25, 30, 32)\nprint(my_vector)\n\n\n\n[1] 25 30 32\n\nprint(my_vector[1])\n\n[1] 25\n\n\n\n\n\n\n💭 Check-in\n\n\nCreate your own vector, this time with character types in it. Try indexing into different parts of the vector."
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#working-with-vectors",
    "href": "lectures/week1-r-intro-slides.html#working-with-vectors",
    "title": "Introduction to R",
    "section": "Working with vectors",
    "text": "Working with vectors\nLike scalars, numeric vectors can be manipulated mathematically.\n\nmy_vector + 1 ### add 1 to each element\n\n\n\n[1] 26 31 33\n\nmy_vector * 5 ### Multiply each element by 5\n\n[1] 125 150 160\n\nmy_vector + c(1, 2, 3) ### Add vector to another vector\n\n[1] 26 32 35\n\n\n\n\n\n\n💭 Check-in\n\n\nWhat do you think will happen if you try to add two vectors of different lengths?"
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#functions",
    "href": "lectures/week1-r-intro-slides.html#functions",
    "title": "Introduction to R",
    "section": "Functions",
    "text": "Functions\n\nA function implements some operation; you can think of it as a verb applied to some input.\n\nIn CSS, you’ll often be using functions to summarize your data (like a vector).\n\n# Creating vector\nheights &lt;- c(60, 65, 62, 70, 72, 73)\n\nmean(heights)\n\n\n\n[1] 67\n\nmedian(heights)\n\n[1] 67.5\n\n\n\n\n\n\n💭 Check-in\n\n\nCreate a vector containing possible incomes; then create the mean and median of this vector."
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#creating-vectors-from-distributions-pt.-1",
    "href": "lectures/week1-r-intro-slides.html#creating-vectors-from-distributions-pt.-1",
    "title": "Introduction to R",
    "section": "Creating vectors from distributions (pt. 1)",
    "text": "Creating vectors from distributions (pt. 1)\nIn addition to creating vectors by hand, we can use functions to create random vectors by sampling from some distribution, e.g., a normal distribution (rnorm(x, mean, sd)).\n\n# Creating vector\nv_norm = rnorm(100, mean = 50, sd = 2)\nhist(v_norm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n💭 Check-in\n\n\nTry changing the different parameters of rnorm and then plotting the resulting vector again using hist. What do you notice about changing the mean or sd?"
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#creating-vectors-from-distributions-pt.-2",
    "href": "lectures/week1-r-intro-slides.html#creating-vectors-from-distributions-pt.-2",
    "title": "Introduction to R",
    "section": "Creating vectors from distributions (pt. 2)",
    "text": "Creating vectors from distributions (pt. 2)\nThere are also many types of distributions beyond normal distributions.\n\nUniform distributions: use runif.\nBinomial distributions: use rbinom.\nPoisson distributions: use rpois.\nSampling from these distributions (and visualizing them) is a helpful way to learn about different statistical distributions.\n\n\n\n\n\n💭 Check-in\n\n\nCreate a uniform distribution with runif with 100 values ranging from 2 to 3. If you’re not sure how to do this, use ?runif to learn more about the function."
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#interim-summary",
    "href": "lectures/week1-r-intro-slides.html#interim-summary",
    "title": "Introduction to R",
    "section": "Interim summary",
    "text": "Interim summary\nSo far, we’ve covered a number of core topics in base R.\n\nAssigning and working with variables.\nDifferent types of variables.\nApplying functions to variables.\nCreating vectors and visualizing them with hist.\nSampling from statistical distribution.\n\n\n\n\n\n💭 Check-in\n\n\nAny questions before we move on to creating dataframes and other kinds of plots?"
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#dataframes",
    "href": "lectures/week1-r-intro-slides.html#dataframes",
    "title": "Introduction to R",
    "section": "Dataframes",
    "text": "Dataframes\n\nThe data.frame class is a “tightly coupled collection of variables”; it’s also a fundamental data structure in R.\n\n\nLike a matrix, but with labeled columns of the same length.\nEach column corresponds to a vector of values (numbers, characters, etc.).\nSupports many useful operations.\nAnalogous to pandas.DataFrame in Python!\n\n\n\n\n\nNote\n\n\nNote that once we move to the tidyverse, we’ll be working with tibbles, which are basically like a data.frame."
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#creating-a-data.frame",
    "href": "lectures/week1-r-intro-slides.html#creating-a-data.frame",
    "title": "Introduction to R",
    "section": "Creating a data.frame",
    "text": "Creating a data.frame\n\nA data.frame can be created using the data.frame function.\nPass in labeled vectors of the same length.\n\n\ndf_example = data.frame(hours_studied = c(0, 2, 2, 3, 5, 8), test_score = c(70, 85, 89, 89, 94, 95))\nhead(df_example, 2)\n\n\n\n  hours_studied test_score\n1             0         70\n2             2         85\n\n\n\n\n\n\n💭 Check-in\n\n\nTry creating your own data.frame with custom columns. For example, one column could be movie_title and another could be your rating of that movie. Make sure the columns are the same length!"
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#exploring-a-data.frame",
    "href": "lectures/week1-r-intro-slides.html#exploring-a-data.frame",
    "title": "Introduction to R",
    "section": "Exploring a data.frame",
    "text": "Exploring a data.frame\nWe can use functions like nrow, head, and colnames to learn about our data.frame.\n\nprint(nrow(df_example)) ### How many rows?\n\n\n\n[1] 6\n\nprint(colnames(df_example)) ### Column names\n\n[1] \"hours_studied\" \"test_score\"   \n\nprint(head(df_example, 2)) ### First two rows\n\n  hours_studied test_score\n1             0         70\n2             2         85\n\nprint(str(df_example)) ### Structure of data\n\n'data.frame':   6 obs. of  2 variables:\n $ hours_studied: num  0 2 2 3 5 8\n $ test_score   : num  70 85 89 89 94 95\nNULL\n\nprint(summary(df_example)) ### Summary of each column\n\n hours_studied     test_score   \n Min.   :0.000   Min.   :70.00  \n 1st Qu.:2.000   1st Qu.:86.00  \n Median :2.500   Median :89.00  \n Mean   :3.333   Mean   :87.00  \n 3rd Qu.:4.500   3rd Qu.:92.75  \n Max.   :8.000   Max.   :95.00"
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#accessing-individual-columns",
    "href": "lectures/week1-r-intro-slides.html#accessing-individual-columns",
    "title": "Introduction to R",
    "section": "Accessing individual columns",
    "text": "Accessing individual columns\nYou can access individual columns using the dataframe$column_name syntax.\n\ndf_example$hours_studied ### Get vector\n\n\n\n[1] 0 2 2 3 5 8\n\nsummary(df_example$hours_studied) ### Get summary of vector\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   2.000   2.500   3.333   4.500   8.000"
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#filtering-a-data.frame",
    "href": "lectures/week1-r-intro-slides.html#filtering-a-data.frame",
    "title": "Introduction to R",
    "section": "Filtering a data.frame",
    "text": "Filtering a data.frame\nIn base R, you can filter a data.frame using the df[CONDITION] syntax, where CONDITION corresponds to a logical statement.\n\ndf_example[df_example$hours_studied &gt; 2, ]\n\n\n\n  hours_studied test_score\n4             3         89\n5             5         94\n6             8         95\n\n\n\n\n\n\nNote\n\n\nIn the tidyverse, we can use the handy filter function."
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#simple-bivariate-plots",
    "href": "lectures/week1-r-intro-slides.html#simple-bivariate-plots",
    "title": "Introduction to R",
    "section": "Simple bivariate plots",
    "text": "Simple bivariate plots\nOnce you have multiple vectors, you can plot the relationship between them, e.g., using a simple scatterplot.\n\nplot(df_example$hours_studied, \n     df_example$test_score,\n     xlab = \"Hours Studied\", \n     ylab = \"Test Score\",\n     pch = 16, # Filled circles\n     col = \"blue\")"
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#calculating-correlations",
    "href": "lectures/week1-r-intro-slides.html#calculating-correlations",
    "title": "Introduction to R",
    "section": "Calculating correlations",
    "text": "Calculating correlations\nYou can also quantify the relationship between variables, e.g., using a Pearson’s r correlation coefficient.\n\ncor.test(df_example$hours_studied, df_example$test_score)\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  df_example$hours_studied and df_example$test_score\nt = 2.8958, df = 4, p-value = 0.0443\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.03391115 0.97998161\nsample estimates:\n      cor \n0.8228274"
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#working-with-missing-data",
    "href": "lectures/week1-r-intro-slides.html#working-with-missing-data",
    "title": "Introduction to R",
    "section": "Working with missing data",
    "text": "Working with missing data\nReal data often contains missing values. R represents these as NA (Not Available). We’ll discuss these in more detail next week, but here’s a preview:\n\n# Vector with missing data\nsurvey_responses &lt;- c(85, 92, NA, 78, NA, 88)\nmean(survey_responses)                    # Returns NA!\n\n\n\n[1] NA\n\nmean(survey_responses, na.rm = TRUE)      # Remove NAs first: 85.75\n\n[1] 85.75"
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#working-with-missing-data-pt.-2",
    "href": "lectures/week1-r-intro-slides.html#working-with-missing-data-pt.-2",
    "title": "Introduction to R",
    "section": "Working with missing data (pt. 2)",
    "text": "Working with missing data (pt. 2)\nYou can remove missing data by filtering the data.frame, using the syntax below and the is.na condition.\n\n# Vector with missing data\nsurvey_responses[is.na(survey_responses) == FALSE]\n\n\n\n[1] 85 92 78 88"
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#putting-it-together-simulating-data",
    "href": "lectures/week1-r-intro-slides.html#putting-it-together-simulating-data",
    "title": "Introduction to R",
    "section": "Putting it together: simulating data",
    "text": "Putting it together: simulating data\nSo far, we’ve discussed a number of useful concepts in R:\n\nWorking with vectors.\nSimulating random distributions using rnorm.\nCreating data.frame objects and plotting or analyzing them.\n\n\n\n\n\n💭 Check-in\n\n\nNow, let’s simulate data.\n\nFirst, use rnorm to create a random normal distribution of parent_heights (use parameters that seem reasonable to you).\nThen, create a second variable called child_heights that’s related to parent_heights, ideally with some random error added. (Hint: Think about how a regression line works.).\nPut these variables in a data.frame.\nFinally, plot the relationship between those variables and calculate the correlation."
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#simulating-data",
    "href": "lectures/week1-r-intro-slides.html#simulating-data",
    "title": "Introduction to R",
    "section": "Simulating data",
    "text": "Simulating data\n\nparent_heights = rnorm(100, 65, 3)\nchild_heights = parent_heights + rnorm(100, 0, 2)\ndf_heights = data.frame(parent_heights, child_heights)\n\nplot(df_heights$parent_heights, \n     df_heights$child_heights,\n     xlab = \"Parent Height\", \n     ylab = \"Child Height\",\n     pch = 16, # Filled circles\n     col = \"blue\")\n\ncor(df_heights$parent_heights, df_heights$child_heights)\n\n[1] 0.888038"
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#a-conceptual-preview-of-the-tidyverse",
    "href": "lectures/week1-r-intro-slides.html#a-conceptual-preview-of-the-tidyverse",
    "title": "Introduction to R",
    "section": "A conceptual preview of the tidyverse",
    "text": "A conceptual preview of the tidyverse\nNext week, we’ll discuss the tidyverse: a set of packages and functions developed to make data analysis and visualization in R easier.\nThis includes (but is not limited to):\n\nFunctions for transforming data, e.g., filter or mutate.\nFunctions for merging data, like left_join or inner_join.\nFunctions for visualizing data, like ggplot."
  },
  {
    "objectID": "lectures/week1-r-intro-slides.html#lecture-wrap-up",
    "href": "lectures/week1-r-intro-slides.html#lecture-wrap-up",
    "title": "Introduction to R",
    "section": "Lecture wrap-up",
    "text": "Lecture wrap-up\nThis course is not primarily about programming in R, but programming in R is a foundational skill for other parts of this course.\nThis lecture (and accompanying lab) is intended to give you more comfort with the following concepts:\n\nWorking with variables and different types of data.\nCreating and working with vectors.\n\nSimple plotting.\nWorking with data.frame objects."
  },
  {
    "objectID": "lectures/week1-philosophy.html",
    "href": "lectures/week1-philosophy.html",
    "title": "The problem of knowledge",
    "section": "",
    "text": "What is knowledge and how do we produce it?\nPerspectives on scientific knowledge.\nEpistemological and ethical challenges facing CSS."
  },
  {
    "objectID": "lectures/week1-philosophy.html#goals-of-the-lecture",
    "href": "lectures/week1-philosophy.html#goals-of-the-lecture",
    "title": "The problem of knowledge",
    "section": "",
    "text": "What is knowledge and how do we produce it?\nPerspectives on scientific knowledge.\nEpistemological and ethical challenges facing CSS."
  },
  {
    "objectID": "lectures/week1-philosophy.html#how-do-we-learn-about-the-world",
    "href": "lectures/week1-philosophy.html#how-do-we-learn-about-the-world",
    "title": "The problem of knowledge",
    "section": "How do we learn about the world?",
    "text": "How do we learn about the world?\nThere are many ways to construct knowledge.\n\n\n\nSubjective experience\n\n\n\nIntuition and reason\n\n\n\nEmpirical research and measurement"
  },
  {
    "objectID": "lectures/week1-philosophy.html#why-not-just-subjective-experience",
    "href": "lectures/week1-philosophy.html#why-not-just-subjective-experience",
    "title": "The problem of knowledge",
    "section": "Why not just subjective experience?",
    "text": "Why not just subjective experience?\n\nSubjective experience refers to the thoughts and experiences an individual has.\n\n\n\nFor many things, no substitute for personal experience.\n“Anecdotal” evidence can still function as a kind of evidence.\nCultural knowledge (rituals, customs, etc.) also functions as a kind of evolved wisdom.\nBut over-reliance on anecdotes can also lead us astray when it comes to building models of the world."
  },
  {
    "objectID": "lectures/week1-philosophy.html#why-not-just-intuition-and-reason",
    "href": "lectures/week1-philosophy.html#why-not-just-intuition-and-reason",
    "title": "The problem of knowledge",
    "section": "Why not just intuition and reason?",
    "text": "Why not just intuition and reason?\n\nIntuition and reason rely on logic and abstract thinking to understand the world.\n\n\n\nRationalist tradition: knowledge comes from reasoning logically.\nMathematics and formal logic are powerful tools for understanding\nBut reason alone can lead to elegant theories that seem empirically wrong\n\nSometimes absurdly so, e.g., Zeno’s paradox\nSometimes empiricism is intuitive, e.g., Galileo’s work on gravity."
  },
  {
    "objectID": "lectures/week1-philosophy.html#the-rise-of-empirical-science",
    "href": "lectures/week1-philosophy.html#the-rise-of-empirical-science",
    "title": "The problem of knowledge",
    "section": "The rise of empirical science",
    "text": "The rise of empirical science\n\nEmpiricism is the idea that knowledge comes primarily from direct sensory experience and observation.\n\n\n\nCombines experience-based approach with systematic measurement and experimentation.\nOver time, systematic observations can help form theories.\nIn turn, theories guide observation.\nBut how exactly should we do empirical science? Many different philosophies…\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWhat’s your intuitive theory of how science works (or should work)?"
  },
  {
    "objectID": "lectures/week1-philosophy.html#empiricism-various-philosophies",
    "href": "lectures/week1-philosophy.html#empiricism-various-philosophies",
    "title": "The problem of knowledge",
    "section": "Empiricism: various philosophies",
    "text": "Empiricism: various philosophies\nHistorically, scientists and philosophers of science have made different arguments about how science works, both descriptively and prescriptively.\n\n\n\nLogical positivism\n\nStatements are meaningful if and only if they can be verified.\nProblem: Many statements can’t be positively verified; problem of induction.\n\n\n\n\nFalsification\n\nTheories cannot be proven, but can be proven wrong.\nE.g., a single black swan can disprove the claim: All swans are white.\nProblem: Science doesn’t always work this way…\n\n\n\n\nBeyond falsification\n\nScience operates under paradigms or research programmes.\nPragmatism: Focus on producing useful explanations."
  },
  {
    "objectID": "lectures/week1-philosophy.html#so-what-makes-a-good-explanation",
    "href": "lectures/week1-philosophy.html#so-what-makes-a-good-explanation",
    "title": "The problem of knowledge",
    "section": "So what makes a good explanation?",
    "text": "So what makes a good explanation?\n\nOne goal of science is to produce explanations of natural phenomena.\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWhat do you think makes a good explanation?\n\n\n\n\n\nCovering law: show how phenomena emerge from general principles\nCausal-mechanical: identify manipulable causes and trace mechanisms\n\nPragmatic: serve the practical needs of the question-asker\nAlso connects to deep questions about what constitutes “scientific understanding” of a phenomenon."
  },
  {
    "objectID": "lectures/week1-philosophy.html#prediction-vs.-understanding",
    "href": "lectures/week1-philosophy.html#prediction-vs.-understanding",
    "title": "The problem of knowledge",
    "section": "Prediction vs. understanding",
    "text": "Prediction vs. understanding\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nDoes being able to predict something mean we understand it?\n\n\n\n\n\nIncreasingly, machine learning models can make accurate predictions…\n…but we don’t always know why the model works.\n\nA model that predicts which word you’ll say doesn’t mean we know why you said that word.\n\nYet simpler, more interpretable models sometimes trade-off with predictive accuracy.\nThis complexity/accuracy trade-off is pervasive in statistical modeling.\nIt also connects to more general challenges facing CSS."
  },
  {
    "objectID": "lectures/week1-philosophy.html#epistemological-challenges-in-css",
    "href": "lectures/week1-philosophy.html#epistemological-challenges-in-css",
    "title": "The problem of knowledge",
    "section": "Epistemological challenges in CSS",
    "text": "Epistemological challenges in CSS\n\nAny given empirical claim can be evaluated according to several validities.\n\nHere, we’ll focus on these validities with respect to CSS specifically.\n\n\n\n\nConstruct validity\n\nAre we measuring what we think we’re measuring?\n\n\n\n\nInternal validity\n\nCan we establish causal relationships?\n\n\n\n\n\n\n\nStatistical validity\n\nAre our analytical methods appropriate?\n\n\n\n\nExternal validity\n\nDo our findings generalize beyond our sample?"
  },
  {
    "objectID": "lectures/week1-philosophy.html#construct-validity-in-css",
    "href": "lectures/week1-philosophy.html#construct-validity-in-css",
    "title": "The problem of knowledge",
    "section": "Construct validity in CSS",
    "text": "Construct validity in CSS\n\nConstruct validity refers to how well a variable is operationalized.\n\nMany variables are somewhat abstract: how do we measure them?\n\n\n\nExamples of hard constructs to operationalize:\n\nHappiness and well-being.\nSocial connectedness.\nInequality and poverty.\nPolitical polarization.\n\n\n\n\nKey questions:\n\nWhat aspects are we capturing?\n\nWhat are we missing?\n\n\n\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWith a partner, choose one of these constructs. How might you operationalize it? What are limitations to this approach?"
  },
  {
    "objectID": "lectures/week1-philosophy.html#internal-validity-establishing-causation",
    "href": "lectures/week1-philosophy.html#internal-validity-establishing-causation",
    "title": "The problem of knowledge",
    "section": "Internal validity: establishing causation",
    "text": "Internal validity: establishing causation\n\nInternal validity is an indication of a study’s ability to eliminate alternative explanations for the effect of interest.\n\n\n\nWe’ve all heard correlation does not imply causation.\nBest way to establish causation is through experiments (RCTs), but that’s not always possible (or realistic) in many CSS domains.\n\nExample: social media use and mental health; digital campaigning and voter turnout; and much more.\n\nAdditionally, experimental control sometimes trades off with external validity!\nIf design is observational (no experiment), need to account for possible confounds.\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWith a partner, think of some observational CSS studies you’ve read about. What might be alternative explanations for the effect of interest?"
  },
  {
    "objectID": "lectures/week1-philosophy.html#statistical-validity-in-css",
    "href": "lectures/week1-philosophy.html#statistical-validity-in-css",
    "title": "The problem of knowledge",
    "section": "Statistical validity in CSS",
    "text": "Statistical validity in CSS\n\nStatistical validity is the extent to which a study’s statistical conclusions are accurate.\n\n\n\nCould include reporting the margin of error associated with a claim (e.g., \\(10 \\pm 2\\)).\nAlso encompasses common pitfalls.\n\nFlexible models applies to large datasets are a recipe for inadvertent p-hacking.\nImportant to use methods like cross-validation to avoid overfitting.\n\nFundamentally an issue of research ethics!\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nSuppose you analyze the correlation between various personality traits and hundreds of outcome measures (life satisfication, income, etc.). You find significant results for about \\(5 \\%\\) of your analyses. What’s a potential concern here?"
  },
  {
    "objectID": "lectures/week1-philosophy.html#external-validity-who-and-when",
    "href": "lectures/week1-philosophy.html#external-validity-who-and-when",
    "title": "The problem of knowledge",
    "section": "External validity: Who and when?",
    "text": "External validity: Who and when?\n\nExternal validity refers to how well a given claim generalizes to the population of interest.\n\n\n\nMuch social science research focuses on WEIRD populations (Western, Educated, Industrialized, Rich, Democratic).\n\nDigital data exhibits even more skew.\n\nHow well do conclusions based on a given sample generalize across populations and times?\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWith a partner, talk about a CSS-related study you’ve read about. How well do you think the conclusions generalize from the sample (people, society, time, etc.) studied?"
  },
  {
    "objectID": "lectures/week1-philosophy.html#ethical-challenges-in-css",
    "href": "lectures/week1-philosophy.html#ethical-challenges-in-css",
    "title": "The problem of knowledge",
    "section": "Ethical challenges in CSS",
    "text": "Ethical challenges in CSS\nCSS research involves many important ethical questions.\n\n\nPrivacy and consent.\nAlgorithmic bias.\nResearch ethics (reproducibility, etc.)."
  },
  {
    "objectID": "lectures/week1-philosophy.html#summary-and-moving-forward",
    "href": "lectures/week1-philosophy.html#summary-and-moving-forward",
    "title": "The problem of knowledge",
    "section": "Summary, and moving forward",
    "text": "Summary, and moving forward\n\n\nCSS is pluralistic in terms of methods and research questions: no single “correct” approach.\n\nMultiple epistemological challenges facing empirical science.\nThis course will focus on statistical methods, but we’ll also touch on other core issues, especially construct validity.\n\n\n\n\n\n\n\n\n\nNote💭 Key takeaway\n\n\n\nProducing knowledge is hard, but methodological and theoretical principles can help guide us."
  },
  {
    "objectID": "lectures/week1-phil-slides.html#goals-of-the-lecture",
    "href": "lectures/week1-phil-slides.html#goals-of-the-lecture",
    "title": "The problem of knowledge",
    "section": "Goals of the lecture",
    "text": "Goals of the lecture\n\nWhat is knowledge and how do we produce it?\nPerspectives on scientific knowledge.\nEpistemological and ethical challenges facing CSS."
  },
  {
    "objectID": "lectures/week1-phil-slides.html#how-do-we-learn-about-the-world",
    "href": "lectures/week1-phil-slides.html#how-do-we-learn-about-the-world",
    "title": "The problem of knowledge",
    "section": "How do we learn about the world?",
    "text": "How do we learn about the world?\nThere are many ways to construct knowledge.\n\n\n\nSubjective experience\n\n\n\nIntuition and reason\n\n\n\nEmpirical research and measurement"
  },
  {
    "objectID": "lectures/week1-phil-slides.html#why-not-just-subjective-experience",
    "href": "lectures/week1-phil-slides.html#why-not-just-subjective-experience",
    "title": "The problem of knowledge",
    "section": "Why not just subjective experience?",
    "text": "Why not just subjective experience?\n\nSubjective experience refers to the thoughts and experiences an individual has.\n\n\nFor many things, no substitute for personal experience.\n“Anecdotal” evidence can still function as a kind of evidence.\nCultural knowledge (rituals, customs, etc.) also functions as a kind of evolved wisdom.\nBut over-reliance on anecdotes can also lead us astray when it comes to building models of the world."
  },
  {
    "objectID": "lectures/week1-phil-slides.html#why-not-just-intuition-and-reason",
    "href": "lectures/week1-phil-slides.html#why-not-just-intuition-and-reason",
    "title": "The problem of knowledge",
    "section": "Why not just intuition and reason?",
    "text": "Why not just intuition and reason?\n\nIntuition and reason rely on logic and abstract thinking to understand the world.\n\n\nRationalist tradition: knowledge comes from reasoning logically.\nMathematics and formal logic are powerful tools for understanding\nBut reason alone can lead to elegant theories that seem empirically wrong\n\nSometimes absurdly so, e.g., Zeno’s paradox\nSometimes empiricism is intuitive, e.g., Galileo’s work on gravity."
  },
  {
    "objectID": "lectures/week1-phil-slides.html#the-rise-of-empirical-science",
    "href": "lectures/week1-phil-slides.html#the-rise-of-empirical-science",
    "title": "The problem of knowledge",
    "section": "The rise of empirical science",
    "text": "The rise of empirical science\n\nEmpiricism is the idea that knowledge comes primarily from direct sensory experience and observation.\n\n\nCombines experience-based approach with systematic measurement and experimentation.\nOver time, systematic observations can help form theories.\nIn turn, theories guide observation.\nBut how exactly should we do empirical science? Many different philosophies…\n\n\n\n\n\n💭 Check-in\n\n\nWhat’s your intuitive theory of how science works (or should work)?"
  },
  {
    "objectID": "lectures/week1-phil-slides.html#empiricism-various-philosophies",
    "href": "lectures/week1-phil-slides.html#empiricism-various-philosophies",
    "title": "The problem of knowledge",
    "section": "Empiricism: various philosophies",
    "text": "Empiricism: various philosophies\nHistorically, scientists and philosophers of science have made different arguments about how science works, both descriptively and prescriptively.\n\n\n\nLogical positivism\n\nStatements are meaningful if and only if they can be verified.\nProblem: Many statements can’t be positively verified; problem of induction.\n\n\n\n\nFalsification\n\nTheories cannot be proven, but can be proven wrong.\nE.g., a single black swan can disprove the claim: All swans are white.\nProblem: Science doesn’t always work this way…\n\n\n\n\nBeyond falsification\n\nScience operates under paradigms or research programmes.\nPragmatism: Focus on producing useful explanations."
  },
  {
    "objectID": "lectures/week1-phil-slides.html#so-what-makes-a-good-explanation",
    "href": "lectures/week1-phil-slides.html#so-what-makes-a-good-explanation",
    "title": "The problem of knowledge",
    "section": "So what makes a good explanation?",
    "text": "So what makes a good explanation?\n\nOne goal of science is to produce explanations of natural phenomena.\n\n\n\n\n\n💭 Check-in\n\n\nWhat do you think makes a good explanation?\n\n\n\n\n\nCovering law: show how phenomena emerge from general principles\nCausal-mechanical: identify manipulable causes and trace mechanisms\n\nPragmatic: serve the practical needs of the question-asker\nAlso connects to deep questions about what constitutes “scientific understanding” of a phenomenon."
  },
  {
    "objectID": "lectures/week1-phil-slides.html#prediction-vs.-understanding",
    "href": "lectures/week1-phil-slides.html#prediction-vs.-understanding",
    "title": "The problem of knowledge",
    "section": "Prediction vs. understanding",
    "text": "Prediction vs. understanding\n\n\n\n\n💭 Check-in\n\n\nDoes being able to predict something mean we understand it?\n\n\n\n\n\nIncreasingly, machine learning models can make accurate predictions…\n…but we don’t always know why the model works.\n\nA model that predicts which word you’ll say doesn’t mean we know why you said that word.\n\nYet simpler, more interpretable models sometimes trade-off with predictive accuracy.\nThis complexity/accuracy trade-off is pervasive in statistical modeling.\nIt also connects to more general challenges facing CSS."
  },
  {
    "objectID": "lectures/week1-phil-slides.html#epistemological-challenges-in-css",
    "href": "lectures/week1-phil-slides.html#epistemological-challenges-in-css",
    "title": "The problem of knowledge",
    "section": "Epistemological challenges in CSS",
    "text": "Epistemological challenges in CSS\n\nAny given empirical claim can be evaluated according to several validities.\n\nHere, we’ll focus on these validities with respect to CSS specifically.\n\n\n\n\nConstruct validity\n\nAre we measuring what we think we’re measuring?\n\n\n\n\nInternal validity\n\nCan we establish causal relationships?\n\n\n\n\n\n\nStatistical validity\n\nAre our analytical methods appropriate?\n\n\n\n\nExternal validity\n\nDo our findings generalize beyond our sample?"
  },
  {
    "objectID": "lectures/week1-phil-slides.html#construct-validity-in-css",
    "href": "lectures/week1-phil-slides.html#construct-validity-in-css",
    "title": "The problem of knowledge",
    "section": "Construct validity in CSS",
    "text": "Construct validity in CSS\n\nConstruct validity refers to how well a variable is operationalized.\n\nMany variables are somewhat abstract: how do we measure them?\n\n\n\nExamples of hard constructs to operationalize:\n\nHappiness and well-being.\nSocial connectedness.\nInequality and poverty.\nPolitical polarization.\n\n\n\n\nKey questions:\n\nWhat aspects are we capturing?\n\nWhat are we missing?\n\n\n\n\n\n\n\n\n💭 Check-in\n\n\nWith a partner, choose one of these constructs. How might you operationalize it? What are limitations to this approach?"
  },
  {
    "objectID": "lectures/week1-phil-slides.html#internal-validity-establishing-causation",
    "href": "lectures/week1-phil-slides.html#internal-validity-establishing-causation",
    "title": "The problem of knowledge",
    "section": "Internal validity: establishing causation",
    "text": "Internal validity: establishing causation\n\nInternal validity is an indication of a study’s ability to eliminate alternative explanations for the effect of interest.\n\n\nWe’ve all heard correlation does not imply causation.\nBest way to establish causation is through experiments (RCTs), but that’s not always possible (or realistic) in many CSS domains.\n\nExample: social media use and mental health; digital campaigning and voter turnout; and much more.\n\nAdditionally, experimental control sometimes trades off with external validity!\nIf design is observational (no experiment), need to account for possible confounds.\n\n\n\n\n\n💭 Check-in\n\n\nWith a partner, think of some observational CSS studies you’ve read about. What might be alternative explanations for the effect of interest?"
  },
  {
    "objectID": "lectures/week1-phil-slides.html#statistical-validity-in-css",
    "href": "lectures/week1-phil-slides.html#statistical-validity-in-css",
    "title": "The problem of knowledge",
    "section": "Statistical validity in CSS",
    "text": "Statistical validity in CSS\n\nStatistical validity is the extent to which a study’s statistical conclusions are accurate.\n\n\nCould include reporting the margin of error associated with a claim (e.g., \\(10 \\pm 2\\)).\nAlso encompasses common pitfalls.\n\nFlexible models applies to large datasets are a recipe for inadvertent p-hacking.\nImportant to use methods like cross-validation to avoid overfitting.\n\nFundamentally an issue of research ethics!\n\n\n\n\n\n💭 Check-in\n\n\nSuppose you analyze the correlation between various personality traits and hundreds of outcome measures (life satisfication, income, etc.). You find significant results for about \\(5 \\%\\) of your analyses. What’s a potential concern here?"
  },
  {
    "objectID": "lectures/week1-phil-slides.html#external-validity-who-and-when",
    "href": "lectures/week1-phil-slides.html#external-validity-who-and-when",
    "title": "The problem of knowledge",
    "section": "External validity: Who and when?",
    "text": "External validity: Who and when?\n\nExternal validity refers to how well a given claim generalizes to the population of interest.\n\n\nMuch social science research focuses on WEIRD populations (Western, Educated, Industrialized, Rich, Democratic).\n\nDigital data exhibits even more skew.\n\nHow well do conclusions based on a given sample generalize across populations and times?\n\n\n\n\n\n💭 Check-in\n\n\nWith a partner, talk about a CSS-related study you’ve read about. How well do you think the conclusions generalize from the sample (people, society, time, etc.) studied?"
  },
  {
    "objectID": "lectures/week1-phil-slides.html#ethical-challenges-in-css",
    "href": "lectures/week1-phil-slides.html#ethical-challenges-in-css",
    "title": "The problem of knowledge",
    "section": "Ethical challenges in CSS",
    "text": "Ethical challenges in CSS\nCSS research involves many important ethical questions.\n\nPrivacy and consent.\nAlgorithmic bias.\nResearch ethics (reproducibility, etc.)."
  },
  {
    "objectID": "lectures/week1-phil-slides.html#summary-and-moving-forward",
    "href": "lectures/week1-phil-slides.html#summary-and-moving-forward",
    "title": "The problem of knowledge",
    "section": "Summary, and moving forward",
    "text": "Summary, and moving forward\n\nCSS is pluralistic in terms of methods and research questions: no single “correct” approach.\n\nMultiple epistemological challenges facing empirical science.\nThis course will focus on statistical methods, but we’ll also touch on other core issues, especially construct validity.\n\n\n\n\n\n💭 Key takeaway\n\n\nProducing knowledge is hard, but methodological and theoretical principles can help guide us."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "CSS 211 Syllabus",
    "section": "",
    "text": "This course is an introduction to statistical methods and computational tools for computational social science (CSS) research. Students will develop proficiency in R programming while learning foundational concepts for working with CSS data, including: data wrangling and visualization, regression, and model selection. The course emphasizes hands-on application of techniques to real-world datasets, as well as discussion of ongoing methodological and epistemological issues in the study of human behavior."
  },
  {
    "objectID": "syllabus.html#course-overview",
    "href": "syllabus.html#course-overview",
    "title": "CSS 211 Syllabus",
    "section": "",
    "text": "This course is an introduction to statistical methods and computational tools for computational social science (CSS) research. Students will develop proficiency in R programming while learning foundational concepts for working with CSS data, including: data wrangling and visualization, regression, and model selection. The course emphasizes hands-on application of techniques to real-world datasets, as well as discussion of ongoing methodological and epistemological issues in the study of human behavior."
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "CSS 211 Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nMy goal is that by the end of this course, students will be able to:\n\nDefine and explain key concepts in statistical inference and regression analysis.\nIdentify appropriate visualizations and statistical methods for different kinds of research questions and datasets.\nImplement data wrangling, visualization, and analysis workflows in R.\nInterpret and evaluate results (visualizations, fit models, etc.) in the context of a research question.\nDesign and implement a complete statistical analysis project from research question to interpretation."
  },
  {
    "objectID": "syllabus.html#teaching-team",
    "href": "syllabus.html#teaching-team",
    "title": "CSS 211 Syllabus",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\nName\nRole\nOH Time\nEmail\n\n\n\n\nSean Trott\nInstructor\nFriday 1-2pm (CSB 259)\nsttrott@ucsd.edu\n\n\nXiaohan Wu\nTA\nMonday 11am-12pm (SSB 104)\nxiw021@ucsd.edu"
  },
  {
    "objectID": "syllabus.html#prerequisites",
    "href": "syllabus.html#prerequisites",
    "title": "CSS 211 Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\nStudents should have a basic understanding of probability theory and statistics; some experience with a programming language (e.g., Python or R) is encouraged but not required."
  },
  {
    "objectID": "syllabus.html#grading-and-assessments",
    "href": "syllabus.html#grading-and-assessments",
    "title": "CSS 211 Syllabus",
    "section": "Grading and Assessments",
    "text": "Grading and Assessments\n\n\n\nAssessment\nPercentage\n\n\n\n\nLabs\n20%\n\n\nConcept quizzes\n20%\n\n\nMidterm\n25%\n\n\nFinal project\n35%\n\n\n\nLabs (20% total): 4 coding labs, completed in R and submitted via Canvas. Students may collaborate on labs with each other and seek help from their TA/instructor.\nConcept quizzes (20%): 4 short quizzes, held over Canvas, designed to test and improve conceptual intuitions about course topics.\nMidterm (25%): A paper midterm held in-class testing knowledge of course material.\nFinal project (35%): A rigorous replication (and possible extension) of existing empirical work (published or otherwise) in a student’s discipline of interest. Final projects should be completed independently; students will turn in both a written report and present their work in a short in-class presentation the final week of class. (A rubric will be provided separately.)\n\nRounding\nMy course policy is not to round up grades for two reasons:\n\nIf rounding is applied selectively (i.e., only to students who ask), it is unfair to other students.\n\nIf rounding is applied across the board, it simply redefines the boundary between two letter grades (e.g., making an 89% the cut-off for an A-)."
  },
  {
    "objectID": "syllabus.html#using-ai",
    "href": "syllabus.html#using-ai",
    "title": "CSS 211 Syllabus",
    "section": "Using AI",
    "text": "Using AI\nTools like ChatGPT can be incredibly helpful for learning to code (or speeding up the process). At the same time, I think it’s crucial to learn the foundational skills and concepts that will allow you to use these tools to their full potential. That is the main motivation for having an in-class midterm: ultimately, I cannot force anyone to complete homework on their own without using ChatGPT (or equivalent tools), and I wouldn’t even claim that people shouldn’t consult ChatGPT for additional help or questions; but in order to perform well on the midterm, you will still need to learn the concepts."
  },
  {
    "objectID": "final_project.html",
    "href": "final_project.html",
    "title": "CSS 211 Final Project)",
    "section": "",
    "text": "The primary goal of the final project is to reproduce the analysis and results from a published paper (or preprint) in your area of interest. Alternatively/additionally, students are allowed (but not required) to conduct an original analysis on a publicly available dataset (or multiple datasets)."
  },
  {
    "objectID": "final_project.html#project-guidelines-and-expectations",
    "href": "final_project.html#project-guidelines-and-expectations",
    "title": "CSS 211 Final Project)",
    "section": "Project guidelines and expectations",
    "text": "Project guidelines and expectations\nYour deliverable will be a report (and short presentation, delivered in class). The report (30 points total) should consist of the following sections (roughly):\n\n\n\nSection\nPoints\nDescription\nExample 1\n\n\n\n\nIntroduction\n3\nWhat dataset are you looking at? Where/how was it created? What was the original paper your analysis is based on?\nDataset about which construction people use in the dative alternation: do they use NP (“She gave the man the box”) or PP (“She gave the box to the man”). I will ask which features predict the use of one construction vs. the other. Originally published in Bresnan et al. (2007).\n\n\nData\n8\nDescriptive statistics about the dataset: number of rows/columns, central tendency (mean/median/mode) of key variables, variability of key variables, any missing values, etc. Should also include details of cleaning or merging datasets, should you need to do that.\nThe dativeSimplified dataset contains 903 observations with 5 variables; it was created by examining transcriptions of conversational data from Switchboard. No cleaning was required.\n\n\nVisualizations\n8\nReproduction of figures from original paper; alternatively, 2-3 original visualizations showing specific patterns or features you’d like to highlight. Each visualization should be accompanied by a short (1-2 sentences) description of what you think it shows.\nBoxplot showing length of the theme argument when recipient is realized as a noun phrase vs. prepositional phrase. Barplot showing proportion of NP realizations depending on animacy of recipient.\n\n\nAnalyses\n8\nReproduction of analyses from original paper; alternatively, 2-3 original analyses using methods discussed in class (e.g., linear regression, logistic regression etc.) to address your question. Each analysis should be accompanied by a short (1-3 sentences) interpretation. Should also include evaluation of your model somehow, e.g., \\(R^2\\), AIC, etc.\nLogistic regression predicting realization (NP vs. PP) from Animacy and Length. Compare AIC of this model to a model omitting each variable in turn.\n\n\nLimitations\n2\nDiscuss any limitations to your approach. If it is a reproduction of published work, discuss any discrepancies; if it’s original work, discuss any issues with your decisions.\nVariables could be inter-related; also only 4 predictor variables total.\n\n\nConclusion\n1\nDrawing a conclusion about the dataset and the questions you posed.\nAn NP realization is more likely for longer themes.\n\n\n\nThe presentation will constitute an additional 5 points of your final project grade. Presentations will be relatively short (~5 minutes), so the key thing will be to summarize the research question(s), display a key figure, and walk through an analysis."
  },
  {
    "objectID": "final_project.html#advice-for-finding-a-result-to-reproduce",
    "href": "final_project.html#advice-for-finding-a-result-to-reproduce",
    "title": "CSS 211 Final Project)",
    "section": "Advice for finding a result to reproduce",
    "text": "Advice for finding a result to reproduce\nFinding a result to reproduce can be the hardest part, since it depends on finding papers with publicly available datasets. I recommend working iteratively here:\n\nStart by finding some papers that interest you (and have analyses you think you can reproduce).\nDetermine whether any of them have publicly available data. 3a. If they do, choose one of those papers. 3b. If they don’t, return to (1).\n\nMore advice on the search process is below.\n\nStart with recent, computational papers\nLook for papers published in the last 3-5 years that use computational methods, statistical analysis, or data visualization. Recent papers are more likely to have accessible data and code.\n\n\nCheck for open science practices\nPrioritize papers that include:\n\nLinks to datasets (often in supplementary materials or data repositories)\nCode repositories (GitHub, OSF, etc.)\nClear methodology sections that explain analytical steps\nPapers with “reproducibility” badges or statements\n\n\n\nGood places to search:\n\nJournal-specific repositories (e.g., PLOS ONE, Behavior Research Methods, Nature Scientific Data)\nPreprint servers (arXiv, bioRxiv, SocArXiv) often have more accessible materials\nYour course readings or papers cited in class\nPapers from faculty in your department\n\n\n\nConsider scope\nChoose a paper where you can reproduce:\n\nAnywhere from 2-4 specific analyses or figures.\nResults using a subset of the data.\nKey findings using similar methods on comparable data"
  },
  {
    "objectID": "final_project.html#advice-for-finding-datasets",
    "href": "final_project.html#advice-for-finding-datasets",
    "title": "CSS 211 Final Project)",
    "section": "Advice for finding datasets",
    "text": "Advice for finding datasets\nSome students may wish to conduct an original analysis on a publicly available dataset. This can be more challenging, since it requires coming up with an original research question of theoretical interest. But if you’re having trouble identifying a suitable paper (see above), you can always go this route.\nHere, your strategy will involve finding a suitable dataset or multiple datasets that you want to combine. You’ll want to make sure this is accessible in a format you can read in and work with (e.g., a csv file).\nSome useful starting points:\n\n\n\nDataset\nSocial Science Domain\nDescription\nAccessing\n\n\n\n\nWorld Bank Open Data\nEconomics / Global Development\nContains time series data for many domains, such as agricultural development, rural poverty, carbon emissions, and much, much more.\nLink to Data Bank; can browse by “indicator”; may require merging datasets for more information.\n\n\nWorld Happiness Report\nEconomics / Global Development\nDataset about global happiness scores; might need to be merged with other datasets to ask useful questions.\nKaggle\n\n\nWorld Energy Consumption\nEconomics / Climate\nContains time series data about consumption of energy and electricity.\nLink on Kaggle\n\n\nSCARFS (Spontaneous, controlled, acts of reference between friends and strangers)\nLinguistics/Communication\nData about friends and strangers playing the game Taboo, which clues they gave, and whether a trial was correct.\nGitHub Link\n\n\nLinguistic norms\nLinguistics/Communication\nMany psycholinguistic norm datasets, including concreteness, age of acquisition, iconicity, and more, are available online (including for this class!)\nConcreteness paper, Iconicity paper\n\n\nCalifornia Housing Prices\nEconomics\nInformation about the median house value for different districts in California.\nLink on Kaggle.\n\n\nStudent alcohol consumption\nPublic Health\nInformation about student behavior, including alcohol consumption and more.\nLink on Kaggle."
  },
  {
    "objectID": "hands_on/week4.html",
    "href": "hands_on/week4.html",
    "title": "Week 4: Regression",
    "section": "",
    "text": "The goal of this hands-on exercise is to familiarize you with building and interpreting regression models in R.\nWe’ll be working (again) with the CLEAR Corpus, a dataset of text excerpts rated for their readability (Crossley et al., 2021)."
  },
  {
    "objectID": "hands_on/week4.html#introduction",
    "href": "hands_on/week4.html#introduction",
    "title": "Week 4: Regression",
    "section": "",
    "text": "The goal of this hands-on exercise is to familiarize you with building and interpreting regression models in R.\nWe’ll be working (again) with the CLEAR Corpus, a dataset of text excerpts rated for their readability (Crossley et al., 2021)."
  },
  {
    "objectID": "hands_on/week4.html#load-dataset",
    "href": "hands_on/week4.html#load-dataset",
    "title": "Week 4: Regression",
    "section": "Load dataset",
    "text": "Load dataset\nTo get started, let’s load the dataset.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggcorrplot)\n\n### Lancaster norms\ndf_clear &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/viz/CLEAR_corpus_final.csv\")\n\nRows: 4724 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): Author, Title, Anthology, URL, Categ, Sub Cat, Lexile Band, Locati...\ndbl (17): ID, Pub Year, MPAA #Max, MPAA# Avg, Google WC, Sentence Count, Par...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe’ve already explored this dataset in past weeks, so we can jump straight to modeling. Our goal is to figure out which predictors are most useful for predicting human judgments of readability (BT_easiness), and how they relate to readability."
  },
  {
    "objectID": "hands_on/week4.html#exercise-1-does-sentence-count-predict-readability",
    "href": "hands_on/week4.html#exercise-1-does-sentence-count-predict-readability",
    "title": "Week 4: Regression",
    "section": "Exercise 1: Does Sentence Count predict readability?",
    "text": "Exercise 1: Does Sentence Count predict readability?\nBuild a linear model predicting BT_easiness from Sentence Count. Call it m_sentence.\n\nWhat is the intercept and slope? Interpret both.\nIs the slope coefficient significant? What does that mean?\nWhat is the \\(R^2\\) of the model? Interpret what it means.\nVisualize the relationship in a scatterplot and draw the regression line on top of it."
  },
  {
    "objectID": "hands_on/week4.html#exercise-2-does-paragraphs-predict-readability",
    "href": "hands_on/week4.html#exercise-2-does-paragraphs-predict-readability",
    "title": "Week 4: Regression",
    "section": "Exercise 2: Does Paragraphs predict readability?",
    "text": "Exercise 2: Does Paragraphs predict readability?\nBuild a linear model predicting BT_easiness from Paragraphs. Call it m_paragraphs.\n\nWhat is the intercept and slope? Interpret both.\nIs the slope coefficient significant? What does that mean?\nWhat is the \\(R^2\\) of the model? Interpret what it means.\nVisualize the relationship in a scatterplot and draw the regression line on top of it."
  },
  {
    "objectID": "hands_on/week4.html#exercise-3-does-flesch-kincaid-grade-level-predict-readability",
    "href": "hands_on/week4.html#exercise-3-does-flesch-kincaid-grade-level-predict-readability",
    "title": "Week 4: Regression",
    "section": "Exercise 3: Does Flesch-Kincaid-Grade-Level predict readability?",
    "text": "Exercise 3: Does Flesch-Kincaid-Grade-Level predict readability?\nBuild a linear model predicting BT_easiness from Flesch-Kincaid-Grade-Level. Call it m_flesch.\n\nWhat is the intercept and slope? Interpret both.\nIs the slope coefficient significant? What does that mean?\nWhat is the \\(R^2\\) of the model? Interpret what it means.\n\nVisualize the relationship in a scatterplot and draw the regression line on top of it."
  },
  {
    "objectID": "hands_on/week4.html#exercise-4-does-categ-predict-readability",
    "href": "hands_on/week4.html#exercise-4-does-categ-predict-readability",
    "title": "Week 4: Regression",
    "section": "Exercise 4: Does Categ predict readability?",
    "text": "Exercise 4: Does Categ predict readability?\nSome texts are categorized as Info and others as Lit. Does this difference covary with differences in readability? Build a model to find out. Call it m_categ.\n\nWhat is the intercept and slope? Interpret both.\nIs the slope coefficient significant? What does that mean?\nWhat is the \\(R^2\\) of the model? Interpret what it means.\n\nQuestion: Would you visualize this in a scatterplot? Or a different kind of plot?"
  },
  {
    "objectID": "hands_on/week4.html#exercise-5-does-location-predict-readability",
    "href": "hands_on/week4.html#exercise-5-does-location-predict-readability",
    "title": "Week 4: Regression",
    "section": "Exercise 5: Does Location predict readability?",
    "text": "Exercise 5: Does Location predict readability?\nDoes the Location in a text from which an excerpt is drawn predict readability? Build a model to find out.\n\nWhat is the intercept and slope(s)? Interpret both.\nIs the slope coefficient significant? What does that mean?\nWhat is the \\(R^2\\) of the model? Interpret what it means.\n\nQuestion: How is this model similar and different from the model with Categ as a predictor? Focus on the number of distinct slope coefficients."
  },
  {
    "objectID": "hands_on/week2.html",
    "href": "hands_on/week2.html",
    "title": "Week 2: Hands-on data wrangling",
    "section": "",
    "text": "The goal of this hands-on exercise is to familiarize you with the basics of data wrangling and descriptive analysis. It will be a “semi-guided session”, with periods for independent (or collaborative) work.\nSpecific topics and skills:\n\nFamiliarity with tools for describing data, e.g., group_by and summarise, as well as simple plots like hist.\nExperience transforming or manipulating data using filter and mutate.\nExperience reshaping and merging datasets using pivot_longer and various joins."
  },
  {
    "objectID": "hands_on/week2.html#introduction",
    "href": "hands_on/week2.html#introduction",
    "title": "Week 2: Hands-on data wrangling",
    "section": "",
    "text": "The goal of this hands-on exercise is to familiarize you with the basics of data wrangling and descriptive analysis. It will be a “semi-guided session”, with periods for independent (or collaborative) work.\nSpecific topics and skills:\n\nFamiliarity with tools for describing data, e.g., group_by and summarise, as well as simple plots like hist.\nExperience transforming or manipulating data using filter and mutate.\nExperience reshaping and merging datasets using pivot_longer and various joins."
  },
  {
    "objectID": "hands_on/week2.html#the-datasets",
    "href": "hands_on/week2.html#the-datasets",
    "title": "Week 2: Hands-on data wrangling",
    "section": "The dataset(s)",
    "text": "The dataset(s)\nWe’ll be working with a few Linguistics datasets:\n\nA dataset about word concreteness, which you’ve already seen from class.\n\nA dataset about word iconicity (the extent to which words sound like what they mean).\n\nTo get started, let’s load the datasets.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n### Concreteness\ndf_concreteness &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/wrangling/concreteness.csv\")\n\nRows: 28612 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Word, Dom_Pos\ndbl (2): Concreteness, Frequency\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n### Iconicity\ndf_iconicity &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/wrangling/iconicity.csv\")\n\nRows: 14774 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): word\ndbl (5): n_ratings, n, prop_known, rating, rating_sd\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "hands_on/week2.html#exercise-1-understand-the-structure-of-your-data",
    "href": "hands_on/week2.html#exercise-1-understand-the-structure-of-your-data",
    "title": "Week 2: Hands-on data wrangling",
    "section": "Exercise 1: Understand the structure of your data",
    "text": "Exercise 1: Understand the structure of your data\nFirst, use functions like nrow, colnames, head, and more to understand the structure and content of each dataset.\nTry to answer the following questions:\n\nWhat does each column mean?\nWould you say the datasets are in wide or long format?\n\nDo the datasets have any overlapping columns?\n\nHow many words are in each dataset? Are they the same words?\n\nWhat’s the range of values for iconicity and concreteness ratings?"
  },
  {
    "objectID": "hands_on/week2.html#exercise-2-join-your-data",
    "href": "hands_on/week2.html#exercise-2-join-your-data",
    "title": "Week 2: Hands-on data wrangling",
    "section": "Exercise 2: Join your data",
    "text": "Exercise 2: Join your data\nUse a join operation to merge the concreteness and iconicity datasets on a common column name. (Hint: You might need to use mutate to create a new column in at least one of the datasets.)\nWhat are the consequences of using left/right/full/inner_join? Which join did you use, and what does the resulting table look like in terms of lost (or gained) rows?"
  },
  {
    "objectID": "hands_on/week2.html#exercise-3-explore-your-data",
    "href": "hands_on/week2.html#exercise-3-explore-your-data",
    "title": "Week 2: Hands-on data wrangling",
    "section": "Exercise 3: Explore your data",
    "text": "Exercise 3: Explore your data\nNow that you’ve merged your datasets, let’s conduct some initial explorations.\n\n3a: How many ratings?\nThe iconicity dataset contains a number of ratings for each word, but some words were rated more times than others.\n\nWhat is the range of n_ratings?\nWhich word was rated the most times? What about the least?\n\n\n\n3b: Which words are most and least iconic?\n\nWhich(s) word has (or have) the highest iconicity rating in the merged dataset? What about the word(s) with the lowest rating? Do these results make sense to you? (Hint: Use slice_max/min).\nWhat about the top 5 iconic words and the bottom 5 iconic words? (Hint: Use arrange and slice_head.)\nCan you draw any conclusions from this preliminary exploration about which kinds of words are most and least iconic?\nAre the extreme words the same before and after merging?\n\n\n\n3c: Which words have the most and least variance in their iconicity ratings?\nRepeat exercise 3b, but for rating_sd instead of rating.\n\n\n3d: Which words are most and least frequent?\nRepeat exercise 3b, but now for Frequency. Which words are the most and least frequent in the joined dataset?\n\n\n3e: Which words are most and least concrete?\nRepeat exercise 3b, but now for Concreteness. Which words are the most and least concrete in the joined dataset?"
  },
  {
    "objectID": "hands_on/week2.html#exercise-4-grouping-and-summarizing",
    "href": "hands_on/week2.html#exercise-4-grouping-and-summarizing",
    "title": "Week 2: Hands-on data wrangling",
    "section": "Exercise 4: Grouping and summarizing",
    "text": "Exercise 4: Grouping and summarizing\nThe dataset also includes information about each word’s part-of-speech. Let’s use that information to learn more about our data.\n\n4a: Which part-of-speech is most and least iconic?\nUse group_by and summarise to calculate the mean iconicity for each part of speech.\n\nWhich part-of-speech has the highest iconicity on average, and which has the least?\n\nIs this broadly consistent with your qualitative inspection of the most and least iconic words earlier?\n\n\n\n4b: Which part-of-speech is most and least concrete?\nUse group_by and summarise to calculate the mean frequency for each part of speech.\n\nWhich part-of-speech has the highest frequency on average, and which has the least?\n\nDo you think these estimates might be affected by the distribution shape of Frequency? If necessary, recalculate the above with a log-transformed measure."
  },
  {
    "objectID": "hands_on/week2.html#exercise-5-correlations-between-our-variables",
    "href": "hands_on/week2.html#exercise-5-correlations-between-our-variables",
    "title": "Week 2: Hands-on data wrangling",
    "section": "Exercise 5: Correlations between our variables",
    "text": "Exercise 5: Correlations between our variables\n\n5a: A correlation matrix\nNow, create a correlation matrix between n_ratings, rating, rating_sd, Concreteness, and Frequency. You might need to use select to first select those columns.\n\nWhat are the strongest correlations?\nAre more concrete words also more iconic?\n\n\n\n5b: Does the strength of correlation depend on part-of-speech?\nFocusing specifically on the relationship between Concreteness and rating, calculate the correlation between these variables for each part-of-speech.\n\nWhich parts-of-speech show the highest correlation? Which show the lowest?\nCompare these values to the count of observations for each part of speech.\n\n\n\n5c: Concrete vs. abstract words\n\nNow, create a binary variable called Concrete that categorizes words with a Concreteness &gt;2.5 as “Concrete”, and words with a Concreteness &lt;= 2.5 as “Abstract”.\n\nThen, calculate the mean iconicity for words categorized as Concrete vs. Abstract."
  },
  {
    "objectID": "hands_on/week2.html#exercise-6-free-exploration",
    "href": "hands_on/week2.html#exercise-6-free-exploration",
    "title": "Week 2: Hands-on data wrangling",
    "section": "Exercise 6: Free exploration!",
    "text": "Exercise 6: Free exploration!\nTry to come up with at least one additional question to ask about your data.\n\nIf necessary, that could include merging with additional datasets (e.g., the AoA dataset used in class).\n\nAlternatively, you could try to build a regression model using lm to model the relationships between the variables more directly."
  },
  {
    "objectID": "labs/lab3.html",
    "href": "labs/lab3.html",
    "title": "Lab 3: Regression in R",
    "section": "",
    "text": "Name: ___________________________\nDate: ___________________________"
  },
  {
    "objectID": "labs/lab3.html#load-datasets",
    "href": "labs/lab3.html#load-datasets",
    "title": "Lab 3: Regression in R",
    "section": "Load datasets",
    "text": "Load datasets\nTo get started, let’s load the datasets.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggcorrplot)\n\n### Concreteness dataset\ndf_concreteness &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/wrangling/concreteness.csv\") \n\nRows: 28612 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Word, Dom_Pos\ndbl (2): Concreteness, Frequency\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnrow(df_concreteness)\n\n[1] 28612\n\n### AoA dataset\ndf_aoa &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/wrangling/AoA.csv\") \n\nRows: 31124 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Word\ndbl (1): AoA\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnrow(df_aoa)\n\n[1] 31124\n\n### Dataset with response times and accuracy\ndf_blp &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/wrangling/blp.csv\")\n\nRows: 55867 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): spelling, lexicality\ndbl (6): rt, zscore, accuracy, rt.sd, zscore.sd, accuracy.sd\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnrow(df_blp)\n\n[1] 55867"
  },
  {
    "objectID": "labs/lab3.html#exercise-1-join-the-datasets",
    "href": "labs/lab3.html#exercise-1-join-the-datasets",
    "title": "Lab 3: Regression in R",
    "section": "Exercise 1: Join the datasets",
    "text": "Exercise 1: Join the datasets\nTo get started, let’s join these datasets on a common column. You might need to add a new column the the BLP data to make that work. You might also want to drop any NA rows."
  },
  {
    "objectID": "labs/lab3.html#exercise-2-a-concreteness-advantage-pt.-1",
    "href": "labs/lab3.html#exercise-2-a-concreteness-advantage-pt.-1",
    "title": "Lab 3: Regression in R",
    "section": "Exercise 2: A concreteness advantage? (pt. 1)",
    "text": "Exercise 2: A concreteness advantage? (pt. 1)\nSome research on word processing has argued for a “concreteness advantage”: i.e., the idea that concrete words are easier to process. Here, “processing ease” is often measured in terms of response time (faster = easier) and also accuracy (more accurate = easier).\n\nCreate a scatterplot showing the relationship between rt and Concreteness.\nBuild a regression model building rt from Concreteness.\nWhat are the coefficients? Interpret them.\nAre any coefficients significant? What does it mean if they are?\nWhat is the overall model fit (i.e., \\(R^2\\))?\nFinally, is this consistent with a concreteness advantage"
  },
  {
    "objectID": "labs/lab3.html#exercise-3-a-concreteness-advantage-pt.-2",
    "href": "labs/lab3.html#exercise-3-a-concreteness-advantage-pt.-2",
    "title": "Lab 3: Regression in R",
    "section": "Exercise 3: A concreteness advantage? (pt. 2)",
    "text": "Exercise 3: A concreteness advantage? (pt. 2)\nNow replicate Exercise 2 but using accuracy as a dependnet measure instead of rt. Do you see a similar pattern of results?"
  },
  {
    "objectID": "labs/lab3.html#exercise-4-what-about-raw-frequency",
    "href": "labs/lab3.html#exercise-4-what-about-raw-frequency",
    "title": "Lab 3: Regression in R",
    "section": "Exercise 4: What about raw frequency?",
    "text": "Exercise 4: What about raw frequency?\nAnother well-attested finding in psycholinguistics is that frequent words are easier to process. Is that true of these data too?\n\nCreate a scatterplot showing the relationship between rt and Frequency.\nBuild an lm model predicting rt from raw Frequency. Interpret the coefficients.\nWhat is the \\(R^2\\) of this model? How does it compare to the concreteness model from exercise 2?"
  },
  {
    "objectID": "labs/lab3.html#exercise-5-what-about-log-frequency",
    "href": "labs/lab3.html#exercise-5-what-about-log-frequency",
    "title": "Lab 3: Regression in R",
    "section": "Exercise 5: What about log frequency?",
    "text": "Exercise 5: What about log frequency?\nFrequency is a right-skewed variable (if you’re curious, make a histogram of Frequency to illustrate what this means!). Skewed data can sometimes influence regression coefficients or model fit. In some cases, researchers address this by log-transforming frequency.\n\nApply a log transformation to Frequency, creating a new variable called log_freq.\nRedo Exercise 4 using log_freq instead. What differences do you observe?\nHow does having a log \\(X\\) variable change our interpretation of the coefficients?"
  },
  {
    "objectID": "labs/lab3.html#exercise-6-an-age-of-acquisition-advantage",
    "href": "labs/lab3.html#exercise-6-an-age-of-acquisition-advantage",
    "title": "Lab 3: Regression in R",
    "section": "Exercise 6: An age of acquisition advantage?",
    "text": "Exercise 6: An age of acquisition advantage?\nAnother common argument is that words that are learned earlier are easier to process, perhaps because they’ve been stored in the lexical network for longer. Is that true of these data?\n\nRedo exercises 2-3 using AoA instead of Concreteness.\nHow does the \\(R^2\\) of this model compare to the models with Concreteness, Frequency, or log_freq?"
  },
  {
    "objectID": "labs/lab3.html#exercise-7-a-multivariate-model",
    "href": "labs/lab3.html#exercise-7-a-multivariate-model",
    "title": "Lab 3: Regression in R",
    "section": "Exercise 7: A multivariate model",
    "text": "Exercise 7: A multivariate model\nBuild a multiple regression model predicting rt with Concreteness, AoA, and log_freq as predictors.\n\nInterpret each of the coefficients (including the Intercept). How is this different from our interpretation of the coefficients in the univariate models?\nUse broom::tidy to store the coefficients in a dataframe. Now, create a visualization of the coefficient values (e.g., a scatterplot with error bars)."
  },
  {
    "objectID": "labs/lab3.html#exercise-8-evaluating-this-model",
    "href": "labs/lab3.html#exercise-8-evaluating-this-model",
    "title": "Lab 3: Regression in R",
    "section": "Exercise 8: Evaluating this model",
    "text": "Exercise 8: Evaluating this model\n\nInterpret the \\(R^2\\) of the model from Exercise 7. How does this compare to the univariate models?\nExtract predictions from the modelusing the predict function and set them to a new column called rt_pred. Create a scatterplot showing the relationship between rt_pred and rt."
  },
  {
    "objectID": "labs/lab3.html#exercise-9-checking-for-multicollinearity",
    "href": "labs/lab3.html#exercise-9-checking-for-multicollinearity",
    "title": "Lab 3: Regression in R",
    "section": "Exercise 9: Checking for multicollinearity",
    "text": "Exercise 9: Checking for multicollinearity\nFinally, let’s check our model for signs of multicollinearity using the vif function. Is there anything to worry about?"
  },
  {
    "objectID": "labs/lab3.html#exercise-9-a-multivariate-model",
    "href": "labs/lab3.html#exercise-9-a-multivariate-model",
    "title": "Lab 3: Regression in R",
    "section": "Exercise 9: A multivariate model",
    "text": "Exercise 9: A multivariate model\nReplicate Exercises 7-8 but with accuracy as a dependent variable instead of rt."
  },
  {
    "objectID": "labs/lab3.html#submission-instructions",
    "href": "labs/lab3.html#submission-instructions",
    "title": "Lab 3: Regression in R",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nMake sure all your code chunks run without errors\nSave this file with your name in the filename (e.g., “Lab1_YourLastName.qmd”)\nRender the document to HTML\nSubmit both the .qmd file and the rendered HTML file to Canvas"
  },
  {
    "objectID": "readings.html",
    "href": "readings.html",
    "title": "Resources",
    "section": "",
    "text": "CSS 211 has no required readings. However, students might benefit from exploring additional content on course topics. The course schedule lists suggested readings for each week; here, I include those readings, as well as additional resources students might benefit from, organized by topic.\n\nGeneral Resources\n\nHadley Wickham’s R for Data Science(R4DS).\nIntroduction to Statistical Learning (James et al.)\n\n\n\nData wrangling and visualization.\n\nWickham, H. (2014). “Tidy data” (Journal of Statistical Software).\nWickham, H. & Grolemund, G. (2017). R for Data Science, [Chapters 5, 10-12](R4DS, data wrangling.\nHealy, K. (2018). Data Visualization, Chapters 1-3\n\n\n\nRegression\n\nJames, G. et al. (2021). Introduction to Statistical Learning, Chapter 3\nJames, G. et al. (2021). Introduction to Statistical Learning, Chapter 4.1-4.3\nJames, G. et al. (2021). Introduction to Statistical Learning, Chapter 6.1-6.2\n\n\n\nMixed Effects Models\n\nWinter, B. (2013). “Linear models and linear mixed effects models in R” (tutorial)\nGelman, A. & Hill, J. (2006). Data Analysis Using Regression, Chapters 11-12\nBates, D. et al. (2015). “Fitting linear mixed-effects models using lme4” (Journal of Statistical Software)\n\n\n\nResampling Methods\n\nJames, G. et al. (2021). Introduction to Statistical Learning, Chapter 5\n\n\n\nBest Practices and Research Design\n\nGelman, A. & Loken, E. (2014). “The garden of forking paths: Why multiple comparisons can be a problem” (American Scientist)"
  },
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "Lab 1: Introduction to R",
    "section": "",
    "text": "Name: ___________________________\nDate: ___________________________"
  },
  {
    "objectID": "labs/lab1.html#introduction",
    "href": "labs/lab1.html#introduction",
    "title": "Lab 1: Introduction to R",
    "section": "",
    "text": "Name: ___________________________\nDate: ___________________________"
  },
  {
    "objectID": "labs/lab1.html#introduction-1",
    "href": "labs/lab1.html#introduction-1",
    "title": "Lab 1: Introduction to R",
    "section": "Introduction",
    "text": "Introduction\nThe goal of this lab is to familiarize you with using R and RStudio. You’ll get hands-on practice with:\n\nPerforming simple calculations (basic arithmetic, mean, etc.).\nWorking with variables.\nCreating simple variables.\n\nNote that this lab will all be in “base R”, meaning we won’t (yet) be working with packages like tidyverse."
  },
  {
    "objectID": "labs/lab1.html#exercise-1-basic-arithmetic",
    "href": "labs/lab1.html#exercise-1-basic-arithmetic",
    "title": "Lab 1: Introduction to R",
    "section": "Exercise 1: Basic Arithmetic",
    "text": "Exercise 1: Basic Arithmetic\nR can be used as a calculator. Try the following operations:\n\n# Addition\n5 + 3\n\n[1] 8\n\n\n\n# Subtraction\n10 - 4\n\n[1] 6\n\n\n\n# Multiplication\n6 * 7\n\n[1] 42\n\n\n\n# Division\n20 / 4\n\n[1] 5\n\n\nYour turn: Calculate \\(15 + 8 * 3 - 2\\)\n\n# Write your code here:"
  },
  {
    "objectID": "labs/lab1.html#exercise-2-variables-and-assignment",
    "href": "labs/lab1.html#exercise-2-variables-and-assignment",
    "title": "Lab 1: Introduction to R",
    "section": "Exercise 2: Variables and Assignment",
    "text": "Exercise 2: Variables and Assignment\nIn R, we can store values in variables using the assignment operator &lt;-.\n\n# Create a variable called 'my_age' and assign it your age\nmy_age &lt;- 25  # Replace 25 with your actual age\n\n# Print the variable\nmy_age\n\n[1] 25\n\n\nYour turn: Create two variables representing the number of hours you studied yesterday and today. Calculate the total hours.\n\n# Write your code here:"
  },
  {
    "objectID": "labs/lab1.html#exercise-3-vectors-and-basic-statistics",
    "href": "labs/lab1.html#exercise-3-vectors-and-basic-statistics",
    "title": "Lab 1: Introduction to R",
    "section": "Exercise 3: Vectors and Basic Statistics",
    "text": "Exercise 3: Vectors and Basic Statistics\nVectors are collections of values. We create them using the c() function.\n\n# Create a vector of test scores\ntest_scores &lt;- c(85, 92, 78, 96, 88, 91, 83)\n\n# Display the vector\ntest_scores\n\n[1] 85 92 78 96 88 91 83\n\n\n\n# Calculate basic statistics\nmean(test_scores)     # Mean (average)\n\n[1] 87.57143\n\nmedian(test_scores)   # Median (middle value)\n\n[1] 88\n\nsd(test_scores)       # Standard deviation\n\n[1] 6.078847\n\nlength(test_scores)   # Number of values\n\n[1] 7\n\n\nYour turn: Create a vector with the ages of 5 family members or friends. Calculate the mean and standard deviation.\n\n# Write your code here:"
  },
  {
    "objectID": "labs/lab1.html#exercise-4-creating-data",
    "href": "labs/lab1.html#exercise-4-creating-data",
    "title": "Lab 1: Introduction to R",
    "section": "Exercise 4: Creating Data",
    "text": "Exercise 4: Creating Data\nLet’s create some sample data to work with:\n\n# Create a vector of 50 random numbers from a normal distribution\n# with mean = 100 and standard deviation = 15\nrandom_data &lt;- rnorm(50, mean = 100, sd = 15)\n\n# Look at the first 10 values\nhead(random_data, 10)\n\n [1]  89.72566 102.23141 100.89582  95.66177 110.21307 114.37167  88.14273\n [8]  93.95558  95.52066  60.88540\n\n\n\n# Calculate summary statistics for this data\nmean(random_data)\n\n[1] 101.0254\n\nsd(random_data)\n\n[1] 14.50248\n\nmin(random_data)\n\n[1] 60.8854\n\nmax(random_data)\n\n[1] 142.9077\n\n\nYour turn: Create a vector of 30 random numbers with mean = 50 and sd = 5. Calculate its summary statistics.\n\n# Write your code here:"
  },
  {
    "objectID": "labs/lab1.html#exercise-5-basic-plotting",
    "href": "labs/lab1.html#exercise-5-basic-plotting",
    "title": "Lab 1: Introduction to R",
    "section": "Exercise 5: Basic Plotting",
    "text": "Exercise 5: Basic Plotting\nR has built-in functions for creating plots. Let’s make a histogram:\n\n# Create a histogram of our random data\nhist(random_data, \n     main = \"Histogram of Random Data\",\n     xlab = \"Value\",\n     ylab = \"Frequency\",\n     col = \"lightblue\")\n\n\n\n\n\n\n\n\n\n# Create a simple scatterplot\nx_values &lt;- c(1, 2, 3, 4, 5)\ny_values &lt;- c(2, 4, 1, 5, 3)\n\nplot(x_values, y_values,\n     main = \"Simple Scatterplot\",\n     xlab = \"X Values\",\n     ylab = \"Y Values\",\n     pch = 16,  # Solid circles\n     col = \"red\")\n\n\n\n\n\n\n\n\nYour turn: Create a histogram of the family ages vector you created in Exercise 3. Add appropriate labels.\n\n# Write your code here:"
  },
  {
    "objectID": "labs/lab1.html#submission-instructions",
    "href": "labs/lab1.html#submission-instructions",
    "title": "Lab 1: Introduction to R",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nMake sure all your code chunks run without errors\nSave this file with your name in the filename (e.g., “Lab1_YourLastName.qmd”)\nRender the document to HTML\nSubmit both the .qmd file and the rendered HTML file to Canvas"
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Lab 2: Data Visualization in R",
    "section": "",
    "text": "Name: ___________________________\nDate: ___________________________"
  },
  {
    "objectID": "labs/lab2.html#load-data",
    "href": "labs/lab2.html#load-data",
    "title": "Lab 2: Data Visualization in R",
    "section": "Load data",
    "text": "Load data\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggcorrplot)\n\n### Concreteness dataset\ndf_concreteness &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/wrangling/concreteness.csv\")\n\nRows: 28612 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Word, Dom_Pos\ndbl (2): Concreteness, Frequency\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n### New dataset with response times and accuracy\ndf_blp &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/wrangling/blp.csv\")\n\nRows: 55867 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): spelling, lexicality\ndbl (6): rt, zscore, accuracy, rt.sd, zscore.sd, accuracy.sd\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "labs/lab2.html#exercise-1-explore-the-blp-data",
    "href": "labs/lab2.html#exercise-1-explore-the-blp-data",
    "title": "Lab 2: Data Visualization in R",
    "section": "Exercise 1: Explore the BLP data",
    "text": "Exercise 1: Explore the BLP data\nThe British Lexicon Project was a large-scale study asking participants to make lexical decision (“is this a word?”) about a series of actual words and non-words. The authors recorded the response time (rt) and accuracy for each word and non-word.\nTake a moment to explore the data and each of the columns:\n\nWhich column contains the word or non-word itself?\nThe lexicality column indicates whether something is a word (W) or not (N). How many of each category are there?\n\nAre there any missing values anywhere? If so, exclude those rows from the dataset using drop_na.\nUse group_by and summarise to calculate the average rt and average accuracy for words and non-words."
  },
  {
    "objectID": "labs/lab2.html#exercise-2-join-with-concreteness",
    "href": "labs/lab2.html#exercise-2-join-with-concreteness",
    "title": "Lab 2: Data Visualization in R",
    "section": "Exercise 2: Join with concreteness",
    "text": "Exercise 2: Join with concreteness\nNow, join the BLP dataset with the concreteness dataset.\n\nFirst, you should drop any non-words from the dataset using the lexicality column and filter.\nYou might need to first rename one of the BLP columns to find a common “key”.\nYou’ll also need to decide what kind of join operation to use. My suggestion is to use a join that only includes words that appear in both datasets.\nHow many words are in this merged dataset?"
  },
  {
    "objectID": "labs/lab2.html#exercise-3-are-concreteness-and-response-patterns-related",
    "href": "labs/lab2.html#exercise-3-are-concreteness-and-response-patterns-related",
    "title": "Lab 2: Data Visualization in R",
    "section": "Exercise 3: Are concreteness and response patterns related?",
    "text": "Exercise 3: Are concreteness and response patterns related?\nLet’s try to figure out whether people respond faster or slower to more concrete words.\n\n3a: Create a scatterplot showing the relationship between rt and Concreteness\nUse geom_point to make the scatterplot and geom_smooth(method = \"lm\") to plot a regression line over it. Try modifying the alpha of the points. Also make sure to clearly label your axes and choose a theme!\nSome questions to consider:\n\nDoes this relationship look positive, negative, or neutral (no relationship)?\nConceptually, what might this mean in terms of a concreteness advantage?\n\n\n\n3b: Create a scatterplot showing the relationship between accuracy and Concreteness\nAgain, use geom_point to make the scatterplot and geom_smooth(method = \"lm\") to plot a regression line over it. Try modifying the alpha of the points. Also make sure to clearly label your axes and choose a theme!\nSome questions to consider:\n\nDoes this relationship look positive, negative, or neutral (no relationship)?\nConceptually, what might this mean in terms of a concreteness advantage?\n\n\n\n3c: Calculate a correlation coefficient\nNow, for each of the relationships above, calculate a correlation coefficient using cor.test. What can you conclude? Does it match your intuitions from the visualization?"
  },
  {
    "objectID": "labs/lab2.html#exercise-4-other-variables",
    "href": "labs/lab2.html#exercise-4-other-variables",
    "title": "Lab 2: Data Visualization in R",
    "section": "Exercise 4: Other variables!",
    "text": "Exercise 4: Other variables!\nRecall that the concreteness dataset also includes information about word frequency and part of speech. Let’s dig into those variables now to figure out how they relate to rt/accuracy.\n\n4a: Is Frequency related to rt or accuracy?\n\nReproduce 3a-3c but using Frequency instead of Concreteness.\nYou should also check whether using log frequency affects your results. Create a new variable called log_frequency and redo the above with that.\n\n\n\n4b: Is Part of Speech related to rt or accuracy?\n\nNow, group_by part-of-speech (Dom_Pos) and calculate the mean rt and accuracy. Which part of speech has the faster rt and highest accuracy?\n\nCreate a violin plot showing rt`` byDom_Pos. Reorder the levels in terms of increasingrt`.\n\n\n\n4c: What about number of letters?\nSome words are longer than others. Does that matter?\n\nUse mutate, as well as the nchar function, to create a new variable called num_letters (for each Word).\nThen recreate exercises 3a-3c for num_letters."
  },
  {
    "objectID": "labs/lab2.html#exercise-5-create-a-correlation-matrix",
    "href": "labs/lab2.html#exercise-5-create-a-correlation-matrix",
    "title": "Lab 2: Data Visualization in R",
    "section": "Exercise 5: Create a correlation matrix",
    "text": "Exercise 5: Create a correlation matrix\nTo round things off, let’s create a correlation matrix between our numeric variables:\n\nFirst, use select and cor to get the correlation between our variables: Frequency, log_frequency, Concreteness, num_letters, rt, and accuracy.\nNext, use ggcorrplot to visualize that correlation matrix."
  },
  {
    "objectID": "labs/lab2.html#submission-instructions",
    "href": "labs/lab2.html#submission-instructions",
    "title": "Lab 2: Data Visualization in R",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nMake sure all your code chunks run without errors\nSave this file with your name in the filename (e.g., “Lab1_YourLastName.qmd”)\nRender the document to HTML\nSubmit both the .qmd file and the rendered HTML file to Canvas"
  },
  {
    "objectID": "hands_on/week3.html",
    "href": "hands_on/week3.html",
    "title": "Week 3: Hands-on data visualization",
    "section": "",
    "text": "The goal of this hands-on exercise is to familiarize you with data visualization, particularly with an emphasis on exploring a dataset and creating clear visualizations that communicate basic insights.\nWe’ll be working with the CLEAR Corpus, a dataset of text excerpts rated for their readability (Crossley et al., 2021)."
  },
  {
    "objectID": "hands_on/week3.html#introduction",
    "href": "hands_on/week3.html#introduction",
    "title": "Week 3: Hands-on data visualization",
    "section": "",
    "text": "The goal of this hands-on exercise is to familiarize you with data visualization, particularly with an emphasis on exploring a dataset and creating clear visualizations that communicate basic insights.\nWe’ll be working with the CLEAR Corpus, a dataset of text excerpts rated for their readability (Crossley et al., 2021)."
  },
  {
    "objectID": "hands_on/week3.html#load-dataset",
    "href": "hands_on/week3.html#load-dataset",
    "title": "Week 3: Hands-on data visualization",
    "section": "Load dataset",
    "text": "Load dataset\nTo get started, let’s load the dataset.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggcorrplot)\n\n### Lancaster norms\ndf_clear &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/viz/CLEAR_corpus_final.csv\")\n\nRows: 4724 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): Author, Title, Anthology, URL, Categ, Sub Cat, Lexile Band, Locati...\ndbl (17): ID, Pub Year, MPAA #Max, MPAA# Avg, Google WC, Sentence Count, Par...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "hands_on/week3.html#exercise-1-understand-the-structure-of-your-data",
    "href": "hands_on/week3.html#exercise-1-understand-the-structure-of-your-data",
    "title": "Week 3: Hands-on data visualization",
    "section": "Exercise 1: Understand the structure of your data",
    "text": "Exercise 1: Understand the structure of your data\nAs always, let’s try to understand our data first.\nUse functions like nrow, colnames, head, and more to understand the structure and content of our dataset.\nTry to answer the following questions:\n\nWhat does each column mean? (Hint: some of these columns represent automated reading indices, and BT_easiness is the normalized human readability judgments.)\nWould you say the dataset is in wide or long format?\n\nAny missing values? What should you do about them?"
  },
  {
    "objectID": "hands_on/week3.html#exercise-2-which-metrics-correlate-and-how-much",
    "href": "hands_on/week3.html#exercise-2-which-metrics-correlate-and-how-much",
    "title": "Week 3: Hands-on data visualization",
    "section": "Exercise 2: Which metrics correlate, and how much?",
    "text": "Exercise 2: Which metrics correlate, and how much?\nFocusing on the various metrics of readability (E.g., Flesch-Kincaid, etc.) as well as the metrics reflecting passage length (e.g., Paragraphs, Sentence Count), create a correlation matrix and plot it with ggcorrplot. Which metrics correlate the most with BT_easiness (the “gold standard”)?\n(Note: Some of the metrics may be negatively correlated, because they capture reading difficulty as opposed to reading ease; if it helps, you can always take the absolute value of the correlations when determining which one is strongest.)"
  },
  {
    "objectID": "hands_on/week3.html#exercise-3-create-a-scatterplot",
    "href": "hands_on/week3.html#exercise-3-create-a-scatterplot",
    "title": "Week 3: Hands-on data visualization",
    "section": "Exercise 3: Create a scatterplot",
    "text": "Exercise 3: Create a scatterplot\nChoose 1-2 of the metrics and create a scatterplot showing how it relates to BT_easiness. Does this roughly match what you found in the correlation exercise above?"
  },
  {
    "objectID": "hands_on/week3.html#exercise-4-modify-your-scatterplot",
    "href": "hands_on/week3.html#exercise-4-modify-your-scatterplot",
    "title": "Week 3: Hands-on data visualization",
    "section": "Exercise 4: Modify your scatterplot",
    "text": "Exercise 4: Modify your scatterplot\nNow, add a new layer to your scatterplot. That could be a categorical factor (e.g., Categ) or another continuous metric: you can decide! What did this layer reveal to you?"
  },
  {
    "objectID": "hands_on/week3.html#exercise-5-are-certain-categories-more-readable",
    "href": "hands_on/week3.html#exercise-5-are-certain-categories-more-readable",
    "title": "Week 3: Hands-on data visualization",
    "section": "Exercise 5: Are certain categories more readable?",
    "text": "Exercise 5: Are certain categories more readable?\n\nNow create a plot showing whether BT_easiness varies by Categ directly. You can choose to use either a barplot (with standard errors) or a violin plot (or barplot).\n\nThen put some numbers to it: use group_by %&gt;% summarise to get the mean BT_easiness by Categ."
  },
  {
    "objectID": "hands_on/week3.html#exercise-6-are-passages-from-different-locations-more-readable",
    "href": "hands_on/week3.html#exercise-6-are-passages-from-different-locations-more-readable",
    "title": "Week 3: Hands-on data visualization",
    "section": "Exercise 6: Are passages from different locations more readable?",
    "text": "Exercise 6: Are passages from different locations more readable?\nThe corpus also contains information about where in a passage text was excerpted from (e.g., the beginning or the end). Repeat exercise 5 above but using Location instead of Categ. What do you find? (Note: Make sure to reorder your plot in terms of increasing or decreasing BT_easiness).\nNow create a plot showing whether BT_easiness varies by Categ directly. You can choose to use either a barplot (with standard errors) or a violin plot (or barplot)."
  },
  {
    "objectID": "hands_on/week3.html#exercise-7-what-about-sub-cat",
    "href": "hands_on/week3.html#exercise-7-what-about-sub-cat",
    "title": "Week 3: Hands-on data visualization",
    "section": "Exercise 7: What about Sub Cat?",
    "text": "Exercise 7: What about Sub Cat?\nSome of the excerpts also contain information about the sub category they are drawn from. Repeat exercise 5 again, this time for Sub Cat.\n\nDoes anything stick out to you?\nWhat might be a limitation to this analysis? (Hint: You might want to count how many observations there are per Sub Cat.)"
  },
  {
    "objectID": "hands_on/week3.html#exercise-8-most-and-least-readable-texts",
    "href": "hands_on/week3.html#exercise-8-most-and-least-readable-texts",
    "title": "Week 3: Hands-on data visualization",
    "section": "Exercise 8: Most and least readable texts?",
    "text": "Exercise 8: Most and least readable texts?\nFinally, use slice_max and slice_min to find the most and least readable texts in the corpus. Do you agree with this assessment?"
  },
  {
    "objectID": "hands_on/week5.html",
    "href": "hands_on/week5.html",
    "title": "Week 5: Multiple regression",
    "section": "",
    "text": "The goal of this hands-on exercise is to familiarize you with building and interpreting multiple regression models in R, and also debugging common issues in regression models.\nWe’ll be working with a dataset of housing prices in California. Our goal is to build a model predicting median_house_value in a given district/county."
  },
  {
    "objectID": "hands_on/week5.html#introduction",
    "href": "hands_on/week5.html#introduction",
    "title": "Week 5: Multiple regression",
    "section": "",
    "text": "The goal of this hands-on exercise is to familiarize you with building and interpreting multiple regression models in R, and also debugging common issues in regression models.\nWe’ll be working with a dataset of housing prices in California. Our goal is to build a model predicting median_house_value in a given district/county."
  },
  {
    "objectID": "hands_on/week5.html#load-dataset",
    "href": "hands_on/week5.html#load-dataset",
    "title": "Week 5: Multiple regression",
    "section": "Load dataset",
    "text": "Load dataset\nTo get started, let’s load the dataset.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggcorrplot)\n\n### Housing dataset\ndf_housing &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/regression/housing.csv\")\n\nRows: 20640 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): ocean_proximity\ndbl (9): longitude, latitude, housing_median_age, total_rooms, total_bedroom...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "hands_on/week5.html#exercise-1-understand-your-data",
    "href": "hands_on/week5.html#exercise-1-understand-your-data",
    "title": "Week 5: Multiple regression",
    "section": "Exercise 1: Understand your data",
    "text": "Exercise 1: Understand your data\nTo get started, let’s explore our data.\n\nLook at each of the variables and create histograms of them (or bar plots, etc.). What do they look like? What are they?\nWhich variables correlate with each other? Use gcorrplot?"
  },
  {
    "objectID": "hands_on/week5.html#exercise-2-build-univariate-models",
    "href": "hands_on/week5.html#exercise-2-build-univariate-models",
    "title": "Week 5: Multiple regression",
    "section": "Exercise 2: Build univariate models",
    "text": "Exercise 2: Build univariate models\nNow, build a series of univariate models to predict median_house_value.\n\nTry building models for each of the following predictors: housing_median_age, total_bedrooms, population, median_income, and ocean_proximity.\nFor each model, interpret the coefficients and also the \\(R^2\\).\nOptionally, make a barplot comparing the \\(R^2\\) of these univariate models."
  },
  {
    "objectID": "hands_on/week5.html#exercise-3-build-a-big-multivariate-model",
    "href": "hands_on/week5.html#exercise-3-build-a-big-multivariate-model",
    "title": "Week 5: Multiple regression",
    "section": "Exercise 3: Build a big multivariate model!",
    "text": "Exercise 3: Build a big multivariate model!\nNow let’s construct a multivariate model. Combine all the predictors from Exercise 2 into a single multivariate model.\n\nHow does this change your interpretation of the coefficients?\nHow does this change the overall model fit?\nUsing vif (from the car package), determine whether there’s multicollinearity in your model."
  },
  {
    "objectID": "hands_on/week5.html#exercise-4-a-more-principled-approach",
    "href": "hands_on/week5.html#exercise-4-a-more-principled-approach",
    "title": "Week 5: Multiple regression",
    "section": "Exercise 4: A more principled approach",
    "text": "Exercise 4: A more principled approach\nNow let’s use a technique called forward stepwise regression to build a locally optimal set of predictors.\n\nStart with the best model from Exercise 2.\nThen, build a series of 2-variable models (i.e., with each other predictor), and choose the best of those.\nDo this until you’ve added all the variables from Exercise 2 in order of how much they help the model.\n\nWhat’s the \\(R^2\\) of each of these progressively more complicated models?"
  },
  {
    "objectID": "hands_on/week5.html#exercise-5-plot-a-map-of-california",
    "href": "hands_on/week5.html#exercise-5-plot-a-map-of-california",
    "title": "Week 5: Multiple regression",
    "section": "Exercise 5: Plot a map of California!",
    "text": "Exercise 5: Plot a map of California!\nOur data also includes latitude and longitude information. Use that (e.g., in a scatterplot) to make a visualization of how housing prices change across California."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSS 211: Statistical Methods for Computational Social Science",
    "section": "",
    "text": "This is the course website for CSS 211/POLI 279. The current iteration of the course is Fall 2025.\nThe goal of this course is to introduce students to foundational statistical methods and tools for computational social science (CSS), including: data wrangling and visualization, regression, and model selection. Students will also develop proficiency in the R programming, and the course emphasizes hands-on application of techniques to real-world datasets."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "CSS 211: Statistical Methods for Computational Social Science",
    "section": "",
    "text": "This is the course website for CSS 211/POLI 279. The current iteration of the course is Fall 2025.\nThe goal of this course is to introduce students to foundational statistical methods and tools for computational social science (CSS), including: data wrangling and visualization, regression, and model selection. Students will also develop proficiency in the R programming, and the course emphasizes hands-on application of techniques to real-world datasets."
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "CSS 211: Statistical Methods for Computational Social Science",
    "section": "Instructor",
    "text": "Instructor\nSean Trott\nDepartment of Cognitive Science, UCSD"
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "CSS 211: Statistical Methods for Computational Social Science",
    "section": "Quick Links",
    "text": "Quick Links\n\nSyllabus\nCourse Schedule\n\nGitHub Repository"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#goals-of-the-lecture",
    "href": "lectures/week6-r-regression-slides.html#goals-of-the-lecture",
    "title": "Logistic Regression",
    "section": "Goals of the lecture",
    "text": "Goals of the lecture\n\nClassification: dealing with categorical outcomes.\nLogistic regression.\n\nWhy not linear regression?\nGeneralized linear models (GLMs).\nOdds and log-odds.\nThe logistic function.\n\nBuilding and interpreting logistic models with glm."
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#what-is-classification",
    "href": "lectures/week6-r-regression-slides.html#what-is-classification",
    "title": "Logistic Regression",
    "section": "What is classification?",
    "text": "What is classification?\n\n“To classify is human…We sort dirty dishes from clean, white laundry from colorfast, important email to be answered from e-junk…. Any part of the home, school, or workplace reveals some such system of classification.”\n— Bowker & Star, 2000\n\n\nClassification = predicting a categorical response variable using features\nCommon examples:\n\nIs an email spam or not spam?\nIs a cell mass cancerous or not cancerous?\nWill this customer buy or not buy?\nIs a credit card transaction fraudulent?\nIs this image a cat, dog, person, or other?"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#binary-vs.-multi-class-classification",
    "href": "lectures/week6-r-regression-slides.html#binary-vs.-multi-class-classification",
    "title": "Logistic Regression",
    "section": "Binary vs. multi-class classification",
    "text": "Binary vs. multi-class classification\n\nBinary classification: sorting inputs into one of two labels\n\nE.g., spam vs. not spam\n\nMulti-class classification: more than two labels\n\nE.g., face recognition with n possible identities\nE.g., image classification (cat, dog, person, other)\n\nToday: focus on binary classification with logistic regression"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#part-1-foundations-of-logistic-regression",
    "href": "lectures/week6-r-regression-slides.html#part-1-foundations-of-logistic-regression",
    "title": "Logistic Regression",
    "section": "Part 1: Foundations of logistic regression",
    "text": "Part 1: Foundations of logistic regression\nMotivation, log-odds, and the logistic function."
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#example-dataset-email-spam",
    "href": "lectures/week6-r-regression-slides.html#example-dataset-email-spam",
    "title": "Logistic Regression",
    "section": "Example dataset: Email spam",
    "text": "Example dataset: Email spam\n\n### Loading the tidyverse\nlibrary(tidyverse)\n\ndf_spam = read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/logistic/spam.csv\")\nnrow(df_spam)\n\n\n\n[1] 3921\n\n\n\nDataset contains information about emails and whether they were spam, along with various features like number of characters, whether they contain certain words, etc."
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#why-not-linear-regression",
    "href": "lectures/week6-r-regression-slides.html#why-not-linear-regression",
    "title": "Logistic Regression",
    "section": "Why not linear regression?",
    "text": "Why not linear regression?\n\nspam is coded as 0 (no) or 1 (yes)\nCould we treat this as continuous and use linear regression?\nInterpret prediction \\(\\hat{y}\\) as probability of outcome?\n\n\n\n\n\n💭 Check-in\n\n\nWhat issues might arise here?"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#the-problem-predictions-beyond-01",
    "href": "lectures/week6-r-regression-slides.html#the-problem-predictions-beyond-01",
    "title": "Logistic Regression",
    "section": "The problem: predictions beyond [0,1]",
    "text": "The problem: predictions beyond [0,1]\n\nggplot(df_spam, aes(x = num_char, y = spam)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Number of characters\",\n       y = \"Spam (0 = no, 1 = yes)\") +\n  theme_minimal()\n\n\nLinear model generates predictions outside [0,1], but probability must be bounded!"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#framing-the-problem-probabilistically",
    "href": "lectures/week6-r-regression-slides.html#framing-the-problem-probabilistically",
    "title": "Logistic Regression",
    "section": "Framing the problem probabilistically",
    "text": "Framing the problem probabilistically\n\nTreat each outcome as Bernoulli trials: “success” (spam) vs. “failure” (not spam)\nEach observation has independent probability of success: \\(p\\)\nOn its own: \\(p\\) = proportion of spam emails\nGoal: model \\(p\\) conditioned on other variables, i.e., \\(P(Y = 1 | X)\\)\n\n\nQuestion: What does \\(p\\) on its own remind you of from linear regression?\n\n\nAnswer: The intercept-only model (the mean of \\(Y\\))"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#generalized-linear-models-glms",
    "href": "lectures/week6-r-regression-slides.html#generalized-linear-models-glms",
    "title": "Logistic Regression",
    "section": "Generalized linear models (GLMs)",
    "text": "Generalized linear models (GLMs)\n\nGeneralized linear models (GLMs) are generalizations of linear regression.\n\nEach GLM has:\n\nA probability distribution for the outcome variable\nA linear model: \\(\\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k\\)\nA link function relating the linear model to the outcome\n\n\nWe need a function that links our linear model to a probability score bounded at [0, 1]."
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#common-glms",
    "href": "lectures/week6-r-regression-slides.html#common-glms",
    "title": "Logistic Regression",
    "section": "Common GLMs",
    "text": "Common GLMs\n\n\n\n\n\n\n\n\n\n\nModel name\nDistribution\nLink function\nUse cases\nExample\n\n\n\n\nLinear regression\nNormal\nIdentity\nContinuous response\nHeight, price\n\n\nLogistic regression\nBernoulli/Binomial\nLogit\nBinary response\nSpam, fraud\n\n\nPoisson regression\nPoisson\nLog\nCount data\n# words, # visitors\n\n\n\nToday: logistic regression"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#the-logit-link-function",
    "href": "lectures/week6-r-regression-slides.html#the-logit-link-function",
    "title": "Logistic Regression",
    "section": "The logit link function",
    "text": "The logit link function\nLogistic regression uses the logit link function:\n\\[\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)\\]\n\nWhere \\(p\\) is the probability of some outcome\nTakes a value between \\([0, 1]\\) and maps it to \\((-\\infty, \\infty)\\)\nAlso called the log-odds"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#introducing-the-odds",
    "href": "lectures/week6-r-regression-slides.html#introducing-the-odds",
    "title": "Logistic Regression",
    "section": "Introducing the odds",
    "text": "Introducing the odds\n\nThe odds of an event are the ratio of the probability of an event occuring (\\(p\\)) and the probability of event not occurring (\\(1-p\\)).\n\n\\[\\text{Odds}(Y) = \\frac{p}{1-p}\\]\n\nUnlike \\(p\\), odds are bounded at \\([0, \\infty)\\)\nOdds of 1 means 50/50 chance\nOdds &gt; 1 means more likely to occur than not\nOdds &lt; 1 means less likely to occur than not"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#visualizing-odds",
    "href": "lectures/week6-r-regression-slides.html#visualizing-odds",
    "title": "Logistic Regression",
    "section": "Visualizing odds",
    "text": "Visualizing odds\n\np &lt;- seq(0.01, 0.99, 0.01)\nodds &lt;- p / (1 - p)\n\nggplot(data.frame(p, odds), aes(x = p, y = odds)) +\n  geom_point() +\n  labs(x = \"P(Y)\", y = \"Odds(Y)\", \n       title = \"Odds(Y) vs. P(Y)\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#introducing-the-log-odds-logit",
    "href": "lectures/week6-r-regression-slides.html#introducing-the-log-odds-logit",
    "title": "Logistic Regression",
    "section": "Introducing the log-odds (logit)",
    "text": "Introducing the log-odds (logit)\n\nThe log-odds is the log of the odds (the logit function).\n\n\\[\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)\\]\n\nUnlike \\(p\\), log-odds are bounded at \\((-\\infty, \\infty)\\)\nThis is what we’ll model linearly!"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#visualizing-log-odds",
    "href": "lectures/week6-r-regression-slides.html#visualizing-log-odds",
    "title": "Logistic Regression",
    "section": "Visualizing log-odds",
    "text": "Visualizing log-odds\n\nlog_odds &lt;- log(p / (1 - p))\n\nggplot(data.frame(p, log_odds), aes(x = p, y = log_odds)) +\n  geom_point() +\n  labs(x = \"P(Y)\", y = \"Log-odds(Y)\", \n       title = \"Log-odds(Y) vs. P(Y)\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#interpreting-the-sign-of-log-odds",
    "href": "lectures/week6-r-regression-slides.html#interpreting-the-sign-of-log-odds",
    "title": "Logistic Regression",
    "section": "Interpreting the sign of log-odds",
    "text": "Interpreting the sign of log-odds\n\nggplot(data.frame(p, log_odds), aes(x = p, y = log_odds)) +\n  geom_point() +\n  geom_vline(xintercept = 0.5, linetype = \"dotted\", color = \"red\") +\n  geom_hline(yintercept = 0, linetype = \"dotted\", color = \"red\") +\n  labs(x = \"P(Y)\", y = \"Log-odds(Y)\") +\n  theme_minimal()\n\n\n\nPositive log-odds → \\(p &gt; 0.5\\)\nNegative log-odds → \\(p &lt; 0.5\\)"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#is-log-odds-linearly-related-to-p",
    "href": "lectures/week6-r-regression-slides.html#is-log-odds-linearly-related-to-p",
    "title": "Logistic Regression",
    "section": "Is log-odds linearly related to p?",
    "text": "Is log-odds linearly related to p?\nNo! The log-odds of \\(Y\\) is non-linearly related to \\(P(Y)\\).\n\nThis means we cannot interpret linear changes in log-odds as linear changes in probability\nThis will be very important when interpreting logistic regression models"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#the-logistic-function",
    "href": "lectures/week6-r-regression-slides.html#the-logistic-function",
    "title": "Logistic Regression",
    "section": "The logistic function",
    "text": "The logistic function\n\nThe logistic function is the inverse of the logit function.\n\n\\[P(Y) = \\frac{e^{\\text{logit}(p)}}{1 + e^{\\text{logit}(p)}}\\]\n\nConverts log-odds back to probability\nMaps \\((-\\infty, \\infty)\\) to \\([0, 1]\\)\nAlso called the sigmoid function"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#mapping-from-log-odds-to-probability",
    "href": "lectures/week6-r-regression-slides.html#mapping-from-log-odds-to-probability",
    "title": "Logistic Regression",
    "section": "Mapping from log-odds to probability",
    "text": "Mapping from log-odds to probability\n\nlog_odds_range &lt;- seq(-10, 10, 0.1)\np_range &lt;- exp(log_odds_range) / (1 + exp(log_odds_range))\nggplot(data.frame(log_odds_range, p_range), \n       aes(x = log_odds_range, y = p_range)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Log-odds(Y)\", y = \"P(Y)\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#where-does-regression-come-in",
    "href": "lectures/week6-r-regression-slides.html#where-does-regression-come-in",
    "title": "Logistic Regression",
    "section": "Where does regression come in?",
    "text": "Where does regression come in?\nWith logistic regression, we learn parameters \\(\\beta\\) for:\n\\[\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k\\]\n\nOur “dependent variable” is the log-odds (logit) of \\(p\\)\nWe learn a linear relationship between \\(X\\) and the log-odds of our outcome\nNOT a linear relationship with probability itself!"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#interpreting-β-log-odds-x-is-linear",
    "href": "lectures/week6-r-regression-slides.html#interpreting-β-log-odds-x-is-linear",
    "title": "Logistic Regression",
    "section": "Interpreting β: log-odds ~ X is linear",
    "text": "Interpreting β: log-odds ~ X is linear\n\\[\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X\\]\n\nIf \\(\\beta_1 &gt; 0\\): For each 1-unit increase in \\(X\\), log-odds increase by \\(\\beta_1\\)\nIf \\(\\beta_1 &lt; 0\\): For each 1-unit increase in \\(X\\), log-odds decrease by \\(|\\beta_1|\\)\nStraightforward linear interpretation for log-odds"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#interpreting-β-py-x-is-not-linear",
    "href": "lectures/week6-r-regression-slides.html#interpreting-β-py-x-is-not-linear",
    "title": "Logistic Regression",
    "section": "Interpreting β: P(Y) ~ X is NOT linear",
    "text": "Interpreting β: P(Y) ~ X is NOT linear\nThe mapping between log-odds and \\(P(Y)\\) is not linear.\n\nWe cannot interpret coefficients linearly with respect to \\(P(Y)\\)!"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#part-2-logistic-regression-in-r",
    "href": "lectures/week6-r-regression-slides.html#part-2-logistic-regression-in-r",
    "title": "Logistic Regression",
    "section": "Part 2: Logistic regression in R",
    "text": "Part 2: Logistic regression in R\nUsing glm, interpreting logistic models."
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#logistic-regression-in-r",
    "href": "lectures/week6-r-regression-slides.html#logistic-regression-in-r",
    "title": "Logistic Regression",
    "section": "Logistic regression in R",
    "text": "Logistic regression in R\nUse the glm() function with family = binomial:\n\nmodel &lt;- glm(y ~ x,  ### formula\n             data = df_name,  ## dataframe name\n             family = binomial(link = \"logit\")) ## using logit link\n\n\nfamily = binomial: specifies we’re modeling binary outcomes\nlink = \"logit\": specifies the logit link function (default for binomial)\nFitting is straightforward—interpreting is the harder part!"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#fitting-a-simple-model",
    "href": "lectures/week6-r-regression-slides.html#fitting-a-simple-model",
    "title": "Logistic Regression",
    "section": "Fitting a simple model",
    "text": "Fitting a simple model\nLet’s predict spam from num_char (message length):\n\nmod_len &lt;- glm(spam ~ num_char, \n               data = df_spam, \n               family = binomial)\nsummary(mod_len)\n\n\nCall:\nglm(formula = spam ~ num_char, family = binomial, data = df_spam)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.798738   0.071562 -25.135  &lt; 2e-16 ***\nnum_char    -0.062071   0.008014  -7.746  9.5e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2437.2  on 3920  degrees of freedom\nResidual deviance: 2346.4  on 3919  degrees of freedom\nAIC: 2350.4\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#interpreting-the-coefficients",
    "href": "lectures/week6-r-regression-slides.html#interpreting-the-coefficients",
    "title": "Logistic Regression",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\n\ncoef(mod_len)\n\n(Intercept)    num_char \n-1.79873764 -0.06207116 \n\n\n\nIntercept (-1.80): log-odds of spam when num_char = 0\nnum_char (-0.06): for every 1-unit increase in num_char, log-odds of spam decrease by 0.06\nNegative coefficient → longer emails are less likely to be spam"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#converting-log-odds-to-probability",
    "href": "lectures/week6-r-regression-slides.html#converting-log-odds-to-probability",
    "title": "Logistic Regression",
    "section": "Converting log-odds to probability",
    "text": "Converting log-odds to probability\nWhat’s \\(P(\\text{spam})\\) when num_char = 0?\n\n# Log-odds (just the intercept)\nlog_odds &lt;- coef(mod_len)[1]\nlog_odds\n\n(Intercept) \n  -1.798738 \n\n# Convert to probability\np &lt;- exp(log_odds) / (1 + exp(log_odds))\np\n\n(Intercept) \n  0.1420048 \n\n\nAbout 14% chance of spam for a message with 0 characters."
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#example-num_char-100",
    "href": "lectures/week6-r-regression-slides.html#example-num_char-100",
    "title": "Logistic Regression",
    "section": "Example: num_char = 100",
    "text": "Example: num_char = 100\nWhat’s \\(P(\\text{spam})\\) when num_char = 100?\n\n# Calculate log-odds\nlog_odds &lt;- coef(mod_len)[1] + coef(mod_len)[2] * 100\nlog_odds\n\n(Intercept) \n  -8.005853 \n\n# Convert to probability\np &lt;- exp(log_odds) / (1 + exp(log_odds))\np\n\n (Intercept) \n0.0003333936 \n\n\nLess than 0.1% chance—very unlikely to be spam!"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#visualizing-log-odds-vs.-probability",
    "href": "lectures/week6-r-regression-slides.html#visualizing-log-odds-vs.-probability",
    "title": "Logistic Regression",
    "section": "Visualizing: log-odds vs. probability",
    "text": "Visualizing: log-odds vs. probability\n\nX &lt;- df_spam$num_char\nlo &lt;- coef(mod_len)[1] + coef(mod_len)[2] * X\np &lt;- exp(lo) / (1 + exp(lo))\n\npar(mfrow = c(1, 2))\nplot(X, lo, xlab = \"# Characters\", ylab = \"Log-odds(spam)\")\nplot(X, p, xlab = \"# Characters\", ylab = \"P(spam)\")\n\n\nLinear on log-odds scale, non-linear on probability scale!"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#categorical-predictors",
    "href": "lectures/week6-r-regression-slides.html#categorical-predictors",
    "title": "Logistic Regression",
    "section": "Categorical predictors",
    "text": "Categorical predictors\nLet’s use winner (whether email contains the word “winner”):\n\nmod_winner &lt;- glm(spam ~ winner, \n                  data = df_spam, \n                  family = binomial)\nsummary(mod_winner)\n\n\nCall:\nglm(formula = spam ~ winner, family = binomial, data = df_spam)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.31405    0.05627 -41.121  &lt; 2e-16 ***\nwinneryes    1.52559    0.27549   5.538 3.06e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2437.2  on 3920  degrees of freedom\nResidual deviance: 2412.7  on 3919  degrees of freedom\nAIC: 2416.7\n\nNumber of Fisher Scoring iterations: 5"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#interpreting-categorical-predictors",
    "href": "lectures/week6-r-regression-slides.html#interpreting-categorical-predictors",
    "title": "Logistic Regression",
    "section": "Interpreting categorical predictors",
    "text": "Interpreting categorical predictors\n\ncoef(mod_winner)\n\n(Intercept)   winneryes \n  -2.314047    1.525589 \n\n\n\nIntercept (-2.31): log-odds of spam when winner = \"no\"\nwinneryes (1.53): change in log-odds (relative to intercept) when winner = \"yes\"\nJust like linear regression: categorical predictors are relative to reference level"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#probability-for-winner-no",
    "href": "lectures/week6-r-regression-slides.html#probability-for-winner-no",
    "title": "Logistic Regression",
    "section": "Probability for winner = “no”",
    "text": "Probability for winner = “no”\n\n# Log-odds (just intercept)\nlo &lt;- coef(mod_winner)[1]\n\n# Convert to probability\np &lt;- exp(lo) / (1 + exp(lo))\np\n\n(Intercept) \n  0.0899663 \n\n\nAbout 9% chance of spam without “winner”"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#probability-for-winner-yes",
    "href": "lectures/week6-r-regression-slides.html#probability-for-winner-yes",
    "title": "Logistic Regression",
    "section": "Probability for winner = “yes”",
    "text": "Probability for winner = “yes”\n\n# Log-odds (intercept + coefficient)\nlo &lt;- coef(mod_winner)[1] + coef(mod_winner)[2]\n\n# Convert to probability\np &lt;- exp(lo) / (1 + exp(lo))\np\n\n(Intercept) \n     0.3125 \n\n\nAbout 31% chance of spam with “winner”—much higher!"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#generating-predictions",
    "href": "lectures/week6-r-regression-slides.html#generating-predictions",
    "title": "Logistic Regression",
    "section": "Generating predictions",
    "text": "Generating predictions\nThe predict() function with type = \"response\" gives predicted \\(P(Y)\\):\n\npredictions &lt;- predict(mod_len, type = \"response\")\n\nggplot(df_spam, aes(x = num_char, y = predictions)) +\n  geom_point(alpha = 0.3) +\n  labs(x = \"Number of characters\",\n       y = \"Predicted P(spam)\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week6-r-regression-slides.html#summary",
    "href": "lectures/week6-r-regression-slides.html#summary",
    "title": "Logistic Regression",
    "section": "Summary",
    "text": "Summary\n\nMany statistical modeling problems involve categorical response variables\nLogistic regression for binary classification tasks\nIt’s a generalized linear model (GLM)\n\nPredicts log-odds of \\(P(Y)\\) as a linear function of \\(X\\)\nLog-odds converted to \\(P(Y)\\) using the logistic function\n\nInterpretation:\n\nLinear relationship with log-odds\nNon-linear relationship with probability"
  },
  {
    "objectID": "lectures/week7-mixed_models.html",
    "href": "lectures/week7-mixed_models.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Classification: dealing with categorical outcomes.\nLogistic regression.\n\nWhy not linear regression?\nGeneralized linear models (GLMs).\nOdds and log-odds.\nThe logistic function.\n\nBuilding and interpreting logistic models with glm."
  },
  {
    "objectID": "lectures/week7-mixed_models.html#goals-of-the-lecture",
    "href": "lectures/week7-mixed_models.html#goals-of-the-lecture",
    "title": "Logistic Regression",
    "section": "",
    "text": "Classification: dealing with categorical outcomes.\nLogistic regression.\n\nWhy not linear regression?\nGeneralized linear models (GLMs).\nOdds and log-odds.\nThe logistic function.\n\nBuilding and interpreting logistic models with glm."
  },
  {
    "objectID": "lectures/week7-mixed_models.html#what-is-classification",
    "href": "lectures/week7-mixed_models.html#what-is-classification",
    "title": "Logistic Regression",
    "section": "What is classification?",
    "text": "What is classification?\n\n“To classify is human…We sort dirty dishes from clean, white laundry from colorfast, important email to be answered from e-junk…. Any part of the home, school, or workplace reveals some such system of classification.”\n— Bowker & Star, 2000\n\n\n\nClassification = predicting a categorical response variable using features\nCommon examples:\n\nIs an email spam or not spam?\nIs a cell mass cancerous or not cancerous?\nWill this customer buy or not buy?\nIs a credit card transaction fraudulent?\nIs this image a cat, dog, person, or other?"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#binary-vs.-multi-class-classification",
    "href": "lectures/week7-mixed_models.html#binary-vs.-multi-class-classification",
    "title": "Logistic Regression",
    "section": "Binary vs. multi-class classification",
    "text": "Binary vs. multi-class classification\n\n\nBinary classification: sorting inputs into one of two labels\n\nE.g., spam vs. not spam\n\nMulti-class classification: more than two labels\n\nE.g., face recognition with n possible identities\nE.g., image classification (cat, dog, person, other)\n\nToday: focus on binary classification with logistic regression"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#part-1-foundations-of-logistic-regression",
    "href": "lectures/week7-mixed_models.html#part-1-foundations-of-logistic-regression",
    "title": "Logistic Regression",
    "section": "Part 1: Foundations of logistic regression",
    "text": "Part 1: Foundations of logistic regression\nMotivation, log-odds, and the logistic function."
  },
  {
    "objectID": "lectures/week7-mixed_models.html#example-dataset-email-spam",
    "href": "lectures/week7-mixed_models.html#example-dataset-email-spam",
    "title": "Logistic Regression",
    "section": "Example dataset: Email spam",
    "text": "Example dataset: Email spam\n\n### Loading the tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf_spam = read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/logistic/spam.csv\")\n\nRows: 3921 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (2): winner, number\ndbl  (18): spam, to_multiple, from, cc, sent_email, image, attach, dollar, i...\ndttm  (1): time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnrow(df_spam)\n\n[1] 3921\n\n\n\nDataset contains information about emails and whether they were spam, along with various features like number of characters, whether they contain certain words, etc."
  },
  {
    "objectID": "lectures/week7-mixed_models.html#why-not-linear-regression",
    "href": "lectures/week7-mixed_models.html#why-not-linear-regression",
    "title": "Logistic Regression",
    "section": "Why not linear regression?",
    "text": "Why not linear regression?\n\n\nspam is coded as 0 (no) or 1 (yes)\nCould we treat this as continuous and use linear regression?\nInterpret prediction \\(\\hat{y}\\) as probability of outcome?\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWhat issues might arise here?"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#the-problem-predictions-beyond-01",
    "href": "lectures/week7-mixed_models.html#the-problem-predictions-beyond-01",
    "title": "Logistic Regression",
    "section": "The problem: predictions beyond [0,1]",
    "text": "The problem: predictions beyond [0,1]\n\nggplot(df_spam, aes(x = num_char, y = spam)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Number of characters\",\n       y = \"Spam (0 = no, 1 = yes)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLinear model generates predictions outside [0,1], but probability must be bounded!"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#framing-the-problem-probabilistically",
    "href": "lectures/week7-mixed_models.html#framing-the-problem-probabilistically",
    "title": "Logistic Regression",
    "section": "Framing the problem probabilistically",
    "text": "Framing the problem probabilistically\n\n\nTreat each outcome as Bernoulli trials: “success” (spam) vs. “failure” (not spam)\nEach observation has independent probability of success: \\(p\\)\nOn its own: \\(p\\) = proportion of spam emails\nGoal: model \\(p\\) conditioned on other variables, i.e., \\(P(Y = 1 | X)\\)\n\n\n\nQuestion: What does \\(p\\) on its own remind you of from linear regression?\n\n\nAnswer: The intercept-only model (the mean of \\(Y\\))"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#generalized-linear-models-glms",
    "href": "lectures/week7-mixed_models.html#generalized-linear-models-glms",
    "title": "Logistic Regression",
    "section": "Generalized linear models (GLMs)",
    "text": "Generalized linear models (GLMs)\n\nGeneralized linear models (GLMs) are generalizations of linear regression.\n\nEach GLM has:\n\n\nA probability distribution for the outcome variable\nA linear model: \\(\\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k\\)\nA link function relating the linear model to the outcome\n\n\n\nWe need a function that links our linear model to a probability score bounded at [0, 1]."
  },
  {
    "objectID": "lectures/week7-mixed_models.html#common-glms",
    "href": "lectures/week7-mixed_models.html#common-glms",
    "title": "Logistic Regression",
    "section": "Common GLMs",
    "text": "Common GLMs\n\n\n\n\n\n\n\n\n\n\nModel name\nDistribution\nLink function\nUse cases\nExample\n\n\n\n\nLinear regression\nNormal\nIdentity\nContinuous response\nHeight, price\n\n\nLogistic regression\nBernoulli/Binomial\nLogit\nBinary response\nSpam, fraud\n\n\nPoisson regression\nPoisson\nLog\nCount data\n# words, # visitors\n\n\n\nToday: logistic regression"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#the-logit-link-function",
    "href": "lectures/week7-mixed_models.html#the-logit-link-function",
    "title": "Logistic Regression",
    "section": "The logit link function",
    "text": "The logit link function\nLogistic regression uses the logit link function:\n\\[\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)\\]\n\n\nWhere \\(p\\) is the probability of some outcome\nTakes a value between \\([0, 1]\\) and maps it to \\((-\\infty, \\infty)\\)\nAlso called the log-odds"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#introducing-the-odds",
    "href": "lectures/week7-mixed_models.html#introducing-the-odds",
    "title": "Logistic Regression",
    "section": "Introducing the odds",
    "text": "Introducing the odds\n\nThe odds of an event are the ratio of the probability of an event occuring (\\(p\\)) and the probability of event not occurring (\\(1-p\\)).\n\n\\[\\text{Odds}(Y) = \\frac{p}{1-p}\\]\n\n\nUnlike \\(p\\), odds are bounded at \\([0, \\infty)\\)\nOdds of 1 means 50/50 chance\nOdds &gt; 1 means more likely to occur than not\nOdds &lt; 1 means less likely to occur than not"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#visualizing-odds",
    "href": "lectures/week7-mixed_models.html#visualizing-odds",
    "title": "Logistic Regression",
    "section": "Visualizing odds",
    "text": "Visualizing odds\n\np &lt;- seq(0.01, 0.99, 0.01)\nodds &lt;- p / (1 - p)\n\nggplot(data.frame(p, odds), aes(x = p, y = odds)) +\n  geom_point() +\n  labs(x = \"P(Y)\", y = \"Odds(Y)\", \n       title = \"Odds(Y) vs. P(Y)\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#introducing-the-log-odds-logit",
    "href": "lectures/week7-mixed_models.html#introducing-the-log-odds-logit",
    "title": "Logistic Regression",
    "section": "Introducing the log-odds (logit)",
    "text": "Introducing the log-odds (logit)\n\nThe log-odds is the log of the odds (the logit function).\n\n\\[\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)\\]\n\n\nUnlike \\(p\\), log-odds are bounded at \\((-\\infty, \\infty)\\)\nThis is what we’ll model linearly!"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#visualizing-log-odds",
    "href": "lectures/week7-mixed_models.html#visualizing-log-odds",
    "title": "Logistic Regression",
    "section": "Visualizing log-odds",
    "text": "Visualizing log-odds\n\nlog_odds &lt;- log(p / (1 - p))\n\nggplot(data.frame(p, log_odds), aes(x = p, y = log_odds)) +\n  geom_point() +\n  labs(x = \"P(Y)\", y = \"Log-odds(Y)\", \n       title = \"Log-odds(Y) vs. P(Y)\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#interpreting-the-sign-of-log-odds",
    "href": "lectures/week7-mixed_models.html#interpreting-the-sign-of-log-odds",
    "title": "Logistic Regression",
    "section": "Interpreting the sign of log-odds",
    "text": "Interpreting the sign of log-odds\n\nggplot(data.frame(p, log_odds), aes(x = p, y = log_odds)) +\n  geom_point() +\n  geom_vline(xintercept = 0.5, linetype = \"dotted\", color = \"red\") +\n  geom_hline(yintercept = 0, linetype = \"dotted\", color = \"red\") +\n  labs(x = \"P(Y)\", y = \"Log-odds(Y)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nPositive log-odds → \\(p &gt; 0.5\\)\nNegative log-odds → \\(p &lt; 0.5\\)"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#is-log-odds-linearly-related-to-p",
    "href": "lectures/week7-mixed_models.html#is-log-odds-linearly-related-to-p",
    "title": "Logistic Regression",
    "section": "Is log-odds linearly related to p?",
    "text": "Is log-odds linearly related to p?\nNo! The log-odds of \\(Y\\) is non-linearly related to \\(P(Y)\\).\n\n\nThis means we cannot interpret linear changes in log-odds as linear changes in probability\nThis will be very important when interpreting logistic regression models"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#the-logistic-function",
    "href": "lectures/week7-mixed_models.html#the-logistic-function",
    "title": "Logistic Regression",
    "section": "The logistic function",
    "text": "The logistic function\n\nThe logistic function is the inverse of the logit function.\n\n\\[P(Y) = \\frac{e^{\\text{logit}(p)}}{1 + e^{\\text{logit}(p)}}\\]\n\n\nConverts log-odds back to probability\nMaps \\((-\\infty, \\infty)\\) to \\([0, 1]\\)\nAlso called the sigmoid function"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#mapping-from-log-odds-to-probability",
    "href": "lectures/week7-mixed_models.html#mapping-from-log-odds-to-probability",
    "title": "Logistic Regression",
    "section": "Mapping from log-odds to probability",
    "text": "Mapping from log-odds to probability\n\nlog_odds_range &lt;- seq(-10, 10, 0.1)\np_range &lt;- exp(log_odds_range) / (1 + exp(log_odds_range))\nggplot(data.frame(log_odds_range, p_range), \n       aes(x = log_odds_range, y = p_range)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Log-odds(Y)\", y = \"P(Y)\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#where-does-regression-come-in",
    "href": "lectures/week7-mixed_models.html#where-does-regression-come-in",
    "title": "Logistic Regression",
    "section": "Where does regression come in?",
    "text": "Where does regression come in?\nWith logistic regression, we learn parameters \\(\\beta\\) for:\n\\[\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k\\]\n\n\nOur “dependent variable” is the log-odds (logit) of \\(p\\)\nWe learn a linear relationship between \\(X\\) and the log-odds of our outcome\nNOT a linear relationship with probability itself!"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#interpreting-β-log-odds-x-is-linear",
    "href": "lectures/week7-mixed_models.html#interpreting-β-log-odds-x-is-linear",
    "title": "Logistic Regression",
    "section": "Interpreting β: log-odds ~ X is linear",
    "text": "Interpreting β: log-odds ~ X is linear\n\\[\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X\\]\n\n\nIf \\(\\beta_1 &gt; 0\\): For each 1-unit increase in \\(X\\), log-odds increase by \\(\\beta_1\\)\nIf \\(\\beta_1 &lt; 0\\): For each 1-unit increase in \\(X\\), log-odds decrease by \\(|\\beta_1|\\)\nStraightforward linear interpretation for log-odds"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#interpreting-β-py-x-is-not-linear",
    "href": "lectures/week7-mixed_models.html#interpreting-β-py-x-is-not-linear",
    "title": "Logistic Regression",
    "section": "Interpreting β: P(Y) ~ X is NOT linear",
    "text": "Interpreting β: P(Y) ~ X is NOT linear\nThe mapping between log-odds and \\(P(Y)\\) is not linear.\n\n\n\n\n\n\n\n\n\nWe cannot interpret coefficients linearly with respect to \\(P(Y)\\)!"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#part-2-logistic-regression-in-r",
    "href": "lectures/week7-mixed_models.html#part-2-logistic-regression-in-r",
    "title": "Logistic Regression",
    "section": "Part 2: Logistic regression in R",
    "text": "Part 2: Logistic regression in R\nUsing glm, interpreting logistic models."
  },
  {
    "objectID": "lectures/week7-mixed_models.html#logistic-regression-in-r",
    "href": "lectures/week7-mixed_models.html#logistic-regression-in-r",
    "title": "Logistic Regression",
    "section": "Logistic regression in R",
    "text": "Logistic regression in R\nUse the glm() function with family = binomial:\n\nmodel &lt;- glm(y ~ x,  ### formula\n             data = df_name,  ## dataframe name\n             family = binomial(link = \"logit\")) ## using logit link\n\n\n\nfamily = binomial: specifies we’re modeling binary outcomes\nlink = \"logit\": specifies the logit link function (default for binomial)\nFitting is straightforward—interpreting is the harder part!"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#fitting-a-simple-model",
    "href": "lectures/week7-mixed_models.html#fitting-a-simple-model",
    "title": "Logistic Regression",
    "section": "Fitting a simple model",
    "text": "Fitting a simple model\nLet’s predict spam from num_char (message length):\n\nmod_len &lt;- glm(spam ~ num_char, \n               data = df_spam, \n               family = binomial)\nsummary(mod_len)\n\n\nCall:\nglm(formula = spam ~ num_char, family = binomial, data = df_spam)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.798738   0.071562 -25.135  &lt; 2e-16 ***\nnum_char    -0.062071   0.008014  -7.746  9.5e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2437.2  on 3920  degrees of freedom\nResidual deviance: 2346.4  on 3919  degrees of freedom\nAIC: 2350.4\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#interpreting-the-coefficients",
    "href": "lectures/week7-mixed_models.html#interpreting-the-coefficients",
    "title": "Logistic Regression",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\n\ncoef(mod_len)\n\n(Intercept)    num_char \n-1.79873764 -0.06207116 \n\n\n\n\nIntercept (-1.80): log-odds of spam when num_char = 0\nnum_char (-0.06): for every 1-unit increase in num_char, log-odds of spam decrease by 0.06\nNegative coefficient → longer emails are less likely to be spam"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#converting-log-odds-to-probability",
    "href": "lectures/week7-mixed_models.html#converting-log-odds-to-probability",
    "title": "Logistic Regression",
    "section": "Converting log-odds to probability",
    "text": "Converting log-odds to probability\nWhat’s \\(P(\\text{spam})\\) when num_char = 0?\n\n# Log-odds (just the intercept)\nlog_odds &lt;- coef(mod_len)[1]\nlog_odds\n\n(Intercept) \n  -1.798738 \n\n# Convert to probability\np &lt;- exp(log_odds) / (1 + exp(log_odds))\np\n\n(Intercept) \n  0.1420048 \n\n\nAbout 14% chance of spam for a message with 0 characters."
  },
  {
    "objectID": "lectures/week7-mixed_models.html#example-num_char-100",
    "href": "lectures/week7-mixed_models.html#example-num_char-100",
    "title": "Logistic Regression",
    "section": "Example: num_char = 100",
    "text": "Example: num_char = 100\nWhat’s \\(P(\\text{spam})\\) when num_char = 100?\n\n# Calculate log-odds\nlog_odds &lt;- coef(mod_len)[1] + coef(mod_len)[2] * 100\nlog_odds\n\n(Intercept) \n  -8.005853 \n\n# Convert to probability\np &lt;- exp(log_odds) / (1 + exp(log_odds))\np\n\n (Intercept) \n0.0003333936 \n\n\nLess than 0.1% chance—very unlikely to be spam!"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#visualizing-log-odds-vs.-probability",
    "href": "lectures/week7-mixed_models.html#visualizing-log-odds-vs.-probability",
    "title": "Logistic Regression",
    "section": "Visualizing: log-odds vs. probability",
    "text": "Visualizing: log-odds vs. probability\n\nX &lt;- df_spam$num_char\nlo &lt;- coef(mod_len)[1] + coef(mod_len)[2] * X\np &lt;- exp(lo) / (1 + exp(lo))\n\npar(mfrow = c(1, 2))\nplot(X, lo, xlab = \"# Characters\", ylab = \"Log-odds(spam)\")\nplot(X, p, xlab = \"# Characters\", ylab = \"P(spam)\")\n\n\n\n\n\n\n\n\nLinear on log-odds scale, non-linear on probability scale!"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#categorical-predictors",
    "href": "lectures/week7-mixed_models.html#categorical-predictors",
    "title": "Logistic Regression",
    "section": "Categorical predictors",
    "text": "Categorical predictors\nLet’s use winner (whether email contains the word “winner”):\n\nmod_winner &lt;- glm(spam ~ winner, \n                  data = df_spam, \n                  family = binomial)\nsummary(mod_winner)\n\n\nCall:\nglm(formula = spam ~ winner, family = binomial, data = df_spam)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.31405    0.05627 -41.121  &lt; 2e-16 ***\nwinneryes    1.52559    0.27549   5.538 3.06e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2437.2  on 3920  degrees of freedom\nResidual deviance: 2412.7  on 3919  degrees of freedom\nAIC: 2416.7\n\nNumber of Fisher Scoring iterations: 5"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#interpreting-categorical-predictors",
    "href": "lectures/week7-mixed_models.html#interpreting-categorical-predictors",
    "title": "Logistic Regression",
    "section": "Interpreting categorical predictors",
    "text": "Interpreting categorical predictors\n\ncoef(mod_winner)\n\n(Intercept)   winneryes \n  -2.314047    1.525589 \n\n\n\n\nIntercept (-2.31): log-odds of spam when winner = \"no\"\nwinneryes (1.53): change in log-odds (relative to intercept) when winner = \"yes\"\nJust like linear regression: categorical predictors are relative to reference level"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#probability-for-winner-no",
    "href": "lectures/week7-mixed_models.html#probability-for-winner-no",
    "title": "Logistic Regression",
    "section": "Probability for winner = “no”",
    "text": "Probability for winner = “no”\n\n# Log-odds (just intercept)\nlo &lt;- coef(mod_winner)[1]\n\n# Convert to probability\np &lt;- exp(lo) / (1 + exp(lo))\np\n\n(Intercept) \n  0.0899663 \n\n\nAbout 9% chance of spam without “winner”"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#probability-for-winner-yes",
    "href": "lectures/week7-mixed_models.html#probability-for-winner-yes",
    "title": "Logistic Regression",
    "section": "Probability for winner = “yes”",
    "text": "Probability for winner = “yes”\n\n# Log-odds (intercept + coefficient)\nlo &lt;- coef(mod_winner)[1] + coef(mod_winner)[2]\n\n# Convert to probability\np &lt;- exp(lo) / (1 + exp(lo))\np\n\n(Intercept) \n     0.3125 \n\n\nAbout 31% chance of spam with “winner”—much higher!"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#generating-predictions",
    "href": "lectures/week7-mixed_models.html#generating-predictions",
    "title": "Logistic Regression",
    "section": "Generating predictions",
    "text": "Generating predictions\nThe predict() function with type = \"response\" gives predicted \\(P(Y)\\):\n\npredictions &lt;- predict(mod_len, type = \"response\")\n\nggplot(df_spam, aes(x = num_char, y = predictions)) +\n  geom_point(alpha = 0.3) +\n  labs(x = \"Number of characters\",\n       y = \"Predicted P(spam)\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week7-mixed_models.html#summary",
    "href": "lectures/week7-mixed_models.html#summary",
    "title": "Logistic Regression",
    "section": "Summary",
    "text": "Summary\n\n\nMany statistical modeling problems involve categorical response variables\nLogistic regression for binary classification tasks\nIt’s a generalized linear model (GLM)\n\nPredicts log-odds of \\(P(Y)\\) as a linear function of \\(X\\)\nLog-odds converted to \\(P(Y)\\) using the logistic function\n\nInterpretation:\n\nLinear relationship with log-odds\nNon-linear relationship with probability"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#goals-of-the-lecture",
    "href": "lectures/week3-r-visualization-slides.html#goals-of-the-lecture",
    "title": "Data Visualization in R",
    "section": "Goals of the lecture",
    "text": "Goals of the lecture\n\nData visualization and exploratory data analysis (EDA).\nBasic principles of data visualization.\nggplot: theory and practice.\nOther plotting add-ons (ggridges, ggcorrplot)."
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#what-is-data-visualization",
    "href": "lectures/week3-r-visualization-slides.html#what-is-data-visualization",
    "title": "Data Visualization in R",
    "section": "What is data visualization?",
    "text": "What is data visualization?\n\nData visualization is the process (and result) of representing data graphically.\n\nWe’ll be focusing on common visualization techniques, such as:\n\nHistograms.\nScatterplots.\nBarplots.\nBoxplots.\n\n\n\n\n\nNote\n\n\nWe’ll also discuss why visualization is so crucial."
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#why-visualization",
    "href": "lectures/week3-r-visualization-slides.html#why-visualization",
    "title": "Data Visualization in R",
    "section": "Why visualization?",
    "text": "Why visualization?\nData visualization serves (at least) a few different purposes:\n\nExploratory data analysis (EDA): discovering relationships in your data, generating hypotheses, confirming intuitions.\n\nCommunicating insights: given some finding, conveying that clearly and accurately.\nImpacting the world: a good (or bad) visualization can change attitudes!"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#eda-checking-your-assumptions",
    "href": "lectures/week3-r-visualization-slides.html#eda-checking-your-assumptions",
    "title": "Data Visualization in R",
    "section": "EDA: Checking your assumptions",
    "text": "EDA: Checking your assumptions\n\n### Loading the tidyverse\nlibrary(tidyverse)\n### Plot anscombe's quartet\nanscombe %&gt;%\n  pivot_longer(\n    cols = everything(),\n    names_to = c(\".value\", \"dataset\"),\n    names_pattern = \"(x|y)(.*)\"\n  ) %&gt;%\n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~dataset)"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#dataviz-impacting-the-world-1",
    "href": "lectures/week3-r-visualization-slides.html#dataviz-impacting-the-world-1",
    "title": "Data Visualization in R",
    "section": "DataViz: impacting the world (1)",
    "text": "DataViz: impacting the world (1)\nFlorence Nightingale (1820-1910) was a social reformer, statistician, and founder of modern nursing."
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#dataviz-impacting-the-world-2",
    "href": "lectures/week3-r-visualization-slides.html#dataviz-impacting-the-world-2",
    "title": "Data Visualization in R",
    "section": "DataViz: impacting the world (2)",
    "text": "DataViz: impacting the world (2)\nJohn Snow (1813-1858) was a physician whose visualization of cholera outbreaks helped identify the source and spreading mechanism (water supply)."
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#what-makes-a-good-data-visualization",
    "href": "lectures/week3-r-visualization-slides.html#what-makes-a-good-data-visualization",
    "title": "Data Visualization in R",
    "section": "What makes a good data visualization?",
    "text": "What makes a good data visualization?\nEdward Tufte argues:\n\nGraphical excellence is the well-designed presentation of interesting data—a matter of substance, of statistics, and of design … [It] consists of complex ideas communicated with clarity, precision, and efficiency. … [It] is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space … [It] is nearly always multivariate … And graphical excellence requires telling the truth about the data. (Tufte, 1983, p. 51).\n\nSome principles:\n\nUse your ink wisely.\nBe true to the data.\nConsider the visual logic of the figure.\nOrder matters.\nKeep scales consistent."
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#principle-1-use-your-ink-wisely",
    "href": "lectures/week3-r-visualization-slides.html#principle-1-use-your-ink-wisely",
    "title": "Data Visualization in R",
    "section": "Principle 1: Use your ink wisely",
    "text": "Principle 1: Use your ink wisely\n\nEvery element in your visualization should serve a purpose.\nRemove “chart junk”: unnecessary gridlines, borders, 3D effects, decorations.\nMaximize your data-ink ratio: the proportion of ink used to display actual data.\n\n\n\n\n\nTufte’s principle\n\n\n“Above all else show the data.” - Edward Tufte"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#principle-2-be-true-to-the-data",
    "href": "lectures/week3-r-visualization-slides.html#principle-2-be-true-to-the-data",
    "title": "Data Visualization in R",
    "section": "Principle 2: Be true to the data",
    "text": "Principle 2: Be true to the data\n\nDon’t manipulate scales to exaggerate or hide effects.\nInclude zero baseline for bar charts (unless there’s good reason not to).\nAvoid cherry-picking data or timeframes.\nRepresent uncertainty when appropriate (e.g., error bars, confidence intervals)."
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#principle-3-consider-the-visual-logic",
    "href": "lectures/week3-r-visualization-slides.html#principle-3-consider-the-visual-logic",
    "title": "Data Visualization in R",
    "section": "Principle 3: Consider the visual logic",
    "text": "Principle 3: Consider the visual logic\n\nPosition is probably easiest to judge accurately.\nAngle and area are harder (e.g., pie charts).\nColor hue can work for categorical data.\n\nUse distinctive and meaningful colors!\n\nStacked bar plots often hard to interpret!\n\n\n\n\n\nTip\n\n\nThis will be relevant when thinking about the layers in ggplot."
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#principle-4-order-matters",
    "href": "lectures/week3-r-visualization-slides.html#principle-4-order-matters",
    "title": "Data Visualization in R",
    "section": "Principle 4: Order matters",
    "text": "Principle 4: Order matters\n\nFor categorical data: order by frequency or a meaningful sequence.\nFor ordinal data: maintain the natural order (e.g., Strongly Disagree → Strongly Agree).\nFor time series: always order chronologically."
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#principle-5-keep-scales-consistent",
    "href": "lectures/week3-r-visualization-slides.html#principle-5-keep-scales-consistent",
    "title": "Data Visualization in R",
    "section": "Principle 5: Keep scales consistent",
    "text": "Principle 5: Keep scales consistent\n\nUse the same axis ranges for meaningful comparison.\nIn faceted plots, decide: fixed scales (scales = “fixed”) or free scales (scales = “free”)?\n\nFree scales can be misleading but useful when ranges differ greatly.\n\n\n\n\n\n\nRule of thumb\n\n\nUse consistent scales when inviting direct comparison; use free scales when showing patterns within each group."
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#ggplot2-theory-and-practice",
    "href": "lectures/week3-r-visualization-slides.html#ggplot2-theory-and-practice",
    "title": "Data Visualization in R",
    "section": "ggplot2: theory and practice",
    "text": "ggplot2: theory and practice\n\nggplot2 is a system for creating graphics, based on the Grammar of Graphics.\n\nJust like natural language has a grammar (nouns, verbs, adjectives), graphics have a grammar too:\n\nData: What you want to visualize.\nAesthetics (aes): How variables map to visual properties (x, y, color, size).\nGeometries (geom): The type of plot (points, lines, bars).\nScales: Control how aesthetic mappings appear.\nFacets: Split into multiple subplots.\nThemes: Control non-data appearance (fonts, backgrounds).\n\n\n\n\n\nTip\n\n\n`gplot builds plots by adding layers with the + operator."
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#anatomy-of-a-ggplot",
    "href": "lectures/week3-r-visualization-slides.html#anatomy-of-a-ggplot",
    "title": "Data Visualization in R",
    "section": "Anatomy of a “ggplot”",
    "text": "Anatomy of a “ggplot”\nEvery ggplot needs, at minimum:\n\nData: a dataframe or tibble.\nAesthetic mappings: which variables map to which visual properties.\nGeometry: how to represent the data visually.\n\n\nggplot(data = mpg,                          # 1. Data\n       aes(x = displ, y = hwy, color = class)) + # 2. Aesthetics\n  geom_point() # 3. Geometry"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#histograms",
    "href": "lectures/week3-r-visualization-slides.html#histograms",
    "title": "Data Visualization in R",
    "section": "Histograms",
    "text": "Histograms\n\nA histogram is a visualization of a single continuous, quantitative variable (e.g., income or temperature).\n\nA histogram can be created with geom_histogram.\n\nmpg %&gt;%\n  ggplot(aes(x = cty)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n💭 Check-in\n\n\nWhat happens if you modify bins or binwidth?"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#histograms-1",
    "href": "lectures/week3-r-visualization-slides.html#histograms-1",
    "title": "Data Visualization in R",
    "section": "Histograms",
    "text": "Histograms\n\nA histogram is a visualization of a single continuous, quantitative variable (e.g., income or temperature).\n\nA histogram can be created with geom_histogram.\n\n\n\n\n💭 Check-in\n\n\nWhat happens if you modify bins or binwidth?"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#histograms-are-very-useful",
    "href": "lectures/week3-r-visualization-slides.html#histograms-are-very-useful",
    "title": "Data Visualization in R",
    "section": "Histograms are very useful!",
    "text": "Histograms are very useful!\nHistograms show important visual information about a distribution:\n\nShape: is it symmetric, skewed, etc.?\nCenter: Where is the “typical” value?\nSpread : How variable is the data?\nOutliers: Are there unusual values?\n\n\n\n\n\n💭 Check-in\n\n\nHow does the skew of a distribution affect measures of central tendency such as the mean or median?"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#histograms-vs.-density-plots",
    "href": "lectures/week3-r-visualization-slides.html#histograms-vs.-density-plots",
    "title": "Data Visualization in R",
    "section": "Histograms vs. density plots",
    "text": "Histograms vs. density plots\n\nA density plot is a smoothed alternative to a histogram, created using kernel density estimation (KDE).\n\nA density plot can be created with geom_density.\n\nggplot(mpg, aes(x = cty)) + \n  geom_density(fill = \"steelblue\")"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#overlaying-multiple-distributions",
    "href": "lectures/week3-r-visualization-slides.html#overlaying-multiple-distributions",
    "title": "Data Visualization in R",
    "section": "Overlaying multiple distributions",
    "text": "Overlaying multiple distributions\nOne benefit of a density plot is that it’s easier to overlay multiple distributions on the same plot. The alpha parameter controls the opacity of each distribution.\n\nmpg %&gt;%\n  filter(class %in% c(\"compact\", \"suv\")) %&gt;%\n  ggplot(aes(x = cty, fill = class)) + \n  geom_density(alpha = .5) +\n  labs(title = \"City MPG: Compact vs. SUV\", fill = \"Car Type\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#scatterplots",
    "href": "lectures/week3-r-visualization-slides.html#scatterplots",
    "title": "Data Visualization in R",
    "section": "Scatterplots",
    "text": "Scatterplots\n\nA scatterplot shows the relationship between two continuous variables, where each point represents an observation.\n\nA scatterplot can be created with geom_point.\n\nmpg %&gt;%\n  ggplot(aes(x = cty, y = hwy)) + \n  geom_point()"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#adding-layers-to-a-scatterplot",
    "href": "lectures/week3-r-visualization-slides.html#adding-layers-to-a-scatterplot",
    "title": "Data Visualization in R",
    "section": "Adding layers to a scatterplot",
    "text": "Adding layers to a scatterplot\nWe can further modify the color, size, and shape of individual dots.\n\nmpg %&gt;%\n  ggplot(aes(x = cty, y = hwy, color = class, size = cyl, shape = drv)) + \n  geom_point(alpha = .5) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nUse categorical variables for shape, categorical or continuous variables for color, and ordinal or continuous variables for size."
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#plotting-a-regression-line",
    "href": "lectures/week3-r-visualization-slides.html#plotting-a-regression-line",
    "title": "Data Visualization in R",
    "section": "Plotting a regression line",
    "text": "Plotting a regression line\nWe can also use geom_smooth to plot a regression line (or another non-linear function) over our scatterplot.\n(If there are multiple colors, etc., a different line will be plotted for each color.)\n\nmpg %&gt;%\n  ggplot(aes(x = cty, y = hwy)) + \n  geom_smooth(method = \"lm\") +\n  geom_point(alpha = .5)"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#bar-plots",
    "href": "lectures/week3-r-visualization-slides.html#bar-plots",
    "title": "Data Visualization in R",
    "section": "Bar plots",
    "text": "Bar plots\n\nA barplot visualizes the relationship between one continuous variable and (at least one) categorical variable.\n\nA barplot can be created with geom_bar.\n\nBy default, geom_bar will count occurrences (like a histogram for categorical variables).\nYou can also calculate values like the mean using geom_bar(stat = \"summary\").\nIf you already have values computed (e.g., a mean or count), you can use geom_col or `geom_bar(stat = “identity”)."
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#bar-plots-counts",
    "href": "lectures/week3-r-visualization-slides.html#bar-plots-counts",
    "title": "Data Visualization in R",
    "section": "Bar plots: counts",
    "text": "Bar plots: counts\nBy default, geom_bar will count the occurrences of each class.\n\nmpg %&gt;%\n  ggplot(aes(x = drv)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n💭 Check-in\n\n\nCreate a barplot showing the counts of some other categorical variable in the mpg dataframe."
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#bar-plots-summaries",
    "href": "lectures/week3-r-visualization-slides.html#bar-plots-summaries",
    "title": "Data Visualization in R",
    "section": "Bar plots: summaries",
    "text": "Bar plots: summaries\nYou can also calculate summary statistics, i.e., the mean of some y variable for each level of the x variable.\nUse reorder to reorder the bars in terms of their values.\n\nmpg %&gt;%\n  ggplot(aes(x = reorder(drv, hwy), y = hwy)) +\n  geom_bar(stat = \"summary\", fun = \"mean\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#bar-plots-with-group_by",
    "href": "lectures/week3-r-visualization-slides.html#bar-plots-with-group_by",
    "title": "Data Visualization in R",
    "section": "Bar plots with group_by",
    "text": "Bar plots with group_by\nAlternatively, you can calculate summary statistics using group_by %&gt;% summarise, and pipe the output into a ggplot call.\n\nmpg %&gt;%\n  group_by(drv) %&gt;%\n  summarise(mean_hwy = mean(hwy)) %&gt;%\n  ggplot(aes(x = reorder(drv, mean_hwy), y = mean_hwy)) +\n  geom_bar(stat = \"identity\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n💭 Check-in\n\n\nHow would you instead plot the mean cty miles per gallon?"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#error-bars-stat_summary",
    "href": "lectures/week3-r-visualization-slides.html#error-bars-stat_summary",
    "title": "Data Visualization in R",
    "section": "Error bars: stat_summary",
    "text": "Error bars: stat_summary\nOften, you want to display some measure of dispersion in addition to the mean. One approach is to use stat_summary.\n\nmpg %&gt;%\n  ggplot(aes(x = reorder(drv, hwy), y = hwy)) +\n  geom_bar(stat = \"summary\", fun = \"mean\") +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +\n  labs(x = \"Drive Type\", y = \"Mean Highway MPG\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#error-bars-with-group_by",
    "href": "lectures/week3-r-visualization-slides.html#error-bars-with-group_by",
    "title": "Data Visualization in R",
    "section": "Error bars with group_by",
    "text": "Error bars with group_by\nAlternatively, with the group_by method, you can use first calculate the standard error, then pipe the result into geom_errorbar.\n\nmpg %&gt;%\n  group_by(drv) %&gt;%\n  summarise(\n    mean_hwy = mean(hwy),\n    se_hwy = sd(hwy) / sqrt(n())\n  ) %&gt;%\n  ggplot(aes(x = reorder(drv, mean_hwy), y = mean_hwy)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin = mean_hwy - se_hwy, \n                    ymax = mean_hwy + se_hwy),\n                width = 0.2) +\n  labs(x = \"Drive Type\", y = \"Mean Highway MPG\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n💭 Check-in\n\n\nHow would you plot a confidence interval for two standard errors?"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#error-bars-with-group_by-1",
    "href": "lectures/week3-r-visualization-slides.html#error-bars-with-group_by-1",
    "title": "Data Visualization in R",
    "section": "Error bars with group_by",
    "text": "Error bars with group_by\nAlternatively, with the group_by method, you can use first calculate the standard error, then pipe the result into geom_errorbar.\n\n\n\n\n💭 Check-in\n\n\nHow would you plot a confidence interval for two standard errors?"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#bar-plots-with-fill",
    "href": "lectures/week3-r-visualization-slides.html#bar-plots-with-fill",
    "title": "Data Visualization in R",
    "section": "Bar plots with fill",
    "text": "Bar plots with fill\nYou can add further information to a barplot using the fill parameter. Use position_dodge to show the bars side by side (rather than stacked).\n\nmpg %&gt;%\n  ggplot(aes(x = reorder(drv, hwy), y = hwy, fill = class)) +\n  geom_bar(stat = \"summary\", fun = \"mean\", position = position_dodge(width = 0.9)) +\n  stat_summary(fun.data = mean_se, \n               geom = \"errorbar\", \n               position = position_dodge(width = 0.9), \n               width = 0.2) +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#boxplots-and-violin-plots",
    "href": "lectures/week3-r-visualization-slides.html#boxplots-and-violin-plots",
    "title": "Data Visualization in R",
    "section": "Boxplots and violin plots",
    "text": "Boxplots and violin plots\n\nBoxplots and violin plots show more detailed information about underlying distribution for a given category.\n\n\nA boxplot shows the median, along with the inter-quartile range.\nA violinplot shows the full distribution as a density curve, rotated and mirrored.\nAs with barplots, you can also modify the color of each box/violin."
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#boxplots-with-geom_boxplot",
    "href": "lectures/week3-r-visualization-slides.html#boxplots-with-geom_boxplot",
    "title": "Data Visualization in R",
    "section": "Boxplots with geom_boxplot",
    "text": "Boxplots with geom_boxplot\n\nmpg %&gt;%\n  ggplot(aes(x = reorder(drv, hwy), y = hwy)) +\n  geom_boxplot() +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#violin-plots-with-geom_violin",
    "href": "lectures/week3-r-visualization-slides.html#violin-plots-with-geom_violin",
    "title": "Data Visualization in R",
    "section": "Violin plots with geom_violin",
    "text": "Violin plots with geom_violin\n\nmpg %&gt;%\n  ggplot(aes(x = reorder(drv, hwy), y = hwy)) +\n  geom_violin() +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#adding-styling-to-a-plot",
    "href": "lectures/week3-r-visualization-slides.html#adding-styling-to-a-plot",
    "title": "Data Visualization in R",
    "section": "Adding styling to a plot",
    "text": "Adding styling to a plot\nLet’s revisit a plot we worked on earlier, with better labels and styling.\n\nmpg %&gt;%\n  ggplot(aes(x = cty, y = hwy, color = drv, size = cyl, shape = drv)) + \n  geom_point(alpha = .5) +\n  labs(x = \"City mpg\",\n       y = \"Highway mpg\",\n       size = \"Number of cylinders\",\n       color = \"Drive Train\",\n       shape = \"Drive Train\") +\n  theme_minimal() +\n  scale_color_viridis_d() +\n  theme(\n    axis.title = element_text(size = 16),\n    legend.title = element_text(size = 12),\n    legend.text = element_text(size = 10)\n  )"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#labeling-specific-points",
    "href": "lectures/week3-r-visualization-slides.html#labeling-specific-points",
    "title": "Data Visualization in R",
    "section": "Labeling specific points",
    "text": "Labeling specific points\nLet’s revisit a plot we worked on earlier, with better labels and styling.\n\nmpg_labeled &lt;- mpg %&gt;%\n  filter(hwy &gt; 42) %&gt;%\n  filter(class == \"compact\")\n\nmpg %&gt;%\n  ggplot(aes(x = cty, y = hwy, color = drv, size = cyl, shape = drv)) + \n  geom_point(alpha = .5) +\n  geom_text(data = mpg_labeled, \n            aes(label = model), \n            hjust = 1.2, vjust = 0, \n            size = 4, \n            color = \"black\",\n            show.legend = FALSE) +\n  theme_minimal(base_size = 14)"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#other-plotting-packages",
    "href": "lectures/week3-r-visualization-slides.html#other-plotting-packages",
    "title": "Data Visualization in R",
    "section": "Other plotting packages",
    "text": "Other plotting packages\nggplot has a ton of useful functions and geom types, and it’s probably all you “need”——but there are other options too.\n\nggcorrplot: gg-style correlation matrices.\nggridges: gg-style “ridge” plots (density plots)."
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#using-ggcorrplot",
    "href": "lectures/week3-r-visualization-slides.html#using-ggcorrplot",
    "title": "Data Visualization in R",
    "section": "Using ggcorrplot",
    "text": "Using ggcorrplot\n\nggcorrplot is a library (and function) that visualizes a correlation matrix.\n\n\nlibrary(ggcorrplot)\n\ncor_matrix = mpg %&gt;%\n  select(hwy, cty, displ, cyl) %&gt;%\n  cor()\nggcorrplot(cor_matrix)"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#using-ggridges",
    "href": "lectures/week3-r-visualization-slides.html#using-ggridges",
    "title": "Data Visualization in R",
    "section": "Using ggridges",
    "text": "Using ggridges\n\nggridges is a library for arranging density plots in a staggered fashion.\n\n\nlibrary(ggridges)\nmpg %&gt;%\n  ggplot(aes(x = hwy, y = reorder(class, hwy), fill = drv)) +\n  geom_density_ridges(alpha = .7, color = NA) +\n  theme_ridges()"
  },
  {
    "objectID": "lectures/week3-r-visualization-slides.html#summary",
    "href": "lectures/week3-r-visualization-slides.html#summary",
    "title": "Data Visualization in R",
    "section": "Summary",
    "text": "Summary\n\nData visualization is central to CSS.\n\nCrucial for exploring data, communicating insights, and impacting decisions.\n\nggplot is a versatile and powerful library for creating clear, elegant figures.\nR also supports a number of additional libraries for visualization.\nThe best way to learn to make visualizations is to make them!"
  },
  {
    "objectID": "lectures/week3-visualization.html",
    "href": "lectures/week3-visualization.html",
    "title": "Data Visualization in R",
    "section": "",
    "text": "Data visualization and exploratory data analysis (EDA).\nBasic principles of data visualization.\nggplot: theory and practice.\nOther plotting add-ons (ggridges, ggcorrplot)."
  },
  {
    "objectID": "lectures/week3-visualization.html#goals-of-the-lecture",
    "href": "lectures/week3-visualization.html#goals-of-the-lecture",
    "title": "Data Visualization in R",
    "section": "",
    "text": "Data visualization and exploratory data analysis (EDA).\nBasic principles of data visualization.\nggplot: theory and practice.\nOther plotting add-ons (ggridges, ggcorrplot)."
  },
  {
    "objectID": "lectures/week3-visualization.html#what-is-data-visualization",
    "href": "lectures/week3-visualization.html#what-is-data-visualization",
    "title": "Data Visualization in R",
    "section": "What is data visualization?",
    "text": "What is data visualization?\n\nData visualization is the process (and result) of representing data graphically.\n\nWe’ll be focusing on common visualization techniques, such as:\n\n\nHistograms.\nScatterplots.\nBarplots.\nBoxplots.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe’ll also discuss why visualization is so crucial."
  },
  {
    "objectID": "lectures/week3-visualization.html#why-visualization",
    "href": "lectures/week3-visualization.html#why-visualization",
    "title": "Data Visualization in R",
    "section": "Why visualization?",
    "text": "Why visualization?\nData visualization serves (at least) a few different purposes:\n\n\nExploratory data analysis (EDA): discovering relationships in your data, generating hypotheses, confirming intuitions.\n\nCommunicating insights: given some finding, conveying that clearly and accurately.\nImpacting the world: a good (or bad) visualization can change attitudes!"
  },
  {
    "objectID": "lectures/week3-visualization.html#eda-checking-your-assumptions",
    "href": "lectures/week3-visualization.html#eda-checking-your-assumptions",
    "title": "Data Visualization in R",
    "section": "EDA: Checking your assumptions",
    "text": "EDA: Checking your assumptions\n\n### Loading the tidyverse\nlibrary(tidyverse)\n### Plot anscombe's quartet\nanscombe %&gt;%\n  pivot_longer(\n    cols = everything(),\n    names_to = c(\".value\", \"dataset\"),\n    names_pattern = \"(x|y)(.*)\"\n  ) %&gt;%\n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~dataset)"
  },
  {
    "objectID": "lectures/week3-visualization.html#dataviz-impacting-the-world-1",
    "href": "lectures/week3-visualization.html#dataviz-impacting-the-world-1",
    "title": "Data Visualization in R",
    "section": "DataViz: impacting the world (1)",
    "text": "DataViz: impacting the world (1)\nFlorence Nightingale (1820-1910) was a social reformer, statistician, and founder of modern nursing."
  },
  {
    "objectID": "lectures/week3-visualization.html#dataviz-impacting-the-world-2",
    "href": "lectures/week3-visualization.html#dataviz-impacting-the-world-2",
    "title": "Data Visualization in R",
    "section": "DataViz: impacting the world (2)",
    "text": "DataViz: impacting the world (2)\nJohn Snow (1813-1858) was a physician whose visualization of cholera outbreaks helped identify the source and spreading mechanism (water supply)."
  },
  {
    "objectID": "lectures/week3-visualization.html#what-makes-a-good-data-visualization",
    "href": "lectures/week3-visualization.html#what-makes-a-good-data-visualization",
    "title": "Data Visualization in R",
    "section": "What makes a good data visualization?",
    "text": "What makes a good data visualization?\nEdward Tufte argues:\n\nGraphical excellence is the well-designed presentation of interesting data—a matter of substance, of statistics, and of design … [It] consists of complex ideas communicated with clarity, precision, and efficiency. … [It] is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space … [It] is nearly always multivariate … And graphical excellence requires telling the truth about the data. (Tufte, 1983, p. 51).\n\nSome principles:\n\n\nUse your ink wisely.\nBe true to the data.\nConsider the visual logic of the figure.\nOrder matters.\nKeep scales consistent."
  },
  {
    "objectID": "lectures/week3-visualization.html#principle-1-use-your-ink-wisely",
    "href": "lectures/week3-visualization.html#principle-1-use-your-ink-wisely",
    "title": "Data Visualization in R",
    "section": "Principle 1: Use your ink wisely",
    "text": "Principle 1: Use your ink wisely\n\n\nEvery element in your visualization should serve a purpose.\nRemove “chart junk”: unnecessary gridlines, borders, 3D effects, decorations.\nMaximize your data-ink ratio: the proportion of ink used to display actual data.\n\n\n\n\n\n\n\n\n\nTipTufte’s principle\n\n\n\n“Above all else show the data.” - Edward Tufte"
  },
  {
    "objectID": "lectures/week3-visualization.html#principle-2-be-true-to-the-data",
    "href": "lectures/week3-visualization.html#principle-2-be-true-to-the-data",
    "title": "Data Visualization in R",
    "section": "Principle 2: Be true to the data",
    "text": "Principle 2: Be true to the data\n\n\nDon’t manipulate scales to exaggerate or hide effects.\nInclude zero baseline for bar charts (unless there’s good reason not to).\nAvoid cherry-picking data or timeframes.\nRepresent uncertainty when appropriate (e.g., error bars, confidence intervals)."
  },
  {
    "objectID": "lectures/week3-visualization.html#principle-3-consider-the-visual-logic",
    "href": "lectures/week3-visualization.html#principle-3-consider-the-visual-logic",
    "title": "Data Visualization in R",
    "section": "Principle 3: Consider the visual logic",
    "text": "Principle 3: Consider the visual logic\n\n\nPosition is probably easiest to judge accurately.\nAngle and area are harder (e.g., pie charts).\nColor hue can work for categorical data.\n\nUse distinctive and meaningful colors!\n\nStacked bar plots often hard to interpret!\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis will be relevant when thinking about the layers in ggplot."
  },
  {
    "objectID": "lectures/week3-visualization.html#principle-4-order-matters",
    "href": "lectures/week3-visualization.html#principle-4-order-matters",
    "title": "Data Visualization in R",
    "section": "Principle 4: Order matters",
    "text": "Principle 4: Order matters\n\n\nFor categorical data: order by frequency or a meaningful sequence.\nFor ordinal data: maintain the natural order (e.g., Strongly Disagree → Strongly Agree).\nFor time series: always order chronologically."
  },
  {
    "objectID": "lectures/week3-visualization.html#principle-5-keep-scales-consistent",
    "href": "lectures/week3-visualization.html#principle-5-keep-scales-consistent",
    "title": "Data Visualization in R",
    "section": "Principle 5: Keep scales consistent",
    "text": "Principle 5: Keep scales consistent\n\n\nUse the same axis ranges for meaningful comparison.\nIn faceted plots, decide: fixed scales (scales = “fixed”) or free scales (scales = “free”)?\n\nFree scales can be misleading but useful when ranges differ greatly.\n\n\n\n\n\n\n\n\n\n\nTipRule of thumb\n\n\n\nUse consistent scales when inviting direct comparison; use free scales when showing patterns within each group."
  },
  {
    "objectID": "lectures/week3-visualization.html#ggplot2-theory-and-practice",
    "href": "lectures/week3-visualization.html#ggplot2-theory-and-practice",
    "title": "Data Visualization in R",
    "section": "ggplot2: theory and practice",
    "text": "ggplot2: theory and practice\n\nggplot2 is a system for creating graphics, based on the Grammar of Graphics.\n\nJust like natural language has a grammar (nouns, verbs, adjectives), graphics have a grammar too:\n\n\nData: What you want to visualize.\nAesthetics (aes): How variables map to visual properties (x, y, color, size).\nGeometries (geom): The type of plot (points, lines, bars).\nScales: Control how aesthetic mappings appear.\nFacets: Split into multiple subplots.\nThemes: Control non-data appearance (fonts, backgrounds).\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n`gplot builds plots by adding layers with the + operator."
  },
  {
    "objectID": "lectures/week3-visualization.html#anatomy-of-a-ggplot",
    "href": "lectures/week3-visualization.html#anatomy-of-a-ggplot",
    "title": "Data Visualization in R",
    "section": "Anatomy of a “ggplot”",
    "text": "Anatomy of a “ggplot”\nEvery ggplot needs, at minimum:\n\nData: a dataframe or tibble.\nAesthetic mappings: which variables map to which visual properties.\nGeometry: how to represent the data visually.\n\n\nggplot(data = mpg,                          # 1. Data\n       aes(x = displ, y = hwy, color = class)) + # 2. Aesthetics\n  geom_point() # 3. Geometry"
  },
  {
    "objectID": "lectures/week3-visualization.html#histograms",
    "href": "lectures/week3-visualization.html#histograms",
    "title": "Data Visualization in R",
    "section": "Histograms",
    "text": "Histograms\n\nA histogram is a visualization of a single continuous, quantitative variable (e.g., income or temperature).\n\nA histogram can be created with geom_histogram.\n\nmpg %&gt;%\n  ggplot(aes(x = cty)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip💭 Check-in\n\n\n\nWhat happens if you modify bins or binwidth?"
  },
  {
    "objectID": "lectures/week3-visualization.html#histograms-1",
    "href": "lectures/week3-visualization.html#histograms-1",
    "title": "Data Visualization in R",
    "section": "Histograms",
    "text": "Histograms\n\nA histogram is a visualization of a single continuous, quantitative variable (e.g., income or temperature).\n\nA histogram can be created with geom_histogram.\n\n\n\n\n\n\n\nTip💭 Check-in\n\n\n\nWhat happens if you modify bins or binwidth?"
  },
  {
    "objectID": "lectures/week3-visualization.html#histograms-are-very-useful",
    "href": "lectures/week3-visualization.html#histograms-are-very-useful",
    "title": "Data Visualization in R",
    "section": "Histograms are very useful!",
    "text": "Histograms are very useful!\nHistograms show important visual information about a distribution:\n\n\nShape: is it symmetric, skewed, etc.?\nCenter: Where is the “typical” value?\nSpread : How variable is the data?\nOutliers: Are there unusual values?\n\n\n\n\n\n\n\n\n\nTip💭 Check-in\n\n\n\nHow does the skew of a distribution affect measures of central tendency such as the mean or median?"
  },
  {
    "objectID": "lectures/week3-visualization.html#histograms-vs.-density-plots",
    "href": "lectures/week3-visualization.html#histograms-vs.-density-plots",
    "title": "Data Visualization in R",
    "section": "Histograms vs. density plots",
    "text": "Histograms vs. density plots\n\nA density plot is a smoothed alternative to a histogram, created using kernel density estimation (KDE).\n\nA density plot can be created with geom_density.\n\nggplot(mpg, aes(x = cty)) + \n  geom_density(fill = \"steelblue\")"
  },
  {
    "objectID": "lectures/week3-visualization.html#overlaying-multiple-distributions",
    "href": "lectures/week3-visualization.html#overlaying-multiple-distributions",
    "title": "Data Visualization in R",
    "section": "Overlaying multiple distributions",
    "text": "Overlaying multiple distributions\nOne benefit of a density plot is that it’s easier to overlay multiple distributions on the same plot. The alpha parameter controls the opacity of each distribution.\n\nmpg %&gt;%\n  filter(class %in% c(\"compact\", \"suv\")) %&gt;%\n  ggplot(aes(x = cty, fill = class)) + \n  geom_density(alpha = .5) +\n  labs(title = \"City MPG: Compact vs. SUV\", fill = \"Car Type\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week3-visualization.html#scatterplots",
    "href": "lectures/week3-visualization.html#scatterplots",
    "title": "Data Visualization in R",
    "section": "Scatterplots",
    "text": "Scatterplots\n\nA scatterplot shows the relationship between two continuous variables, where each point represents an observation.\n\nA scatterplot can be created with geom_point.\n\nmpg %&gt;%\n  ggplot(aes(x = cty, y = hwy)) + \n  geom_point()"
  },
  {
    "objectID": "lectures/week3-visualization.html#adding-layers-to-a-scatterplot",
    "href": "lectures/week3-visualization.html#adding-layers-to-a-scatterplot",
    "title": "Data Visualization in R",
    "section": "Adding layers to a scatterplot",
    "text": "Adding layers to a scatterplot\nWe can further modify the color, size, and shape of individual dots.\n\nmpg %&gt;%\n  ggplot(aes(x = cty, y = hwy, color = class, size = cyl, shape = drv)) + \n  geom_point(alpha = .5) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nUse categorical variables for shape, categorical or continuous variables for color, and ordinal or continuous variables for size."
  },
  {
    "objectID": "lectures/week3-visualization.html#plotting-a-regression-line",
    "href": "lectures/week3-visualization.html#plotting-a-regression-line",
    "title": "Data Visualization in R",
    "section": "Plotting a regression line",
    "text": "Plotting a regression line\nWe can also use geom_smooth to plot a regression line (or another non-linear function) over our scatterplot.\n(If there are multiple colors, etc., a different line will be plotted for each color.)\n\nmpg %&gt;%\n  ggplot(aes(x = cty, y = hwy)) + \n  geom_smooth(method = \"lm\") +\n  geom_point(alpha = .5) \n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lectures/week3-visualization.html#bar-plots",
    "href": "lectures/week3-visualization.html#bar-plots",
    "title": "Data Visualization in R",
    "section": "Bar plots",
    "text": "Bar plots\n\nA barplot visualizes the relationship between one continuous variable and (at least one) categorical variable.\n\nA barplot can be created with geom_bar.\n\n\nBy default, geom_bar will count occurrences (like a histogram for categorical variables).\nYou can also calculate values like the mean using geom_bar(stat = \"summary\").\nIf you already have values computed (e.g., a mean or count), you can use geom_col or `geom_bar(stat = “identity”)."
  },
  {
    "objectID": "lectures/week3-visualization.html#bar-plots-counts",
    "href": "lectures/week3-visualization.html#bar-plots-counts",
    "title": "Data Visualization in R",
    "section": "Bar plots: counts",
    "text": "Bar plots: counts\nBy default, geom_bar will count the occurrences of each class.\n\nmpg %&gt;%\n  ggplot(aes(x = drv)) +\n  geom_bar() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip💭 Check-in\n\n\n\nCreate a barplot showing the counts of some other categorical variable in the mpg dataframe."
  },
  {
    "objectID": "lectures/week3-visualization.html#bar-plots-summaries",
    "href": "lectures/week3-visualization.html#bar-plots-summaries",
    "title": "Data Visualization in R",
    "section": "Bar plots: summaries",
    "text": "Bar plots: summaries\nYou can also calculate summary statistics, i.e., the mean of some y variable for each level of the x variable.\nUse reorder to reorder the bars in terms of their values.\n\nmpg %&gt;%\n  ggplot(aes(x = reorder(drv, hwy), y = hwy)) +\n  geom_bar(stat = \"summary\", fun = \"mean\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week3-visualization.html#bar-plots-with-group_by",
    "href": "lectures/week3-visualization.html#bar-plots-with-group_by",
    "title": "Data Visualization in R",
    "section": "Bar plots with group_by",
    "text": "Bar plots with group_by\nAlternatively, you can calculate summary statistics using group_by %&gt;% summarise, and pipe the output into a ggplot call.\n\nmpg %&gt;%\n  group_by(drv) %&gt;%\n  summarise(mean_hwy = mean(hwy)) %&gt;%\n  ggplot(aes(x = reorder(drv, mean_hwy), y = mean_hwy)) +\n  geom_bar(stat = \"identity\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip💭 Check-in\n\n\n\nHow would you instead plot the mean cty miles per gallon?"
  },
  {
    "objectID": "lectures/week3-visualization.html#error-bars-stat_summary",
    "href": "lectures/week3-visualization.html#error-bars-stat_summary",
    "title": "Data Visualization in R",
    "section": "Error bars: stat_summary",
    "text": "Error bars: stat_summary\nOften, you want to display some measure of dispersion in addition to the mean. One approach is to use stat_summary.\n\nmpg %&gt;%\n  ggplot(aes(x = reorder(drv, hwy), y = hwy)) +\n  geom_bar(stat = \"summary\", fun = \"mean\") +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +\n  labs(x = \"Drive Type\", y = \"Mean Highway MPG\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week3-visualization.html#error-bars-with-group_by",
    "href": "lectures/week3-visualization.html#error-bars-with-group_by",
    "title": "Data Visualization in R",
    "section": "Error bars with group_by",
    "text": "Error bars with group_by\nAlternatively, with the group_by method, you can use first calculate the standard error, then pipe the result into geom_errorbar.\n\nmpg %&gt;%\n  group_by(drv) %&gt;%\n  summarise(\n    mean_hwy = mean(hwy),\n    se_hwy = sd(hwy) / sqrt(n())\n  ) %&gt;%\n  ggplot(aes(x = reorder(drv, mean_hwy), y = mean_hwy)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin = mean_hwy - se_hwy, \n                    ymax = mean_hwy + se_hwy),\n                width = 0.2) +\n  labs(x = \"Drive Type\", y = \"Mean Highway MPG\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip💭 Check-in\n\n\n\nHow would you plot a confidence interval for two standard errors?"
  },
  {
    "objectID": "lectures/week3-visualization.html#error-bars-with-group_by-1",
    "href": "lectures/week3-visualization.html#error-bars-with-group_by-1",
    "title": "Data Visualization in R",
    "section": "Error bars with group_by",
    "text": "Error bars with group_by\nAlternatively, with the group_by method, you can use first calculate the standard error, then pipe the result into geom_errorbar.\n\n\n\n\n\n\n\nTip💭 Check-in\n\n\n\nHow would you plot a confidence interval for two standard errors?"
  },
  {
    "objectID": "lectures/week3-visualization.html#bar-plots-with-fill",
    "href": "lectures/week3-visualization.html#bar-plots-with-fill",
    "title": "Data Visualization in R",
    "section": "Bar plots with fill",
    "text": "Bar plots with fill\nYou can add further information to a barplot using the fill parameter. Use position_dodge to show the bars side by side (rather than stacked).\n\nmpg %&gt;%\n  ggplot(aes(x = reorder(drv, hwy), y = hwy, fill = class)) +\n  geom_bar(stat = \"summary\", fun = \"mean\", position = position_dodge(width = 0.9)) +\n  stat_summary(fun.data = mean_se, \n               geom = \"errorbar\", \n               position = position_dodge(width = 0.9), \n               width = 0.2) +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week3-visualization.html#boxplots-and-violin-plots",
    "href": "lectures/week3-visualization.html#boxplots-and-violin-plots",
    "title": "Data Visualization in R",
    "section": "Boxplots and violin plots",
    "text": "Boxplots and violin plots\n\nBoxplots and violin plots show more detailed information about underlying distribution for a given category.\n\n\n\nA boxplot shows the median, along with the inter-quartile range.\nA violinplot shows the full distribution as a density curve, rotated and mirrored.\nAs with barplots, you can also modify the color of each box/violin."
  },
  {
    "objectID": "lectures/week3-visualization.html#boxplots-with-geom_boxplot",
    "href": "lectures/week3-visualization.html#boxplots-with-geom_boxplot",
    "title": "Data Visualization in R",
    "section": "Boxplots with geom_boxplot",
    "text": "Boxplots with geom_boxplot\n\nmpg %&gt;%\n  ggplot(aes(x = reorder(drv, hwy), y = hwy)) +\n  geom_boxplot() +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week3-visualization.html#violin-plots-with-geom_violin",
    "href": "lectures/week3-visualization.html#violin-plots-with-geom_violin",
    "title": "Data Visualization in R",
    "section": "Violin plots with geom_violin",
    "text": "Violin plots with geom_violin\n\nmpg %&gt;%\n  ggplot(aes(x = reorder(drv, hwy), y = hwy)) +\n  geom_violin() +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week3-visualization.html#adding-styling-to-a-plot",
    "href": "lectures/week3-visualization.html#adding-styling-to-a-plot",
    "title": "Data Visualization in R",
    "section": "Adding styling to a plot",
    "text": "Adding styling to a plot\nLet’s revisit a plot we worked on earlier, with better labels and styling.\n\nmpg %&gt;%\n  ggplot(aes(x = cty, y = hwy, color = drv, size = cyl, shape = drv)) + \n  geom_point(alpha = .5) +\n  labs(x = \"City mpg\",\n       y = \"Highway mpg\",\n       size = \"Number of cylinders\",\n       color = \"Drive Train\",\n       shape = \"Drive Train\") +\n  theme_minimal() +\n  scale_color_viridis_d() +\n  theme(\n    axis.title = element_text(size = 16),\n    legend.title = element_text(size = 12),\n    legend.text = element_text(size = 10)\n  )"
  },
  {
    "objectID": "lectures/week3-visualization.html#labeling-specific-points",
    "href": "lectures/week3-visualization.html#labeling-specific-points",
    "title": "Data Visualization in R",
    "section": "Labeling specific points",
    "text": "Labeling specific points\nLet’s revisit a plot we worked on earlier, with better labels and styling.\n\nmpg_labeled &lt;- mpg %&gt;%\n  filter(hwy &gt; 42) %&gt;%\n  filter(class == \"compact\")\n\nmpg %&gt;%\n  ggplot(aes(x = cty, y = hwy, color = drv, size = cyl, shape = drv)) + \n  geom_point(alpha = .5) +\n  geom_text(data = mpg_labeled, \n            aes(label = model), \n            hjust = 1.2, vjust = 0, \n            size = 4, \n            color = \"black\",\n            show.legend = FALSE) +\n  theme_minimal(base_size = 14)"
  },
  {
    "objectID": "lectures/week3-visualization.html#other-plotting-packages",
    "href": "lectures/week3-visualization.html#other-plotting-packages",
    "title": "Data Visualization in R",
    "section": "Other plotting packages",
    "text": "Other plotting packages\nggplot has a ton of useful functions and geom types, and it’s probably all you “need”——but there are other options too.\n\n\nggcorrplot: gg-style correlation matrices.\nggridges: gg-style “ridge” plots (density plots)."
  },
  {
    "objectID": "lectures/week3-visualization.html#using-ggcorrplot",
    "href": "lectures/week3-visualization.html#using-ggcorrplot",
    "title": "Data Visualization in R",
    "section": "Using ggcorrplot",
    "text": "Using ggcorrplot\n\nggcorrplot is a library (and function) that visualizes a correlation matrix.\n\n\nlibrary(ggcorrplot)\n\ncor_matrix = mpg %&gt;%\n  select(hwy, cty, displ, cyl) %&gt;%\n  cor()\nggcorrplot(cor_matrix)"
  },
  {
    "objectID": "lectures/week3-visualization.html#using-ggridges",
    "href": "lectures/week3-visualization.html#using-ggridges",
    "title": "Data Visualization in R",
    "section": "Using ggridges",
    "text": "Using ggridges\n\nggridges is a library for arranging density plots in a staggered fashion.\n\n\nlibrary(ggridges)\nmpg %&gt;%\n  ggplot(aes(x = hwy, y = reorder(class, hwy), fill = drv)) +\n  geom_density_ridges(alpha = .7, color = NA) +\n  theme_ridges()\n\nPicking joint bandwidth of 2.29"
  },
  {
    "objectID": "lectures/week3-visualization.html#summary",
    "href": "lectures/week3-visualization.html#summary",
    "title": "Data Visualization in R",
    "section": "Summary",
    "text": "Summary\n\n\nData visualization is central to CSS.\n\nCrucial for exploring data, communicating insights, and impacting decisions.\n\nggplot is a versatile and powerful library for creating clear, elegant figures.\nR also supports a number of additional libraries for visualization.\nThe best way to learn to make visualizations is to make them!"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#goals-of-the-lecture",
    "href": "lectures/week5-r-regression-slides.html#goals-of-the-lecture",
    "title": "Regression: Advanced Issues",
    "section": "Goals of the lecture",
    "text": "Goals of the lecture\n\nMultiple regression: beyond single predictors.\n\nModeling interaction effects.\n\nAdvanced topics in regression:\n\nUnderstanding standard errors and coefficient sampling distributions.\nHeteroscedasticity.\nMulticollinearity."
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#part-1-multiple-regression",
    "href": "lectures/week5-r-regression-slides.html#part-1-multiple-regression",
    "title": "Regression: Advanced Issues",
    "section": "Part 1: Multiple regression",
    "text": "Part 1: Multiple regression\nMultiple predictors, interpreting coefficients, and interaction effects."
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#what-is-multiple-regression",
    "href": "lectures/week5-r-regression-slides.html#what-is-multiple-regression",
    "title": "Regression: Advanced Issues",
    "section": "What is multiple regression",
    "text": "What is multiple regression\n\nMultiple regression means building a regression model (e.g., linear regression) with more than one predictor.\n\nThere are a few reasons to do this:\n\nMultiple predictors helps “adjust for” multiple correlated effects, i.e., \\(X_1\\) and \\(X_2\\).\n\nTells us the effect of \\(X_1\\) on \\(Y\\), holding constant the effect of \\(X_2\\) on \\(Y\\).\nImportant for addressing confounding variables.\n\nIn general, adding more predictors often results in more powerful models.\n\nBetter for predicting things in the real world!"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#loading-a-dataset",
    "href": "lectures/week5-r-regression-slides.html#loading-a-dataset",
    "title": "Regression: Advanced Issues",
    "section": "Loading a dataset",
    "text": "Loading a dataset\nFortunately, we can still use lm to build multiple regression models in R. To get started, let’s load the income dataset we used in previous lecutres.\n\n### Loading the tidyverse\nlibrary(tidyverse)\n\ndf_income &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/regression/income.csv\")\nnrow(df_income)\n\n\n\n[1] 30"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#building-a-multiple-regression-model",
    "href": "lectures/week5-r-regression-slides.html#building-a-multiple-regression-model",
    "title": "Regression: Advanced Issues",
    "section": "Building a multiple regression model",
    "text": "Building a multiple regression model\nTo add multiple predictors to our formula, simply use the + syntax.\n\nmod_multiple = lm(data = df_income,\n                  Income ~ Seniority + Education)\nsummary(mod_multiple)\n\n\n\n\nCall:\nlm(formula = Income ~ Seniority + Education, data = df_income)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.113 -5.718 -1.095  3.134 17.235 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -50.08564    5.99878  -8.349 5.85e-09 ***\nSeniority     0.17286    0.02442   7.079 1.30e-07 ***\nEducation     5.89556    0.35703  16.513 1.23e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.187 on 27 degrees of freedom\nMultiple R-squared:  0.9341,    Adjusted R-squared:  0.9292 \nF-statistic: 191.4 on 2 and 27 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n💭 Check-in\n\n\nTry building two models with each of those predictors on their own. Compare the coefficient values and \\(R^2\\). Any differences?"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#comparing-coefficients-to-single-predictors",
    "href": "lectures/week5-r-regression-slides.html#comparing-coefficients-to-single-predictors",
    "title": "Regression: Advanced Issues",
    "section": "Comparing coefficients to single predictors",
    "text": "Comparing coefficients to single predictors\nIn general, we see that the coefficients are pretty similar from the single-variable models and the multiple-variable models, though generally reduced in the multiple-variable model.\n\n# Model 1: Single predictor - Seniority only\nmod_seniority &lt;- lm(Income ~ Seniority, data = df_income)\n# Model 2: Single predictor - Education only  \nmod_education &lt;- lm(Income ~ Education, data = df_income)\n# Model 3: Multiple regression - Both predictors\nmod_multiple &lt;- lm(Income ~ Seniority + Education, data = df_income)\n\nmod_seniority$coefficients\n\n\n\n(Intercept)   Seniority \n  39.158326    0.251288 \n\nmod_education$coefficients\n\n(Intercept)   Education \n -41.916612    6.387161 \n\nmod_multiple$coefficients\n\n(Intercept)   Seniority   Education \n-50.0856388   0.1728555   5.8955560"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#comparing-fit-to-single-predictors",
    "href": "lectures/week5-r-regression-slides.html#comparing-fit-to-single-predictors",
    "title": "Regression: Advanced Issues",
    "section": "Comparing fit to single predictors",
    "text": "Comparing fit to single predictors\nWe also see that the model with multiple predictors achieves a better \\(R^2\\).\n\nsummary(mod_seniority)$r.squared\n\n\n\n[1] 0.2686226\n\nsummary(mod_education)$r.squared\n\n[1] 0.8118069\n\nsummary(mod_multiple)$r.squared\n\n[1] 0.9341035\n\n\n\n\n\n\n💭 Check-in\n\n\nAre there any potential concerns with using standard \\(R^2\\) here?"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#using-adjusted-r-squared",
    "href": "lectures/week5-r-regression-slides.html#using-adjusted-r-squared",
    "title": "Regression: Advanced Issues",
    "section": "Using adjusted R-squared",
    "text": "Using adjusted R-squared\n\nAdjusted \\(R^2\\) is a modified version of \\(R^2\\) that accounts for the fact that adding more variables will always slightly improve model fit.\n\n\\(R^2_{adj} = 1 - \\frac{RSS/(n - p - 1)}{SS_Y/(n - 1)}\\)\nWhere:\n\n\\(n\\): number of observations.\n\\(p\\): number of parameters in model.\n\n\nsummary(mod_seniority)$adj.r.squared\n\n\n\n[1] 0.242502\n\nsummary(mod_education)$adj.r.squared\n\n[1] 0.8050857\n\nsummary(mod_multiple)$adj.r.squared\n\n[1] 0.9292223\n\n\n\n\n\n\nNote\n\n\nAs \\(p\\) increases, the numerator decreases, i.e., it’s a penalty for adding more predictors."
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#aic-another-metric-of-fit",
    "href": "lectures/week5-r-regression-slides.html#aic-another-metric-of-fit",
    "title": "Regression: Advanced Issues",
    "section": "AIC: Another metric of fit",
    "text": "AIC: Another metric of fit\n\nAkaike Information Criterion (or AIC) is another commonly used metric of model fit.\n\n\\(AIC = 2k - 2\\ln(L)\\)\nWhere:\n\n\\(k\\) is the number of parameters.\n\\(L\\) is the likelihood of the data under the model (we’ll discuss this soon!).\n\n\nAIC(mod_seniority)\n\n\n\n[1] 278.5142\n\nAIC(mod_education)\n\n[1] 237.7904\n\nAIC(mod_multiple)\n\n[1] 208.3089\n\n\n\n\n\n\nNote\n\n\nUnlike \\(R^2\\), a lower AIC is better. We’ll talk more about model likelihood later in the course!"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#your-turn-using-mtcars",
    "href": "lectures/week5-r-regression-slides.html#your-turn-using-mtcars",
    "title": "Regression: Advanced Issues",
    "section": "Your turn: using mtcars",
    "text": "Your turn: using mtcars\nNow, let’s apply this using the mtcars dataset:\n\nTry to predict mpg from wt. Interpret the coefficients and model fit.\nThen try to predict mpg from am. Interpret the coefficients and model fit. Note that you might need to convert am into a factor first.\nFinally, predict mpg from both variables. Interpret the coefficients and model fit."
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#writing-out-the-linear-equation",
    "href": "lectures/week5-r-regression-slides.html#writing-out-the-linear-equation",
    "title": "Regression: Advanced Issues",
    "section": "Writing out the linear equation",
    "text": "Writing out the linear equation\nIt’s often helpful to practice writing out the linear equations for a fit lm. Let’s write out the equations for the multiple variable model above.\n\\(Y_{mpg} = 37.32 - 5.35X_{wt} - 0.02X_{am}\\)\n\n\n\n\n💭 Check-in\n\n\nWhat about interactions between our terms?"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#interaction-effects",
    "href": "lectures/week5-r-regression-slides.html#interaction-effects",
    "title": "Regression: Advanced Issues",
    "section": "Interaction effects",
    "text": "Interaction effects\n\nAn interaction effect occurs when the effect of one variable (\\(X_1\\)) depends on the value of another variable (\\(X_2\\)).\n\n\nSo far, we’ve assumed variables have roughly additive effects.\nAn interaction removes that assumption.\nIn model, allows for \\(X_1 * X_2\\), i.e., a multiplicative effect."
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#examples-of-interaction-effects",
    "href": "lectures/week5-r-regression-slides.html#examples-of-interaction-effects",
    "title": "Regression: Advanced Issues",
    "section": "Examples of interaction effects",
    "text": "Examples of interaction effects\nAny set of variables could interact (including more than two!), but it’s often easiest to interpret interactions with categorical variables.\n\nFood x Condiment: The enjoyment of a Food (ice cream vs. hot dog) depends on the Condiment (Chocolate vs. Ketchup).\nTreatment x Gender: The effect of some treatment depends on the Gender of the recipient.\nBrain Region x Task: The activation in different Brain Regions depends on the Task (e.g., motor vs. visual)."
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#building-an-interaction-effect",
    "href": "lectures/week5-r-regression-slides.html#building-an-interaction-effect",
    "title": "Regression: Advanced Issues",
    "section": "Building an interaction effect",
    "text": "Building an interaction effect\nWe can interpret the coefficients as follows:\n\nIntercept: Automatic cars (am = 0) with wt = 0 have expected mpg of 31.\nwt: As wt increases, we expect automatic cars to decrease mpg by about 3.7.\nfactor(am): Holding wt constant, manual cars have an expected mpg of about 14 more than automatic cars.\nInteraction: Among manual cars, increases in wt correspond to even sharper declines in expected mpg.\n\n\nmod_int = lm(data = mtcars, mpg ~ am * wt)\nsummary(mod_int)\n\n\n\n\nCall:\nlm(formula = mpg ~ am * wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6004 -1.5446 -0.5325  0.9012  6.0909 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  31.4161     3.0201  10.402 4.00e-11 ***\nam           14.8784     4.2640   3.489  0.00162 ** \nwt           -3.7859     0.7856  -4.819 4.55e-05 ***\nam:wt        -5.2984     1.4447  -3.667  0.00102 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.591 on 28 degrees of freedom\nMultiple R-squared:  0.833, Adjusted R-squared:  0.8151 \nF-statistic: 46.57 on 3 and 28 DF,  p-value: 5.209e-11"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#visualizing-an-interaction",
    "href": "lectures/week5-r-regression-slides.html#visualizing-an-interaction",
    "title": "Regression: Advanced Issues",
    "section": "Visualizing an interaction",
    "text": "Visualizing an interaction\nBy default, R will draw different regression lines for each level of a categorical factor we’ve added an aes for.\n\nmtcars %&gt;%\n  ggplot(aes(x = wt, y = mpg, color = factor(am))) +\n  geom_point() +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#interim-summary",
    "href": "lectures/week5-r-regression-slides.html#interim-summary",
    "title": "Regression: Advanced Issues",
    "section": "Interim summary",
    "text": "Interim summary\n\nMultiple regression models are often more powerful than single-variable models.\nIn some cases, adding an interaction helps account for relationships that depend on another variable.\nBut adding multiple variables does make interpretation slightly more challenging."
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#coefficient-standard-errors-ses",
    "href": "lectures/week5-r-regression-slides.html#coefficient-standard-errors-ses",
    "title": "Regression: Advanced Issues",
    "section": "Coefficient standard errors (SEs)",
    "text": "Coefficient standard errors (SEs)\n\nThe standard error of a coefficient measures the uncertainty in the estimated coefficient value.\n\nWhen we inspect the output of a fit lm model, we can see not only the coefficients but the standard errors (Std. Error):\n\n\n\nCall:\nlm(formula = Income ~ Education, data = df_income)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.568  -8.012   1.474   5.754  23.701 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -41.9166     9.7689  -4.291 0.000192 ***\nEducation     6.3872     0.5812  10.990 1.15e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.93 on 28 degrees of freedom\nMultiple R-squared:  0.8118,    Adjusted R-squared:  0.8051 \nF-statistic: 120.8 on 1 and 28 DF,  p-value: 1.151e-11\n\n\n\n\n\n\n💭 Check-in\n\n\nDoes a larger SE correspond to more or less uncertainty about our estimate?"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#the-sampling-distribution-of-coefficients",
    "href": "lectures/week5-r-regression-slides.html#the-sampling-distribution-of-coefficients",
    "title": "Regression: Advanced Issues",
    "section": "The sampling distribution of coefficients",
    "text": "The sampling distribution of coefficients\n\nIf we could repeatedly sample from our population and fit our model each time, we’d get a distribution of coefficient estimates.\nThis is called the sampling distribution of the coefficient.\nThe standard error is the standard deviation of this sampling distribution.\nIt tells us: “If we repeated this study many times, how much would our coefficient estimate vary?”\n\n\n\n\n\n💭 Check-in\n\n\nWhich properties affect the standard deviation of that sampling distribution?"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#calculating-the-se",
    "href": "lectures/week5-r-regression-slides.html#calculating-the-se",
    "title": "Regression: Advanced Issues",
    "section": "Calculating the SE",
    "text": "Calculating the SE\nFor a simple linear regression, the SE of the slope coefficient is:\n\\(SE(\\beta_1) = \\sqrt{\\frac{\\sum(y_i - \\hat{y}_i)^2}{(n-2)\\sum(x_i - \\bar{x})^2}}\\)\n\nNumerator: Larger residuals → more uncertainty → larger SE\nDenominator: More data points and more spread in \\(X\\) → less uncertainty → smaller SE\nFor multiple regression, the calculation is more complex (involves matrix algebra)\nFortunately, R calculates this for us!\n\n\n\n\n\nNote\n\n\nYou will not be expected to calculate this from scratch!"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#the-t-statistic",
    "href": "lectures/week5-r-regression-slides.html#the-t-statistic",
    "title": "Regression: Advanced Issues",
    "section": "The t-statistic",
    "text": "The t-statistic\n\nThe t-statistic measures how many standard errors a coefficient is away from zero.\n\n\\(t = \\frac{\\hat{\\beta} - 0}{SE(\\hat{\\beta})}\\)\n\n# Extract coefficient and SE for Education\ncoef_value &lt;- summary(mod_education)$coefficients[\"Education\", \"Estimate\"]\nse_value &lt;- summary(mod_education)$coefficients[\"Education\", \"Std. Error\"]\n\n# Calculate t-statistic manually\nt_stat &lt;- coef_value / se_value\nt_stat\n\n\n\n[1] 10.99015\n\n\n\nThis matches the t value column in our summary output!"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#interpreting-the-t-statistic",
    "href": "lectures/week5-r-regression-slides.html#interpreting-the-t-statistic",
    "title": "Regression: Advanced Issues",
    "section": "Interpreting the t-statistic",
    "text": "Interpreting the t-statistic\n\nThe t-statistic follows a t-distribution under the null hypothesis that \\(\\beta = 0\\).\nRule of thumb: \\(|t| \\geq 2\\) typically indicates statistical significance (p &lt; 0.05).\nMore precisely: We calculate the probability of observing a \\(|t|\\) this large (or larger) if the true coefficient were zero.\nThis probability is the p-value.\n\n\n\n\n\n💭 Check-in\n\n\nLooking at our mod_education output, is the Education coefficient significantly different from zero?"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#confidence-intervals",
    "href": "lectures/week5-r-regression-slides.html#confidence-intervals",
    "title": "Regression: Advanced Issues",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nWe can use the SE to construct confidence intervals around our coefficient estimates:\n\\(\\hat{\\beta} \\pm t_{critical} \\times SE(\\hat{\\beta})\\)\n\n# 95% confidence intervals for our regression model\nconfint(mod_education)\n\n\n\n                 2.5 %     97.5 %\n(Intercept) -61.927397 -21.905827\nEducation     5.196685   7.577637"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#your-turn-ses-and-t-statistics",
    "href": "lectures/week5-r-regression-slides.html#your-turn-ses-and-t-statistics",
    "title": "Regression: Advanced Issues",
    "section": "Your turn: SEs and t-statistics",
    "text": "Your turn: SEs and t-statistics\nUsing your mtcars model from earlier predicting mpg from wt and am:\n\nWhich coefficient has the largest standard error?\n\nWhich coefficient has the smallest p-value (most significant)?\n\nCalculate 99% confidence intervals using confint(model, level = 0.99)."
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#heteroscedasticity",
    "href": "lectures/week5-r-regression-slides.html#heteroscedasticity",
    "title": "Regression: Advanced Issues",
    "section": "Heteroscedasticity",
    "text": "Heteroscedasticity\n\nHeteroscedasticity occurs when the variance of residuals is not constant across all levels of the predictors.\n\n\nHomoscedasticity (the ideal): Residuals have constant variance.\nHeteroscedasticity (a problem): Residual variance changes with predictor values.\nThis violates one of the key assumptions of linear regression!\nConsequences: Standard errors become unreliable, affecting our hypothesis tests.\n\n\n\n\n\n💭 Check-in\n\n\nWhy would heteroscecasticity affect our interpretation of standard errors?"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#heteroscedasticity-and-prediction-error",
    "href": "lectures/week5-r-regression-slides.html#heteroscedasticity-and-prediction-error",
    "title": "Regression: Advanced Issues",
    "section": "Heteroscedasticity and prediction error",
    "text": "Heteroscedasticity and prediction error\n\nThe standard error of the estimate gives us an estimate of how much prediction error to expect.\n\n\nWith homoscedasticity, our expected error shouldn’t really depend on \\(X\\).\nBut with heteroscedasticity, our margin of error might actually vary as a function of \\(X\\).\nYet we only have a single estimate of our prediction error, which might overestimate or underestimate error for a given \\(X\\) value."
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#why-heteroscedasticity-biases-ses",
    "href": "lectures/week5-r-regression-slides.html#why-heteroscedasticity-biases-ses",
    "title": "Regression: Advanced Issues",
    "section": "Why heteroscedasticity biases SEs",
    "text": "Why heteroscedasticity biases SEs\nThe key insight:\n\nOLS gives equal weight to all observations when estimating SEs.\nBut with heteroscedasticity, some observations have more noise than others.\nHigh-variance observations should be trusted less, but OLS doesn’t know this.\nResult: SEs don’t accurately reflect true uncertainty."
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#detecting-heteroscedasticity",
    "href": "lectures/week5-r-regression-slides.html#detecting-heteroscedasticity",
    "title": "Regression: Advanced Issues",
    "section": "Detecting heteroscedasticity",
    "text": "Detecting heteroscedasticity\nWe can diagnose heteroscedasticity by plotting residuals vs. fitted values:\n\n# Create diagnostic plot\ntibble(\n  fitted = fitted(mod_multiple),\n  residuals = residuals(mod_multiple)\n) %&gt;%\n  ggplot(aes(x = fitted, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residuals vs. Fitted Values\",\n       x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nLook for: funnel shapes, increasing/decreasing spread, or other patterns."
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#what-to-do-about-heteroscedasticity",
    "href": "lectures/week5-r-regression-slides.html#what-to-do-about-heteroscedasticity",
    "title": "Regression: Advanced Issues",
    "section": "What to do about heteroscedasticity?",
    "text": "What to do about heteroscedasticity?\nSeveral options to address heteroscedasticity:\n\nTransform the outcome variable (e.g., log transformation)\nUse robust standard errors (also called heteroscedasticity-consistent SEs)\nUse weighted least squares regression\n\n\nWe won’t be covering (2) or (3), but there are R packages for implementing both."
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#multicollinearity",
    "href": "lectures/week5-r-regression-slides.html#multicollinearity",
    "title": "Regression: Advanced Issues",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMulticollinearity occurs when predictor variables are highly correlated with each other.\n\n\nMakes it difficult to isolate the individual effect of each predictor.\nLeads to unstable coefficient estimates with large standard errors.\nCoefficients may have unexpected signs or magnitudes.\nDoes NOT affect predictions, but makes interpretation problematic."
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#multicollinearity-the-core-problem",
    "href": "lectures/week5-r-regression-slides.html#multicollinearity-the-core-problem",
    "title": "Regression: Advanced Issues",
    "section": "Multicollinearity: The core problem",
    "text": "Multicollinearity: The core problem\n\nWhen predictors are highly correlated, the model struggles to determine which predictor is “responsible” for variation in Y.\n\nConsider predicting income from: - Years of education - Number of graduate degrees\n\nThese are highly correlated - more years usually means more degrees.\nThe model faces a credit assignment challenge: Is income higher because of years OR degrees?\nMany combinations of coefficients can fit the data equally well.\nResult: Coefficients become unstable and uncertain."
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#example-detecting-multicollinearity",
    "href": "lectures/week5-r-regression-slides.html#example-detecting-multicollinearity",
    "title": "Regression: Advanced Issues",
    "section": "Example: Detecting multicollinearity",
    "text": "Example: Detecting multicollinearity\nLet’s check the correlation between our predictors:\n\n# Correlation between predictors\ncor(df_income$Seniority, df_income$Education)\n\n\n\n[1] 0.1945146\n\n# Visualize the relationship\ndf_income %&gt;%\n  ggplot(aes(x = Seniority, y = Education)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Relationship between Seniority and Education\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#variance-inflation-factor-vif",
    "href": "lectures/week5-r-regression-slides.html#variance-inflation-factor-vif",
    "title": "Regression: Advanced Issues",
    "section": "Variance Inflation Factor (VIF)",
    "text": "Variance Inflation Factor (VIF)\n\nThe VIF quantifies how much the variance of a coefficient is inflated due to multicollinearity.\n\n\nlibrary(car)\nvif(mod_multiple)\n\n\n\nSeniority Education \n 1.039324  1.039324 \n\n\n\nVIF = 1: No correlation with other predictors\nVIF &lt; 5: Generally acceptable\nVIF &gt; 10: Serious multicollinearity problem\nVIF &gt; 5: Some concern, investigate further"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#understanding-vif-the-concept",
    "href": "lectures/week5-r-regression-slides.html#understanding-vif-the-concept",
    "title": "Regression: Advanced Issues",
    "section": "Understanding VIF: The concept",
    "text": "Understanding VIF: The concept\n\nVIF (Variance Inflation Factor) quantifies how much multicollinearity inflates the variance of a coefficient.\n\nFrom the formula on the previous slide:\n\\(VIF_j = \\frac{1}{1-R^2_j}\\)\nWhere \\(R^2_j\\) is the R-squared from regressing predictor \\(X_j\\) on all other predictors.\n\nInterpretation: “How much is the variance of \\(\\hat{\\beta}_j\\) inflated compared to if \\(X_j\\) were uncorrelated with other predictors?”\nIf \\(X_j\\) is uncorrelated with others: \\(R^2_j = 0\\) → \\(VIF = 1\\) (no inflation)\nIf \\(X_j\\) is highly correlated with others: \\(R^2_j\\) close to 1 → \\(VIF\\) is large"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#what-to-do-about-multicollinearity",
    "href": "lectures/week5-r-regression-slides.html#what-to-do-about-multicollinearity",
    "title": "Regression: Advanced Issues",
    "section": "What to do about multicollinearity?",
    "text": "What to do about multicollinearity?\nSeveral strategies to address multicollinearity:\n\nRemove one of the correlated predictors\nCombine correlated predictors (e.g., create an index)\nUse principal components analysis (PCA) to create uncorrelated predictors\nUse regularization methods (Ridge, Lasso regression)\nCollect more data with greater variation in predictors\n\n\n\n\n\nNote\n\n\nThe best solution depends on your research question and theoretical considerations!"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#your-turn-diagnostics",
    "href": "lectures/week5-r-regression-slides.html#your-turn-diagnostics",
    "title": "Regression: Advanced Issues",
    "section": "Your turn: Diagnostics",
    "text": "Your turn: Diagnostics\nUsing the mtcars dataset:\n\nBuild a model predicting mpg from wt, hp, and disp\nCheck for multicollinearity using cor() and vif()\nCreate a residual plot to check for heteroscedasticity\nWhat potential issues do you notice?"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#final-summary",
    "href": "lectures/week5-r-regression-slides.html#final-summary",
    "title": "Regression: Advanced Issues",
    "section": "Final summary",
    "text": "Final summary\n\nStandard errors quantify uncertainty in coefficient estimates\nt-statistics test whether coefficients differ significantly from zero\nHeteroscedasticity (non-constant variance) violates regression assumptions\nMulticollinearity (correlated predictors) inflates standard errors and makes interpretation difficult\nBoth issues can be diagnosed and addressed with appropriate techniques\nAlways check your model assumptions!"
  },
  {
    "objectID": "lectures/week5-r-regression-slides.html#key-takeaways",
    "href": "lectures/week5-r-regression-slides.html#key-takeaways",
    "title": "Regression: Advanced Issues",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nMultiple regression allows us to model complex relationships\nAdding interactions captures when one variable’s effect depends on another\nAdjusted \\(R^2\\) and AIC help compare models with different numbers of predictors\nUnderstanding SEs helps us assess the reliability of our estimates\nChecking assumptions (homoscedasticity, no multicollinearity) is crucial\nWhen assumptions are violated, specialized techniques can help"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html",
    "href": "lectures/week5-regression_advanced.html",
    "title": "Regression: Advanced Issues",
    "section": "",
    "text": "Multiple regression: beyond single predictors.\n\nModeling interaction effects.\n\nAdvanced topics in regression:\n\nUnderstanding standard errors and coefficient sampling distributions.\nHeteroscedasticity.\nMulticollinearity."
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#goals-of-the-lecture",
    "href": "lectures/week5-regression_advanced.html#goals-of-the-lecture",
    "title": "Regression: Advanced Issues",
    "section": "",
    "text": "Multiple regression: beyond single predictors.\n\nModeling interaction effects.\n\nAdvanced topics in regression:\n\nUnderstanding standard errors and coefficient sampling distributions.\nHeteroscedasticity.\nMulticollinearity."
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#part-1-multiple-regression",
    "href": "lectures/week5-regression_advanced.html#part-1-multiple-regression",
    "title": "Regression: Advanced Issues",
    "section": "Part 1: Multiple regression",
    "text": "Part 1: Multiple regression\nMultiple predictors, interpreting coefficients, and interaction effects."
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#what-is-multiple-regression",
    "href": "lectures/week5-regression_advanced.html#what-is-multiple-regression",
    "title": "Regression: Advanced Issues",
    "section": "What is multiple regression",
    "text": "What is multiple regression\n\nMultiple regression means building a regression model (e.g., linear regression) with more than one predictor.\n\nThere are a few reasons to do this:\n\n\nMultiple predictors helps “adjust for” multiple correlated effects, i.e., \\(X_1\\) and \\(X_2\\).\n\nTells us the effect of \\(X_1\\) on \\(Y\\), holding constant the effect of \\(X_2\\) on \\(Y\\).\nImportant for addressing confounding variables.\n\nIn general, adding more predictors often results in more powerful models.\n\nBetter for predicting things in the real world!"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#loading-a-dataset",
    "href": "lectures/week5-regression_advanced.html#loading-a-dataset",
    "title": "Regression: Advanced Issues",
    "section": "Loading a dataset",
    "text": "Loading a dataset\nFortunately, we can still use lm to build multiple regression models in R. To get started, let’s load the income dataset we used in previous lecutres.\n\n### Loading the tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf_income &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/regression/income.csv\")\n\nRows: 30 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): Education, Seniority, Income\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnrow(df_income)\n\n[1] 30"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#building-a-multiple-regression-model",
    "href": "lectures/week5-regression_advanced.html#building-a-multiple-regression-model",
    "title": "Regression: Advanced Issues",
    "section": "Building a multiple regression model",
    "text": "Building a multiple regression model\nTo add multiple predictors to our formula, simply use the + syntax.\n\nmod_multiple = lm(data = df_income,\n                  Income ~ Seniority + Education)\nsummary(mod_multiple)\n\n\nCall:\nlm(formula = Income ~ Seniority + Education, data = df_income)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.113 -5.718 -1.095  3.134 17.235 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -50.08564    5.99878  -8.349 5.85e-09 ***\nSeniority     0.17286    0.02442   7.079 1.30e-07 ***\nEducation     5.89556    0.35703  16.513 1.23e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.187 on 27 degrees of freedom\nMultiple R-squared:  0.9341,    Adjusted R-squared:  0.9292 \nF-statistic: 191.4 on 2 and 27 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nTry building two models with each of those predictors on their own. Compare the coefficient values and \\(R^2\\). Any differences?"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#comparing-coefficients-to-single-predictors",
    "href": "lectures/week5-regression_advanced.html#comparing-coefficients-to-single-predictors",
    "title": "Regression: Advanced Issues",
    "section": "Comparing coefficients to single predictors",
    "text": "Comparing coefficients to single predictors\nIn general, we see that the coefficients are pretty similar from the single-variable models and the multiple-variable models, though generally reduced in the multiple-variable model.\n\n# Model 1: Single predictor - Seniority only\nmod_seniority &lt;- lm(Income ~ Seniority, data = df_income)\n# Model 2: Single predictor - Education only  \nmod_education &lt;- lm(Income ~ Education, data = df_income)\n# Model 3: Multiple regression - Both predictors\nmod_multiple &lt;- lm(Income ~ Seniority + Education, data = df_income)\n\nmod_seniority$coefficients\n\n(Intercept)   Seniority \n  39.158326    0.251288 \n\nmod_education$coefficients\n\n(Intercept)   Education \n -41.916612    6.387161 \n\nmod_multiple$coefficients\n\n(Intercept)   Seniority   Education \n-50.0856388   0.1728555   5.8955560"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#comparing-fit-to-single-predictors",
    "href": "lectures/week5-regression_advanced.html#comparing-fit-to-single-predictors",
    "title": "Regression: Advanced Issues",
    "section": "Comparing fit to single predictors",
    "text": "Comparing fit to single predictors\nWe also see that the model with multiple predictors achieves a better \\(R^2\\).\n\nsummary(mod_seniority)$r.squared\n\n[1] 0.2686226\n\nsummary(mod_education)$r.squared\n\n[1] 0.8118069\n\nsummary(mod_multiple)$r.squared\n\n[1] 0.9341035\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nAre there any potential concerns with using standard \\(R^2\\) here?"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#using-adjusted-r-squared",
    "href": "lectures/week5-regression_advanced.html#using-adjusted-r-squared",
    "title": "Regression: Advanced Issues",
    "section": "Using adjusted R-squared",
    "text": "Using adjusted R-squared\n\nAdjusted \\(R^2\\) is a modified version of \\(R^2\\) that accounts for the fact that adding more variables will always slightly improve model fit.\n\n\\(R^2_{adj} = 1 - \\frac{RSS/(n - p - 1)}{SS_Y/(n - 1)}\\)\nWhere:\n\n\\(n\\): number of observations.\n\\(p\\): number of parameters in model.\n\n\nsummary(mod_seniority)$adj.r.squared\n\n[1] 0.242502\n\nsummary(mod_education)$adj.r.squared\n\n[1] 0.8050857\n\nsummary(mod_multiple)$adj.r.squared\n\n[1] 0.9292223\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs \\(p\\) increases, the numerator decreases, i.e., it’s a penalty for adding more predictors."
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#aic-another-metric-of-fit",
    "href": "lectures/week5-regression_advanced.html#aic-another-metric-of-fit",
    "title": "Regression: Advanced Issues",
    "section": "AIC: Another metric of fit",
    "text": "AIC: Another metric of fit\n\nAkaike Information Criterion (or AIC) is another commonly used metric of model fit.\n\n\\(AIC = 2k - 2\\ln(L)\\)\nWhere:\n\n\\(k\\) is the number of parameters.\n\\(L\\) is the likelihood of the data under the model (we’ll discuss this soon!).\n\n\nAIC(mod_seniority)\n\n[1] 278.5142\n\nAIC(mod_education)\n\n[1] 237.7904\n\nAIC(mod_multiple)\n\n[1] 208.3089\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUnlike \\(R^2\\), a lower AIC is better. We’ll talk more about model likelihood later in the course!"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#your-turn-using-mtcars",
    "href": "lectures/week5-regression_advanced.html#your-turn-using-mtcars",
    "title": "Regression: Advanced Issues",
    "section": "Your turn: using mtcars",
    "text": "Your turn: using mtcars\nNow, let’s apply this using the mtcars dataset:\n\nTry to predict mpg from wt. Interpret the coefficients and model fit.\nThen try to predict mpg from am. Interpret the coefficients and model fit. Note that you might need to convert am into a factor first.\nFinally, predict mpg from both variables. Interpret the coefficients and model fit."
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#writing-out-the-linear-equation",
    "href": "lectures/week5-regression_advanced.html#writing-out-the-linear-equation",
    "title": "Regression: Advanced Issues",
    "section": "Writing out the linear equation",
    "text": "Writing out the linear equation\nIt’s often helpful to practice writing out the linear equations for a fit lm. Let’s write out the equations for the multiple variable model above.\n\\(Y_{mpg} = 37.32 - 5.35X_{wt} - 0.02X_{am}\\)\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWhat about interactions between our terms?"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#interaction-effects",
    "href": "lectures/week5-regression_advanced.html#interaction-effects",
    "title": "Regression: Advanced Issues",
    "section": "Interaction effects",
    "text": "Interaction effects\n\nAn interaction effect occurs when the effect of one variable (\\(X_1\\)) depends on the value of another variable (\\(X_2\\)).\n\n\n\nSo far, we’ve assumed variables have roughly additive effects.\nAn interaction removes that assumption.\nIn model, allows for \\(X_1 * X_2\\), i.e., a multiplicative effect."
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#examples-of-interaction-effects",
    "href": "lectures/week5-regression_advanced.html#examples-of-interaction-effects",
    "title": "Regression: Advanced Issues",
    "section": "Examples of interaction effects",
    "text": "Examples of interaction effects\nAny set of variables could interact (including more than two!), but it’s often easiest to interpret interactions with categorical variables.\n\n\nFood x Condiment: The enjoyment of a Food (ice cream vs. hot dog) depends on the Condiment (Chocolate vs. Ketchup).\nTreatment x Gender: The effect of some treatment depends on the Gender of the recipient.\nBrain Region x Task: The activation in different Brain Regions depends on the Task (e.g., motor vs. visual)."
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#building-an-interaction-effect",
    "href": "lectures/week5-regression_advanced.html#building-an-interaction-effect",
    "title": "Regression: Advanced Issues",
    "section": "Building an interaction effect",
    "text": "Building an interaction effect\nWe can interpret the coefficients as follows:\n\nIntercept: Automatic cars (am = 0) with wt = 0 have expected mpg of 31.\nwt: As wt increases, we expect automatic cars to decrease mpg by about 3.7.\nfactor(am): Holding wt constant, manual cars have an expected mpg of about 14 more than automatic cars.\nInteraction: Among manual cars, increases in wt correspond to even sharper declines in expected mpg.\n\n\nmod_int = lm(data = mtcars, mpg ~ am * wt)\nsummary(mod_int)\n\n\nCall:\nlm(formula = mpg ~ am * wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6004 -1.5446 -0.5325  0.9012  6.0909 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  31.4161     3.0201  10.402 4.00e-11 ***\nam           14.8784     4.2640   3.489  0.00162 ** \nwt           -3.7859     0.7856  -4.819 4.55e-05 ***\nam:wt        -5.2984     1.4447  -3.667  0.00102 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.591 on 28 degrees of freedom\nMultiple R-squared:  0.833, Adjusted R-squared:  0.8151 \nF-statistic: 46.57 on 3 and 28 DF,  p-value: 5.209e-11"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#visualizing-an-interaction",
    "href": "lectures/week5-regression_advanced.html#visualizing-an-interaction",
    "title": "Regression: Advanced Issues",
    "section": "Visualizing an interaction",
    "text": "Visualizing an interaction\nBy default, R will draw different regression lines for each level of a categorical factor we’ve added an aes for.\n\nmtcars %&gt;%\n  ggplot(aes(x = wt, y = mpg, color = factor(am))) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#interim-summary",
    "href": "lectures/week5-regression_advanced.html#interim-summary",
    "title": "Regression: Advanced Issues",
    "section": "Interim summary",
    "text": "Interim summary\n\n\nMultiple regression models are often more powerful than single-variable models.\nIn some cases, adding an interaction helps account for relationships that depend on another variable.\nBut adding multiple variables does make interpretation slightly more challenging."
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#coefficient-standard-errors-ses",
    "href": "lectures/week5-regression_advanced.html#coefficient-standard-errors-ses",
    "title": "Regression: Advanced Issues",
    "section": "Coefficient standard errors (SEs)",
    "text": "Coefficient standard errors (SEs)\n\nThe standard error of a coefficient measures the uncertainty in the estimated coefficient value.\n\nWhen we inspect the output of a fit lm model, we can see not only the coefficients but the standard errors (Std. Error):\n\nsummary(mod_education)\n\n\nCall:\nlm(formula = Income ~ Education, data = df_income)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.568  -8.012   1.474   5.754  23.701 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -41.9166     9.7689  -4.291 0.000192 ***\nEducation     6.3872     0.5812  10.990 1.15e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.93 on 28 degrees of freedom\nMultiple R-squared:  0.8118,    Adjusted R-squared:  0.8051 \nF-statistic: 120.8 on 1 and 28 DF,  p-value: 1.151e-11\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nDoes a larger SE correspond to more or less uncertainty about our estimate?"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#the-sampling-distribution-of-coefficients",
    "href": "lectures/week5-regression_advanced.html#the-sampling-distribution-of-coefficients",
    "title": "Regression: Advanced Issues",
    "section": "The sampling distribution of coefficients",
    "text": "The sampling distribution of coefficients\n\n\nIf we could repeatedly sample from our population and fit our model each time, we’d get a distribution of coefficient estimates.\nThis is called the sampling distribution of the coefficient.\nThe standard error is the standard deviation of this sampling distribution.\nIt tells us: “If we repeated this study many times, how much would our coefficient estimate vary?”\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWhich properties affect the standard deviation of that sampling distribution?"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#calculating-the-se",
    "href": "lectures/week5-regression_advanced.html#calculating-the-se",
    "title": "Regression: Advanced Issues",
    "section": "Calculating the SE",
    "text": "Calculating the SE\nFor a simple linear regression, the SE of the slope coefficient is:\n\\(SE(\\beta_1) = \\sqrt{\\frac{\\sum(y_i - \\hat{y}_i)^2}{(n-2)\\sum(x_i - \\bar{x})^2}}\\)\n\n\nNumerator: Larger residuals → more uncertainty → larger SE\nDenominator: More data points and more spread in \\(X\\) → less uncertainty → smaller SE\nFor multiple regression, the calculation is more complex (involves matrix algebra)\nFortunately, R calculates this for us!\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou will not be expected to calculate this from scratch!"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#the-t-statistic",
    "href": "lectures/week5-regression_advanced.html#the-t-statistic",
    "title": "Regression: Advanced Issues",
    "section": "The t-statistic",
    "text": "The t-statistic\n\nThe t-statistic measures how many standard errors a coefficient is away from zero.\n\n\\(t = \\frac{\\hat{\\beta} - 0}{SE(\\hat{\\beta})}\\)\n\n# Extract coefficient and SE for Education\ncoef_value &lt;- summary(mod_education)$coefficients[\"Education\", \"Estimate\"]\nse_value &lt;- summary(mod_education)$coefficients[\"Education\", \"Std. Error\"]\n\n# Calculate t-statistic manually\nt_stat &lt;- coef_value / se_value\nt_stat\n\n[1] 10.99015\n\n\n\nThis matches the t value column in our summary output!"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#interpreting-the-t-statistic",
    "href": "lectures/week5-regression_advanced.html#interpreting-the-t-statistic",
    "title": "Regression: Advanced Issues",
    "section": "Interpreting the t-statistic",
    "text": "Interpreting the t-statistic\n\n\nThe t-statistic follows a t-distribution under the null hypothesis that \\(\\beta = 0\\).\nRule of thumb: \\(|t| \\geq 2\\) typically indicates statistical significance (p &lt; 0.05).\nMore precisely: We calculate the probability of observing a \\(|t|\\) this large (or larger) if the true coefficient were zero.\nThis probability is the p-value.\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nLooking at our mod_education output, is the Education coefficient significantly different from zero?"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#confidence-intervals",
    "href": "lectures/week5-regression_advanced.html#confidence-intervals",
    "title": "Regression: Advanced Issues",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nWe can use the SE to construct confidence intervals around our coefficient estimates:\n\\(\\hat{\\beta} \\pm t_{critical} \\times SE(\\hat{\\beta})\\)\n\n# 95% confidence intervals for our regression model\nconfint(mod_education)\n\n                 2.5 %     97.5 %\n(Intercept) -61.927397 -21.905827\nEducation     5.196685   7.577637"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#your-turn-ses-and-t-statistics",
    "href": "lectures/week5-regression_advanced.html#your-turn-ses-and-t-statistics",
    "title": "Regression: Advanced Issues",
    "section": "Your turn: SEs and t-statistics",
    "text": "Your turn: SEs and t-statistics\nUsing your mtcars model from earlier predicting mpg from wt and am:\n\nWhich coefficient has the largest standard error?\n\nWhich coefficient has the smallest p-value (most significant)?\n\nCalculate 99% confidence intervals using confint(model, level = 0.99)."
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#heteroscedasticity",
    "href": "lectures/week5-regression_advanced.html#heteroscedasticity",
    "title": "Regression: Advanced Issues",
    "section": "Heteroscedasticity",
    "text": "Heteroscedasticity\n\nHeteroscedasticity occurs when the variance of residuals is not constant across all levels of the predictors.\n\n\n\nHomoscedasticity (the ideal): Residuals have constant variance.\nHeteroscedasticity (a problem): Residual variance changes with predictor values.\nThis violates one of the key assumptions of linear regression!\nConsequences: Standard errors become unreliable, affecting our hypothesis tests.\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWhy would heteroscecasticity affect our interpretation of standard errors?"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#heteroscedasticity-and-prediction-error",
    "href": "lectures/week5-regression_advanced.html#heteroscedasticity-and-prediction-error",
    "title": "Regression: Advanced Issues",
    "section": "Heteroscedasticity and prediction error",
    "text": "Heteroscedasticity and prediction error\n\nThe standard error of the estimate gives us an estimate of how much prediction error to expect.\n\n\n\nWith homoscedasticity, our expected error shouldn’t really depend on \\(X\\).\nBut with heteroscedasticity, our margin of error might actually vary as a function of \\(X\\).\nYet we only have a single estimate of our prediction error, which might overestimate or underestimate error for a given \\(X\\) value."
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#why-heteroscedasticity-biases-ses",
    "href": "lectures/week5-regression_advanced.html#why-heteroscedasticity-biases-ses",
    "title": "Regression: Advanced Issues",
    "section": "Why heteroscedasticity biases SEs",
    "text": "Why heteroscedasticity biases SEs\nThe key insight:\n\n\nOLS gives equal weight to all observations when estimating SEs.\nBut with heteroscedasticity, some observations have more noise than others.\nHigh-variance observations should be trusted less, but OLS doesn’t know this.\nResult: SEs don’t accurately reflect true uncertainty."
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#detecting-heteroscedasticity",
    "href": "lectures/week5-regression_advanced.html#detecting-heteroscedasticity",
    "title": "Regression: Advanced Issues",
    "section": "Detecting heteroscedasticity",
    "text": "Detecting heteroscedasticity\nWe can diagnose heteroscedasticity by plotting residuals vs. fitted values:\n\n# Create diagnostic plot\ntibble(\n  fitted = fitted(mod_multiple),\n  residuals = residuals(mod_multiple)\n) %&gt;%\n  ggplot(aes(x = fitted, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residuals vs. Fitted Values\",\n       x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nLook for: funnel shapes, increasing/decreasing spread, or other patterns."
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#what-to-do-about-heteroscedasticity",
    "href": "lectures/week5-regression_advanced.html#what-to-do-about-heteroscedasticity",
    "title": "Regression: Advanced Issues",
    "section": "What to do about heteroscedasticity?",
    "text": "What to do about heteroscedasticity?\nSeveral options to address heteroscedasticity:\n\n\nTransform the outcome variable (e.g., log transformation)\nUse robust standard errors (also called heteroscedasticity-consistent SEs)\nUse weighted least squares regression\n\n\n\nWe won’t be covering (2) or (3), but there are R packages for implementing both."
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#multicollinearity",
    "href": "lectures/week5-regression_advanced.html#multicollinearity",
    "title": "Regression: Advanced Issues",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMulticollinearity occurs when predictor variables are highly correlated with each other.\n\n\n\nMakes it difficult to isolate the individual effect of each predictor.\nLeads to unstable coefficient estimates with large standard errors.\nCoefficients may have unexpected signs or magnitudes.\nDoes NOT affect predictions, but makes interpretation problematic."
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#multicollinearity-the-core-problem",
    "href": "lectures/week5-regression_advanced.html#multicollinearity-the-core-problem",
    "title": "Regression: Advanced Issues",
    "section": "Multicollinearity: The core problem",
    "text": "Multicollinearity: The core problem\n\nWhen predictors are highly correlated, the model struggles to determine which predictor is “responsible” for variation in Y.\n\nConsider predicting income from: - Years of education - Number of graduate degrees\n\n\nThese are highly correlated - more years usually means more degrees.\nThe model faces a credit assignment challenge: Is income higher because of years OR degrees?\nMany combinations of coefficients can fit the data equally well.\nResult: Coefficients become unstable and uncertain."
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#example-detecting-multicollinearity",
    "href": "lectures/week5-regression_advanced.html#example-detecting-multicollinearity",
    "title": "Regression: Advanced Issues",
    "section": "Example: Detecting multicollinearity",
    "text": "Example: Detecting multicollinearity\nLet’s check the correlation between our predictors:\n\n# Correlation between predictors\ncor(df_income$Seniority, df_income$Education)\n\n[1] 0.1945146\n\n# Visualize the relationship\ndf_income %&gt;%\n  ggplot(aes(x = Seniority, y = Education)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Relationship between Seniority and Education\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#variance-inflation-factor-vif",
    "href": "lectures/week5-regression_advanced.html#variance-inflation-factor-vif",
    "title": "Regression: Advanced Issues",
    "section": "Variance Inflation Factor (VIF)",
    "text": "Variance Inflation Factor (VIF)\n\nThe VIF quantifies how much the variance of a coefficient is inflated due to multicollinearity.\n\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nvif(mod_multiple)\n\nSeniority Education \n 1.039324  1.039324 \n\n\n\n\nVIF = 1: No correlation with other predictors\nVIF &lt; 5: Generally acceptable\nVIF &gt; 10: Serious multicollinearity problem\nVIF &gt; 5: Some concern, investigate further"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#understanding-vif-the-concept",
    "href": "lectures/week5-regression_advanced.html#understanding-vif-the-concept",
    "title": "Regression: Advanced Issues",
    "section": "Understanding VIF: The concept",
    "text": "Understanding VIF: The concept\n\nVIF (Variance Inflation Factor) quantifies how much multicollinearity inflates the variance of a coefficient.\n\nFrom the formula on the previous slide:\n\\(VIF_j = \\frac{1}{1-R^2_j}\\)\nWhere \\(R^2_j\\) is the R-squared from regressing predictor \\(X_j\\) on all other predictors.\n\n\nInterpretation: “How much is the variance of \\(\\hat{\\beta}_j\\) inflated compared to if \\(X_j\\) were uncorrelated with other predictors?”\nIf \\(X_j\\) is uncorrelated with others: \\(R^2_j = 0\\) → \\(VIF = 1\\) (no inflation)\nIf \\(X_j\\) is highly correlated with others: \\(R^2_j\\) close to 1 → \\(VIF\\) is large"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#what-to-do-about-multicollinearity",
    "href": "lectures/week5-regression_advanced.html#what-to-do-about-multicollinearity",
    "title": "Regression: Advanced Issues",
    "section": "What to do about multicollinearity?",
    "text": "What to do about multicollinearity?\nSeveral strategies to address multicollinearity:\n\n\nRemove one of the correlated predictors\nCombine correlated predictors (e.g., create an index)\nUse principal components analysis (PCA) to create uncorrelated predictors\nUse regularization methods (Ridge, Lasso regression)\nCollect more data with greater variation in predictors\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe best solution depends on your research question and theoretical considerations!"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#your-turn-diagnostics",
    "href": "lectures/week5-regression_advanced.html#your-turn-diagnostics",
    "title": "Regression: Advanced Issues",
    "section": "Your turn: Diagnostics",
    "text": "Your turn: Diagnostics\nUsing the mtcars dataset:\n\nBuild a model predicting mpg from wt, hp, and disp\nCheck for multicollinearity using cor() and vif()\nCreate a residual plot to check for heteroscedasticity\nWhat potential issues do you notice?"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#final-summary",
    "href": "lectures/week5-regression_advanced.html#final-summary",
    "title": "Regression: Advanced Issues",
    "section": "Final summary",
    "text": "Final summary\n\n\nStandard errors quantify uncertainty in coefficient estimates\nt-statistics test whether coefficients differ significantly from zero\nHeteroscedasticity (non-constant variance) violates regression assumptions\nMulticollinearity (correlated predictors) inflates standard errors and makes interpretation difficult\nBoth issues can be diagnosed and addressed with appropriate techniques\nAlways check your model assumptions!"
  },
  {
    "objectID": "lectures/week5-regression_advanced.html#key-takeaways",
    "href": "lectures/week5-regression_advanced.html#key-takeaways",
    "title": "Regression: Advanced Issues",
    "section": "Key takeaways",
    "text": "Key takeaways\n\n\nMultiple regression allows us to model complex relationships\nAdding interactions captures when one variable’s effect depends on another\nAdjusted \\(R^2\\) and AIC help compare models with different numbers of predictors\nUnderstanding SEs helps us assess the reliability of our estimates\nChecking assumptions (homoscedasticity, no multicollinearity) is crucial\nWhen assumptions are violated, specialized techniques can help"
  },
  {
    "objectID": "lectures/week0-intro-slides.html#goals-of-the-lecture",
    "href": "lectures/week0-intro-slides.html#goals-of-the-lecture",
    "title": "Introduction to CSS 211",
    "section": "Goals of the lecture",
    "text": "Goals of the lecture\n\nIntroductions\n\nWhat is CSS?\nWhat is this course for?\nLogistics"
  },
  {
    "objectID": "lectures/week0-intro-slides.html#who-am-i",
    "href": "lectures/week0-intro-slides.html#who-am-i",
    "title": "Introduction to CSS 211",
    "section": "Who am I?",
    "text": "Who am I?\n\nAssistant Teaching Professor, Cognitive Science.\nResearch interests: large language models (LLMs), language comprehension, Theory of Mind."
  },
  {
    "objectID": "lectures/week0-intro-slides.html#who-are-you",
    "href": "lectures/week0-intro-slides.html#who-are-you",
    "title": "Introduction to CSS 211",
    "section": "Who are you?",
    "text": "Who are you?\nTurn to someone near you and share:\n\nYour name\nYour research interests\nWhat drew you to CSS"
  },
  {
    "objectID": "lectures/week0-intro-slides.html#what-is-css",
    "href": "lectures/week0-intro-slides.html#what-is-css",
    "title": "Introduction to CSS 211",
    "section": "What is CSS?",
    "text": "What is CSS?\n\nIn a nutshell, Computational Social Science focuses on computational approaches to social science.\n\nAt UCSD, Social Sciences encompasses many disciplines:\n\nEconomics.\nPolitical Science.\nCognitive Science.\nSociology."
  },
  {
    "objectID": "lectures/week0-intro-slides.html#what-is-social-science",
    "href": "lectures/week0-intro-slides.html#what-is-social-science",
    "title": "Introduction to CSS 211",
    "section": "What is social science?",
    "text": "What is social science?\nSocial science refers to a domain of study: social phenomena.\n\nEncompasses many scales: individual human behavior, political trends, etc.\nInvolves multiple methods: qualitative interviews, statistical analysis, simulations."
  },
  {
    "objectID": "lectures/week0-intro-slides.html#what-are-computational-methods-for",
    "href": "lectures/week0-intro-slides.html#what-are-computational-methods-for",
    "title": "Introduction to CSS 211",
    "section": "What are computational methods for?",
    "text": "What are computational methods for?\nAnother way of asking this is: how do computational methods help us learn what we want to know?\n\nMany research questions benefit from large-scale quantitative analysis.\n\nMeasuring trends over time.\nComparing effects of policy interventions.\nModeling the relationships between variables.\n\nComputational methods help us do this efficiently and reliably."
  },
  {
    "objectID": "lectures/week0-intro-slides.html#what-is-this-course-for",
    "href": "lectures/week0-intro-slides.html#what-is-this-course-for",
    "title": "Introduction to CSS 211",
    "section": "What is this course for?",
    "text": "What is this course for?\nThe goal of this course is to introduce students to foundational concepts and methods in statistical modeling. We will emphasize both hands-on application and conceptual understanding.\nKey concepts include:\n\nData wrangling, summarization, and visualization.\n\nModeling data and designing analyses.\nInterrogating the assumptions of an analysis or dataset.\n\n\nHands-on work will be done using the R programming language."
  },
  {
    "objectID": "lectures/week0-intro-slides.html#learning-outcomes",
    "href": "lectures/week0-intro-slides.html#learning-outcomes",
    "title": "Introduction to CSS 211",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nMy goal is that by the end of this course, students will be able to:\n\nDefine and explain key concepts in statistical inference and regression analysis.\nIdentify appropriate visualizations and statistical methods for different kinds of research questions and datasets.\nImplement data wrangling, visualization, and analysis workflows in R.\nInterpret and evaluate results (visualizations, fit models, etc.) in the context of a research question.\nDesign and implement a complete statistical analysis project from research question to interpretation."
  },
  {
    "objectID": "lectures/week0-intro-slides.html#why-r",
    "href": "lectures/week0-intro-slides.html#why-r",
    "title": "Introduction to CSS 211",
    "section": "Why R?",
    "text": "Why R?\n\nR is a programming language used to perform statistical analysis, wrangle data, make data visualizaitons, and more.\n\nR is certainly not the only way to analyze data, but it’s a useful tool in your toolbox for a couple reasons.\n\nR has excellent packages (lme4) for running analyses with mixed effects models.\nR has the tidyverse, a collection of packages for writing really clean code to wrangle, summarize, and visualize data.\n\nThe tidyverse includes ggplot, one of the best plotting packages I’ve ever used.\n\nR is integrated with platforms like RStudio, which enable creation of sharable code, websites, and slides (like these)."
  },
  {
    "objectID": "lectures/week0-intro-slides.html#r-basics",
    "href": "lectures/week0-intro-slides.html#r-basics",
    "title": "Introduction to CSS 211",
    "section": "R: basics",
    "text": "R: basics\nR can be used to perform simple arithmetic calculations:\n\n# Our first R calculation\nx &lt;- 1 + 1\nprint(paste(\"The answer is:\", x))\n\n\n\n[1] \"The answer is: 2\"\n\n\nIt can also be used to calculate summary statistics, like a mean or standard deviation (std).\n\nsample_vector = c(2, 4, 6, 8)\nprint(paste(\"Mean: \", mean(sample_vector)))\n\n\n\n[1] \"Mean:  5\"\n\nprint(paste(\"SD: \", sd(sample_vector)))\n\n[1] \"SD:  2.58198889747161\""
  },
  {
    "objectID": "lectures/week0-intro-slides.html#r-simple-histogram",
    "href": "lectures/week0-intro-slides.html#r-simple-histogram",
    "title": "Introduction to CSS 211",
    "section": "R: simple histogram",
    "text": "R: simple histogram\nR can also be used to visualize data.\n\nvector = rnorm(100, mean = 50, sd = 2)\nhist(vector)"
  },
  {
    "objectID": "lectures/week0-intro-slides.html#r-simple-scatterplot",
    "href": "lectures/week0-intro-slides.html#r-simple-scatterplot",
    "title": "Introduction to CSS 211",
    "section": "R: simple scatterplot",
    "text": "R: simple scatterplot\nScatterplots can be used to visualize relationships between data.\n\nv1 = rnorm(100, mean = 50, sd = 2)\nv2 = v1 * 2 + rnorm(100, mean = 0, sd = 2)\nplot(v1, v2)"
  },
  {
    "objectID": "lectures/week0-intro-slides.html#r-simple-scatterplot-1",
    "href": "lectures/week0-intro-slides.html#r-simple-scatterplot-1",
    "title": "Introduction to CSS 211",
    "section": "R: simple scatterplot",
    "text": "R: simple scatterplot\nScatterplots can be used to visualize relationships between data.\n\nv1 = rnorm(100, mean = 50, sd = 2)\nv2 = v1 * 2 + rnorm(100, mean = 0, sd = 2)\n\n\n\n\n\n\n\n\n💭 Check-in\n\n\nWhat do you think the v2 line of code is doing?"
  },
  {
    "objectID": "lectures/week0-intro-slides.html#logistics-and-course-structure",
    "href": "lectures/week0-intro-slides.html#logistics-and-course-structure",
    "title": "Introduction to CSS 211",
    "section": "Logistics and course structure",
    "text": "Logistics and course structure\nDesigning this course required some forethought. CSS is a broad discipline: what methods and content are useful for everyone?\nMy approach:\n\nCourse content and assessments should reflect underlying learning outcomes.\n\n\nDefine and explain key concepts in statistical inference and regression analysis.\nIdentify appropriate visualizations and statistical methods for different kinds of research questions and datasets.\nImplement data wrangling, visualization, and analysis workflows in R.\nInterpret and evaluate results (visualizations, fit models, etc.) in the context of a research question.\nDesign and implement a complete statistical analysis project from research question to interpretation."
  },
  {
    "objectID": "lectures/week0-intro-slides.html#assessment-strategy",
    "href": "lectures/week0-intro-slides.html#assessment-strategy",
    "title": "Introduction to CSS 211",
    "section": "Assessment strategy",
    "text": "Assessment strategy\n\n\n\n\n\n\n\n\nAssessment\nPercentage\nLearning Outcome\n\n\n\n\nLabs\n20%\nImplementation, interpretation, evaluation\n\n\nConcept quizzes\n20%\nDefining, explaining\n\n\nMidterm\n25%\nDefining, interpreting, evaluating\n\n\nFinal project\n35%\nDesigning, identifying, implementing, evaluating"
  },
  {
    "objectID": "lectures/week0-intro-slides.html#course-policies",
    "href": "lectures/week0-intro-slides.html#course-policies",
    "title": "Introduction to CSS 211",
    "section": "Course policies",
    "text": "Course policies\n\nAttendance: Recommended, not required.\nDropped quiz: Your lowest quiz will be dropped from your grade.\nLate assignments: Generally not accepted except in cases of emergencies.\n\nIf you have a midterm scheduling conflict, let me know as soon as possible so we can try to accommodate it.\n\nGetting help: Office hours from the teaching team generally preferred to email."
  },
  {
    "objectID": "lectures/week0-intro-slides.html#note-on-ai-usage",
    "href": "lectures/week0-intro-slides.html#note-on-ai-usage",
    "title": "Introduction to CSS 211",
    "section": "Note on AI usage",
    "text": "Note on AI usage\nMy research focuses on large language models (LLMs), which have seen incredible progress in recent years. I probably don’t need to tell you how powerful ChatGPT can be!\nA few points need to be made here:\n\nTools like ChatGPT will likely keep getting better, and we shouldn’t ignore that.\nDoing CSS “in the wild” may sometimes benefit from using ChatGPT!\nSeparately, CSS “in the wild” is not done in an insolated test-taking environment.\nIt’s also still important (I think) to learn the fundamentals .\n\nI can use ChatGPT more effectively when I know what I want to do.\n\nThus, our assessments combine:\n\nTake-home work (more “ecologically valid”).\nWork requiring “endogenous” problem-solving (in-person midterm)."
  },
  {
    "objectID": "lectures/week0-intro-slides.html#final-project",
    "href": "lectures/week0-intro-slides.html#final-project",
    "title": "Introduction to CSS 211",
    "section": "Final project",
    "text": "Final project\nThe final project will be done independently, and will involve replicating from start to finish a published analysis in the CSS domain of your choice.\n\nYou have a lot of freedom here, but I recommend:\n\nThinking about this early on. Happy to point people to useful resources!\nChoosing something that’s challenging (but also doable!).\n\nExtensions to existing work (e.g., new analyses) will be encouraged.\nYour deliverables will be:\n\nA final report (turned in via Canvas).\nA final presentation (delivered week 10)."
  },
  {
    "objectID": "lectures/week0-intro-slides.html#tooling-prerequisites-and-tech-setup",
    "href": "lectures/week0-intro-slides.html#tooling-prerequisites-and-tech-setup",
    "title": "Introduction to CSS 211",
    "section": "Tooling, prerequisites, and tech setup",
    "text": "Tooling, prerequisites, and tech setup\nThis course will involve quite a bit of programming in R.\n\nWeek 1 will cover tooling. You’ll need R installed and RStudio.\nNo explicit prerequisites, though some programming background (like CSS bootcamp!) will be helpful."
  },
  {
    "objectID": "lectures/week0-intro-slides.html#topics-and-schedule",
    "href": "lectures/week0-intro-slides.html#topics-and-schedule",
    "title": "Introduction to CSS 211",
    "section": "Topics and schedule",
    "text": "Topics and schedule\nTopics will be roughly as follows:\n\nWeek 1: Introduction to R and the RStudio environment.\n\n(Plus some philophy of science.)\n\nWeeks 2-3: Data wrangling and visualization.\nWeeks 4-5: Deep dive into linear regression.\nWeeks 6-8: Advanced statistical techniques.\n\nWeeks 9-10: Best practices, final project work and presentations."
  },
  {
    "objectID": "lectures/week0-intro-slides.html#welcome-to-css-211",
    "href": "lectures/week0-intro-slides.html#welcome-to-css-211",
    "title": "Introduction to CSS 211",
    "section": "Welcome to CSS 211!",
    "text": "Welcome to CSS 211!"
  },
  {
    "objectID": "lectures/week0-intro.html",
    "href": "lectures/week0-intro.html",
    "title": "Introduction to CSS 211",
    "section": "",
    "text": "Introductions\n\nWhat is CSS?\nWhat is this course for?\nLogistics"
  },
  {
    "objectID": "lectures/week0-intro.html#goals-of-the-lecture",
    "href": "lectures/week0-intro.html#goals-of-the-lecture",
    "title": "Introduction to CSS 211",
    "section": "",
    "text": "Introductions\n\nWhat is CSS?\nWhat is this course for?\nLogistics"
  },
  {
    "objectID": "lectures/week0-intro.html#who-am-i",
    "href": "lectures/week0-intro.html#who-am-i",
    "title": "Introduction to CSS 211",
    "section": "Who am I?",
    "text": "Who am I?\n\nAssistant Teaching Professor, Cognitive Science.\nResearch interests: large language models (LLMs), language comprehension, Theory of Mind."
  },
  {
    "objectID": "lectures/week0-intro.html#what-is-css",
    "href": "lectures/week0-intro.html#what-is-css",
    "title": "Introduction to CSS 211",
    "section": "What is CSS?",
    "text": "What is CSS?\n\nIn a nutshell, Computational Social Science focuses on computational approaches to social science.\n\nAt UCSD, Social Sciences encompasses many disciplines:\n\n\nEconomics.\nPolitical Science.\nCognitive Science.\nSociology."
  },
  {
    "objectID": "lectures/week0-intro.html#what-is-social-science",
    "href": "lectures/week0-intro.html#what-is-social-science",
    "title": "Introduction to CSS 211",
    "section": "What is social science?",
    "text": "What is social science?\nSocial science refers to a domain of study: social phenomena.\n\n\nEncompasses many scales: individual human behavior, political trends, etc.\nInvolves multiple methods: qualitative interviews, statistical analysis, simulations."
  },
  {
    "objectID": "lectures/week0-intro.html#what-are-computational-methods-for",
    "href": "lectures/week0-intro.html#what-are-computational-methods-for",
    "title": "Introduction to CSS 211",
    "section": "What are computational methods for?",
    "text": "What are computational methods for?\nAnother way of asking this is: how do computational methods help us learn what we want to know?\n\n\nMany research questions benefit from large-scale quantitative analysis.\n\nMeasuring trends over time.\nComparing effects of policy interventions.\nModeling the relationships between variables.\n\nComputational methods help us do this efficiently and reliably."
  },
  {
    "objectID": "lectures/week0-intro.html#what-is-this-course-for",
    "href": "lectures/week0-intro.html#what-is-this-course-for",
    "title": "Introduction to CSS 211",
    "section": "What is this course for?",
    "text": "What is this course for?\nThe goal of this course is to introduce students to foundational concepts and methods in statistical modeling. We will emphasize both hands-on application and conceptual understanding.\nKey concepts include:\n\n\nData wrangling, summarization, and visualization.\n\nModeling data and designing analyses.\nInterrogating the assumptions of an analysis or dataset.\n\n\n\nHands-on work will be done using the R programming language."
  },
  {
    "objectID": "lectures/week0-intro.html#learning-outcomes",
    "href": "lectures/week0-intro.html#learning-outcomes",
    "title": "Introduction to CSS 211",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nMy goal is that by the end of this course, students will be able to:\n\nDefine and explain key concepts in statistical inference and regression analysis.\nIdentify appropriate visualizations and statistical methods for different kinds of research questions and datasets.\nImplement data wrangling, visualization, and analysis workflows in R.\nInterpret and evaluate results (visualizations, fit models, etc.) in the context of a research question.\nDesign and implement a complete statistical analysis project from research question to interpretation."
  },
  {
    "objectID": "lectures/week0-intro.html#why-r",
    "href": "lectures/week0-intro.html#why-r",
    "title": "Introduction to CSS 211",
    "section": "Why R?",
    "text": "Why R?\n\nR is a programming language used to perform statistical analysis, wrangle data, make data visualizaitons, and more.\n\nR is certainly not the only way to analyze data, but it’s a useful tool in your toolbox for a couple reasons.\n\n\nR has excellent packages (lme4) for running analyses with mixed effects models.\nR has the tidyverse, a collection of packages for writing really clean code to wrangle, summarize, and visualize data.\n\nThe tidyverse includes ggplot, one of the best plotting packages I’ve ever used.\n\nR is integrated with platforms like RStudio, which enable creation of sharable code, websites, and slides (like these)."
  },
  {
    "objectID": "lectures/week0-intro.html#r-basics",
    "href": "lectures/week0-intro.html#r-basics",
    "title": "Introduction to CSS 211",
    "section": "R: basics",
    "text": "R: basics\nR can be used to perform simple arithmetic calculations:\n\n# Our first R calculation\nx &lt;- 1 + 1\nprint(paste(\"The answer is:\", x))\n\n[1] \"The answer is: 2\"\n\n\nIt can also be used to calculate summary statistics, like a mean or standard deviation (std).\n\nsample_vector = c(2, 4, 6, 8)\nprint(paste(\"Mean: \", mean(sample_vector)))\n\n[1] \"Mean:  5\"\n\nprint(paste(\"SD: \", sd(sample_vector)))\n\n[1] \"SD:  2.58198889747161\""
  },
  {
    "objectID": "lectures/week0-intro.html#r-simple-histogram",
    "href": "lectures/week0-intro.html#r-simple-histogram",
    "title": "Introduction to CSS 211",
    "section": "R: simple histogram",
    "text": "R: simple histogram\nR can also be used to visualize data.\n\nvector = rnorm(100, mean = 50, sd = 2)\nhist(vector)"
  },
  {
    "objectID": "lectures/week0-intro.html#r-simple-scatterplot",
    "href": "lectures/week0-intro.html#r-simple-scatterplot",
    "title": "Introduction to CSS 211",
    "section": "R: simple scatterplot",
    "text": "R: simple scatterplot\nScatterplots can be used to visualize relationships between data.\n\nv1 = rnorm(100, mean = 50, sd = 2)\nv2 = v1 * 2 + rnorm(100, mean = 0, sd = 2)\nplot(v1, v2)"
  },
  {
    "objectID": "lectures/week0-intro.html#r-simple-scatterplot-1",
    "href": "lectures/week0-intro.html#r-simple-scatterplot-1",
    "title": "Introduction to CSS 211",
    "section": "R: simple scatterplot",
    "text": "R: simple scatterplot\nScatterplots can be used to visualize relationships between data.\n\nv1 = rnorm(100, mean = 50, sd = 2)\nv2 = v1 * 2 + rnorm(100, mean = 0, sd = 2)\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWhat do you think the v2 line of code is doing?"
  },
  {
    "objectID": "lectures/week0-intro.html#logistics-and-course-structure",
    "href": "lectures/week0-intro.html#logistics-and-course-structure",
    "title": "Introduction to CSS 211",
    "section": "Logistics and course structure",
    "text": "Logistics and course structure\nDesigning this course required some forethought. CSS is a broad discipline: what methods and content are useful for everyone?\nMy approach:\n\nCourse content and assessments should reflect underlying learning outcomes.\n\n\n\nDefine and explain key concepts in statistical inference and regression analysis.\nIdentify appropriate visualizations and statistical methods for different kinds of research questions and datasets.\nImplement data wrangling, visualization, and analysis workflows in R.\nInterpret and evaluate results (visualizations, fit models, etc.) in the context of a research question.\nDesign and implement a complete statistical analysis project from research question to interpretation."
  },
  {
    "objectID": "lectures/week0-intro.html#assessment-strategy",
    "href": "lectures/week0-intro.html#assessment-strategy",
    "title": "Introduction to CSS 211",
    "section": "Assessment strategy",
    "text": "Assessment strategy\n\n\n\n\n\n\n\n\n\nAssessment\nPercentage\nLearning Outcome\n\n\n\n\nLabs\n20%\nImplementation, interpretation, evaluation\n\n\nConcept quizzes\n20%\nDefining, explaining\n\n\nMidterm\n25%\nDefining, interpreting, evaluating\n\n\nFinal project\n35%\nDesigning, identifying, implementing, evaluating"
  },
  {
    "objectID": "lectures/week0-intro.html#course-policies",
    "href": "lectures/week0-intro.html#course-policies",
    "title": "Introduction to CSS 211",
    "section": "Course policies",
    "text": "Course policies\n\n\nAttendance: Recommended, not required.\nDropped quiz: Your lowest quiz will be dropped from your grade.\nLate assignments: Generally not accepted except in cases of emergencies.\n\nIf you have a midterm scheduling conflict, let me know as soon as possible so we can try to accommodate it.\n\nGetting help: Office hours from the teaching team generally preferred to email."
  },
  {
    "objectID": "lectures/week0-intro.html#note-on-ai-usage",
    "href": "lectures/week0-intro.html#note-on-ai-usage",
    "title": "Introduction to CSS 211",
    "section": "Note on AI usage",
    "text": "Note on AI usage\nMy research focuses on large language models (LLMs), which have seen incredible progress in recent years. I probably don’t need to tell you how powerful ChatGPT can be!\nA few points need to be made here:\n\n\nTools like ChatGPT will likely keep getting better, and we shouldn’t ignore that.\nDoing CSS “in the wild” may sometimes benefit from using ChatGPT!\nSeparately, CSS “in the wild” is not done in an insolated test-taking environment.\nIt’s also still important (I think) to learn the fundamentals .\n\nI can use ChatGPT more effectively when I know what I want to do.\n\nThus, our assessments combine:\n\nTake-home work (more “ecologically valid”).\nWork requiring “endogenous” problem-solving (in-person midterm)."
  },
  {
    "objectID": "lectures/week0-intro.html#final-project",
    "href": "lectures/week0-intro.html#final-project",
    "title": "Introduction to CSS 211",
    "section": "Final project",
    "text": "Final project\nThe final project will be done independently, and will involve replicating from start to finish a published analysis in the CSS domain of your choice.\n\n\nYou have a lot of freedom here, but I recommend:\n\nThinking about this early on. Happy to point people to useful resources!\nChoosing something that’s challenging (but also doable!).\n\nExtensions to existing work (e.g., new analyses) will be encouraged.\nYour deliverables will be:\n\nA final report (turned in via Canvas).\nA final presentation (delivered week 10)."
  },
  {
    "objectID": "lectures/week0-intro.html#tooling-prerequisites-and-tech-setup",
    "href": "lectures/week0-intro.html#tooling-prerequisites-and-tech-setup",
    "title": "Introduction to CSS 211",
    "section": "Tooling, prerequisites, and tech setup",
    "text": "Tooling, prerequisites, and tech setup\nThis course will involve quite a bit of programming in R.\n\n\nWeek 1 will cover tooling. You’ll need R installed and RStudio.\nNo explicit prerequisites, though some programming background (like CSS bootcamp!) will be helpful."
  },
  {
    "objectID": "lectures/week0-intro.html#topics-and-schedule",
    "href": "lectures/week0-intro.html#topics-and-schedule",
    "title": "Introduction to CSS 211",
    "section": "Topics and schedule",
    "text": "Topics and schedule\nTopics will be roughly as follows:\n\n\nWeek 1: Introduction to R and the RStudio environment.\n\n(Plus some philophy of science.)\n\nWeeks 2-3: Data wrangling and visualization.\nWeeks 4-5: Deep dive into linear regression.\nWeeks 6-8: Advanced statistical techniques.\n\nWeeks 9-10: Best practices, final project work and presentations."
  },
  {
    "objectID": "lectures/week0-intro.html#welcome-to-css-211",
    "href": "lectures/week0-intro.html#welcome-to-css-211",
    "title": "Introduction to CSS 211",
    "section": "Welcome to CSS 211!",
    "text": "Welcome to CSS 211!"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#goals-of-the-lecture",
    "href": "lectures/week2-r-wrangling-slides.html#goals-of-the-lecture",
    "title": "Data Wrangling in R",
    "section": "Goals of the lecture",
    "text": "Goals of the lecture\n\nWhat is data wrangling and why should we learn it?\nPrinciples of tidy data.\nWrangling with the tidyverse."
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#what-is-data-wrangling",
    "href": "lectures/week2-r-wrangling-slides.html#what-is-data-wrangling",
    "title": "Data Wrangling in R",
    "section": "What is data wrangling?",
    "text": "What is data wrangling?\n\nData wrangling refers to the processes of transforming or manipulating raw data into a useful format for downstream analysis and visualization.\n\nIn CSS, you’ll find yourself needing to:\n\nImport data from various sources, in various formats.\nClean messy variables or recast them to different types.\nFilter and transform data.\nAddress missing values.\nReshape data between different formats.\nJoin multiple datasets together.\n\nThese are all supported by the tidyverse! But what is tidy data?"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#tidy-data",
    "href": "lectures/week2-r-wrangling-slides.html#tidy-data",
    "title": "Data Wrangling in R",
    "section": "Tidy data",
    "text": "Tidy data\nWickham (2014) defines tidy data as follows:\n\n“Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.”\n\nLet’s address each of these in turn:\n\nEach variable is a column.\nEach observation is a row.\nEach type of observational unit is a table.\n\n\n\n\n\n💭 Check-in\n\n\nWith a partner, discuss what each of these criteria might mean."
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#each-variable-is-a-column",
    "href": "lectures/week2-r-wrangling-slides.html#each-variable-is-a-column",
    "title": "Data Wrangling in R",
    "section": "Each variable is a column",
    "text": "Each variable is a column\n\nA variable contains values that measure the same attribute across units.\n\n\nExamples: height, age, income, test_score, population.\nEach variable gets its own column.\n\n\nUntidy example:\n\n\n\ncountry\ngdp_2020\ngdp_2021\ngdp_2022\n\n\n\n\nUSA\n21.35\n23.368\n26.01\n\n\nChina\n14.72\n18.2\n18.32\n\n\nGermany\n3.94\n4.35\n4.16\n\n\n\n\n\n\n\n\n💭 Check-in\n\n\nWhy is this not tidy? How would you make it tidy?"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#each-variable-is-a-column-1",
    "href": "lectures/week2-r-wrangling-slides.html#each-variable-is-a-column-1",
    "title": "Data Wrangling in R",
    "section": "Each variable is a column",
    "text": "Each variable is a column\nNow, we have a column for country, year, and gdp.\n\nTidy example:\n\n\n\ncountry\nyear\ngdp_trillion\n\n\n\n\nUSA\n2020\n21.35\n\n\nUSA\n2021\n23.368\n\n\nUSA\n2022\n26.01\n\n\nChina\n2020\n14.72\n\n\nChina\n2021\n18.2\n\n\nChina\n2022\n18.32\n\n\nGermany\n2020\n3.94\n\n\nGermany\n2021\n4.35\n\n\nGermany\n2022\n4.16"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#each-observation-is-a-row",
    "href": "lectures/week2-r-wrangling-slides.html#each-observation-is-a-row",
    "title": "Data Wrangling in R",
    "section": "Each observation is a row",
    "text": "Each observation is a row\n\nAn observation contains all values measured on the same unit across attributes.\n\n\nOne row = one complete case or measurement.\nNo splitting observations across multiple rows.\n\n\nUntidy example:\n\n\n\nstate\nmeasure\nvalue\n\n\n\n\nCalifornia\npopulation\n39.5\n\n\nCalifornia\nelectoral_votes\n54\n\n\nTexas\npopulation\n31\n\n\nTexas\nelectoral_votes\n40\n\n\n\n\n\n\n\n\n💭 Check-in\n\n\nWhy is this not tidy? How would you make it tidy?"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#each-observation-is-a-row-1",
    "href": "lectures/week2-r-wrangling-slides.html#each-observation-is-a-row-1",
    "title": "Data Wrangling in R",
    "section": "Each observation is a row",
    "text": "Each observation is a row\npopulation and electoral_votes are separate measures for the same case (observation), so should be in the same row.\n\nTidy version:\n\n\n\nstate\npopulation_millions\nelectoral_votes\n\n\n\n\nCalifornia\n39.5\n54\n\n\nTexas\n31.0\n40"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#each-type-of-observational-unit-is-a-table",
    "href": "lectures/week2-r-wrangling-slides.html#each-type-of-observational-unit-is-a-table",
    "title": "Data Wrangling in R",
    "section": "Each type of observational unit is a table",
    "text": "Each type of observational unit is a table\n\nDifferent types of things being measured should be in separate tables.\n\n\nExample: Record student test scores in one table, and school-level information in another.\nExample 2: Record monthly temperatures for cities in one table, and city altitude in another.\nThis avoids repeating information (like school name for every student).\nOnce we’re ready, we can join these tables.\n\n\n\n\n\n💭 Check-in\n\n\nCan you think of other examples of “observational units” you would want to keep in separate tables?"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#tidy-data-revisited",
    "href": "lectures/week2-r-wrangling-slides.html#tidy-data-revisited",
    "title": "Data Wrangling in R",
    "section": "Tidy data revisited",
    "text": "Tidy data revisited\n\n“Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.” (Wickham, 2014)\n\n\nThese guidelines aren’t arbitrary.\nA standard tidy structure is helpful for visualizing and analyzing data.\n\n\n\n\n\nNote\n\n\nIn fact, I often conceptualize “tidy” data in terms of:\n\nWhat are the axes in a plot I want to make? (These should be separate columns.)\nWhat are the terms in a regression formula I want to write? (These should be separate columns.)"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#welcome-to-the-tidyverse",
    "href": "lectures/week2-r-wrangling-slides.html#welcome-to-the-tidyverse",
    "title": "Data Wrangling in R",
    "section": "Welcome to the tidyverse",
    "text": "Welcome to the tidyverse\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.”\n\n\nFundamentally, the tidyverse rests on the philosophy of tidy data.\nIncludes a range of functions, from importing data (read_csv) to merging datasets (inner_join).\n\n\n\n\n\n💭 Installation Help\n\n\n\nInstall the tidyverse using install.packages(\"tidyverse\").\nThen, load it using library(tidyverse)."
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#loading-the-tidyverse",
    "href": "lectures/week2-r-wrangling-slides.html#loading-the-tidyverse",
    "title": "Data Wrangling in R",
    "section": "Loading the tidyverse",
    "text": "Loading the tidyverse\n\n### Loading the tidyverse\nlibrary(tidyverse)\ntidyverse::tidyverse_packages()\n\n\n\n [1] \"broom\"         \"conflicted\"    \"cli\"           \"dbplyr\"       \n [5] \"dplyr\"         \"dtplyr\"        \"forcats\"       \"ggplot2\"      \n [9] \"googledrive\"   \"googlesheets4\" \"haven\"         \"hms\"          \n[13] \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n[17] \"modelr\"        \"pillar\"        \"purrr\"         \"ragg\"         \n[21] \"readr\"         \"readxl\"        \"reprex\"        \"rlang\"        \n[25] \"rstudioapi\"    \"rvest\"         \"stringr\"       \"tibble\"       \n[29] \"tidyr\"         \"xml2\"          \"tidyverse\"    \n\n\n\nWe’ll cover some of these packages later in this class, like broom and ggplot2.\nThis week, we’ll focus on functions from readr, magrittr, dplyr, and tidyr."
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#the-pipe",
    "href": "lectures/week2-r-wrangling-slides.html#the-pipe",
    "title": "Data Wrangling in R",
    "section": "The pipe (%>%)",
    "text": "The pipe (%&gt;%)\n\nThe pipe operator (%&gt;%) lets you chain functions together in a readable way.\n\n\nTakes the output from the left side and feeds it as the first argument to the right side.\nInstead of nesting functions, can pipe outputs into each other.\nMakes code more readable and easier to follow.\nWe’ll see examples coming up!"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#part-1-basic-data-manipulation",
    "href": "lectures/week2-r-wrangling-slides.html#part-1-basic-data-manipulation",
    "title": "Data Wrangling in R",
    "section": "Part 1: Basic data manipulation",
    "text": "Part 1: Basic data manipulation\nImporting, filtering, mutating, and summarizing data."
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#reading-in-a-dataset",
    "href": "lectures/week2-r-wrangling-slides.html#reading-in-a-dataset",
    "title": "Data Wrangling in R",
    "section": "Reading in a dataset",
    "text": "Reading in a dataset\nOne of the most fundamental things you’ll need to do is import data, e.g., from a .csv file.\n\nread_csv takes as input a filepath (either locally, or a URL).\nOutput is a tibble.\n\n\ndf_conc &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/wrangling/concreteness.csv\")\nnrow(df_conc)\n\n\n\n[1] 28612\n\n\n\n\n\n\n💭 Check-in\n\n\nUse of the functions we’ve discussed (nrow, names, head, etc.) to explore the dataset."
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#basics-of-dplyr",
    "href": "lectures/week2-r-wrangling-slides.html#basics-of-dplyr",
    "title": "Data Wrangling in R",
    "section": "Basics of dplyr",
    "text": "Basics of dplyr\n\ndplyr is a package in the tidyverse that contains functions for transforming and manipulating dataframes.\n\nEach function is a “verb”, for instance:\n\nfilter relevant rows.\nmutate the dataset by adding new variables.\npull specific columns by name.\n\nrename specific columns.\ngroup_by specific values or factors.\nsummarize dataset with summary statistics."
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#the-filter",
    "href": "lectures/week2-r-wrangling-slides.html#the-filter",
    "title": "Data Wrangling in R",
    "section": "The filter",
    "text": "The filter\n\nfilter works by selecting a subset of rows based on some boolean condition.\n\n\ndf_conc %&gt;%  ### piping!\n  filter(Word == \"class\")\n\n\n\n# A tibble: 1 × 4\n  Word  Concreteness Frequency Dom_Pos\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  \n1 class         3.85      5985 Noun   \n\n\n\n\n\n\n💭 Check-in\n\n\nTry using filter to return words that:\n\nHave a Concreteness &gt; 4.5.\nHave a Dom_Pos of Noun.\n\nHint: You can chain multiple filter statements together with a %&gt;% operator."
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#mutate-your-data",
    "href": "lectures/week2-r-wrangling-slides.html#mutate-your-data",
    "title": "Data Wrangling in R",
    "section": "mutate your data",
    "text": "mutate your data\n\nmutate works by creating a new variable (or overwriting an existing one) via applying some transformation to an existing variable or set of variables.\n\nLet’s create a log frequency variable, since Frequency is highly right-skewed.\n\ndf_conc = df_conc %&gt;%  ### piping!\n  mutate(log_freq = log10(Frequency))\n\nsummary(df_conc$log_freq)\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   -Inf  0.8451  1.4624    -Inf  2.1004  6.3293 \n\n\n\n\n\n\n💭 Check-in\n\n\nWhy do you think the mininum log_freq value is -Inf?"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#mutate-pt.-2",
    "href": "lectures/week2-r-wrangling-slides.html#mutate-pt.-2",
    "title": "Data Wrangling in R",
    "section": "mutate (pt. 2)",
    "text": "mutate (pt. 2)\nSome words in our dataset have a Frequency of 0. One strategy is to add one to those values before taking the log.\n\ndf_conc = df_conc %&gt;%  ### piping!\n  mutate(log_freq = log10(Frequency + 1))\n\nsummary(df_conc$log_freq)\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.9031  1.4771  1.5793  2.1038  6.3293"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#group_by-specific-factors-and-summarise",
    "href": "lectures/week2-r-wrangling-slides.html#group_by-specific-factors-and-summarise",
    "title": "Data Wrangling in R",
    "section": "group_by specific factors and summarise",
    "text": "group_by specific factors and summarise\n\nThe group_by function is extremely useful for grouping your data by specific factors.\nYou can %&gt;% this into summarise to calculate summary statistics for each group.\n\n\ndf_conc %&gt;%\n  group_by(Dom_Pos) %&gt;%\n  summarise(mean_conc = mean(Concreteness)) %&gt;%\n  head(2)\n\n\n\n# A tibble: 2 × 2\n  Dom_Pos   mean_conc\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Adjective      2.50\n2 Adverb         2.06\n\n\n\n\n\n\n💭 Check-in\n\n\nIn addition to calculating mean, try also calculating the sd; additionally, try calculating the mean and sd for log_freq by Dom_Pos."
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#counting-the-number-of-observations",
    "href": "lectures/week2-r-wrangling-slides.html#counting-the-number-of-observations",
    "title": "Data Wrangling in R",
    "section": "Counting the number of observations",
    "text": "Counting the number of observations\nOften relevant for a group_by calculation is the number of observations per group.\n\ndf_conc %&gt;%\n  group_by(Dom_Pos) %&gt;%\n  summarise(mean_conc = mean(Concreteness),\n            count = n()) %&gt;%\n  head(2)\n\n\n\n# A tibble: 2 × 3\n  Dom_Pos   mean_conc count\n  &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Adjective      2.50  6112\n2 Adverb         2.06  1876"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#hands-on-practice",
    "href": "lectures/week2-r-wrangling-slides.html#hands-on-practice",
    "title": "Data Wrangling in R",
    "section": "Hands-on practice",
    "text": "Hands-on practice\nNow let’s get some hands-on practice applying these new techniques.\n\nRead in the pokemon.csv dataset from the ucsd_211_datasets GitHub using read_csv.\nUse group_by to count the number of Pokemon of each Type 1. Which Type 1 is most frequent?\nThen, filter the dataset to only include Pokemon of the most frequent type (based on what you learned).\nOf this filtered dataset, group_by legendary status and calculate the mean(Attack)"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#hands-on-practice-solution",
    "href": "lectures/week2-r-wrangling-slides.html#hands-on-practice-solution",
    "title": "Data Wrangling in R",
    "section": "Hands-on practice (solution)",
    "text": "Hands-on practice (solution)\n\n### Part 1\ndf_pokemon &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/wrangling/pokemon.csv\")\n\n### Part 2\nmax_type = df_pokemon %&gt;% \n  group_by(`Type 1`) %&gt;% \n  summarise(count = n()) %&gt;%\n  slice_max(count) %&gt;%\n  pull(`Type 1`)\n\n### Parts 3-4\ndf_pokemon %&gt;%\n  filter(`Type 1` == max_type) %&gt;%\n  group_by(Legendary) %&gt;%\n  summarise(mean_attack = mean(Attack))\n\n\n\n# A tibble: 2 × 2\n  Legendary mean_attack\n  &lt;lgl&gt;           &lt;dbl&gt;\n1 FALSE            72.8\n2 TRUE            111. \n\n\n\n\n\n\n💭 Check-in\n\n\nNotice any functions we haven’t discussed?"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#some-new-functions",
    "href": "lectures/week2-r-wrangling-slides.html#some-new-functions",
    "title": "Data Wrangling in R",
    "section": "Some new functions",
    "text": "Some new functions\nAbove, we used a combination of read_csv, filter, group_by, and summarise. But we also used some other new functions:\n\nslice_max: returns rows with the max values of a given column.\n\nCan also use slice_head, slice_tail, slice_min, and slice_sample.\n\npull: returns the individual vector/value from a given column.\n\n\n\n\n\n💭 Check-in\n\n\nAny questions before we move on to more dplyr functions and more aspects of the tidyverse more generally?"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#select-specific-columns",
    "href": "lectures/week2-r-wrangling-slides.html#select-specific-columns",
    "title": "Data Wrangling in R",
    "section": "select specific columns",
    "text": "select specific columns\n\nThe select function can be used to select certain columns for further analysis.\n\nThis is especially useful if you want to %&gt;% those columns into, say, a correlation matrix.\n\ndf_conc %&gt;%\n  select(log_freq, Concreteness, Frequency) %&gt;%\n  cor()\n\n\n\n              log_freq Concreteness   Frequency\nlog_freq     1.0000000   0.16983740  0.21158008\nConcreteness 0.1698374   1.00000000 -0.01501261\nFrequency    0.2115801  -0.01501261  1.00000000\n\n\n\n\n\n\n💭 Check-in\n\n\nTry building a correlation matrix of the numerical columns from the Pokemon dataset (e.g., Attack, Defense, etc.). Are there any conclusions you can draw?"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#part-2-more-complex-wrangling",
    "href": "lectures/week2-r-wrangling-slides.html#part-2-more-complex-wrangling",
    "title": "Data Wrangling in R",
    "section": "Part 2: More complex wrangling",
    "text": "Part 2: More complex wrangling\nMissing data, reshaping, and joining."
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#missing-values",
    "href": "lectures/week2-r-wrangling-slides.html#missing-values",
    "title": "Data Wrangling in R",
    "section": "Missing values",
    "text": "Missing values\n\nReal-world data often has missing values, which are treated as NA (“Not Available”) by R.\n\nThere are a few relevant issues here:\n\nFirst, you need to identify the presence of missing values.\nSecond, you need to address the issue.\n\nIgnore them?\nRemove any rows with missing values?\nTry to impute their values?"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#identifying-missing-values",
    "href": "lectures/week2-r-wrangling-slides.html#identifying-missing-values",
    "title": "Data Wrangling in R",
    "section": "Identifying missing values",
    "text": "Identifying missing values\nWe can use the is.na function to check for missing values.\n\n### Reading in dataset\ndf_titanic &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/wrangling/titanic.csv\")\n\n### Check for missing values; returns a vector of boolean values\nis.na(df_titanic$Age)[0:5]\n\n\n\n[1] FALSE FALSE FALSE FALSE FALSE\n\n\n\n\n\n\n💭 Check-in\n\n\nHow many missing values are there total in the Age column? What about Survived or Cabin? (Hint: You can sum boolean values like TRUE and FALSE)."
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#identify-missing-values-pt.-2",
    "href": "lectures/week2-r-wrangling-slides.html#identify-missing-values-pt.-2",
    "title": "Data Wrangling in R",
    "section": "Identify missing values (pt. 2)",
    "text": "Identify missing values (pt. 2)\nWe can combine some handy functions (across and everything) to sum up the number of NA values per column.\n\n## Get number of NA across all columns\ndf_titanic %&gt;% \n  summarise(across(everything(), ~ sum(is.na(.))))\n\n\n\n# A tibble: 1 × 12\n  PassengerId Survived Pclass  Name   Sex   Age SibSp Parch Ticket  Fare Cabin\n        &lt;int&gt;    &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1           0        0      0     0     0   177     0     0      0     0   687\n# ℹ 1 more variable: Embarked &lt;int&gt;\n\n\n\n\n\n\nNote\n\n\nNA values seem disproportionately located in the Cabin column."
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#addressing-missing-values",
    "href": "lectures/week2-r-wrangling-slides.html#addressing-missing-values",
    "title": "Data Wrangling in R",
    "section": "Addressing missing values",
    "text": "Addressing missing values\nNow that we’ve identified missing values, we have a few options:\n\nWe could ignore them, i.e., perform analyses as if they don’t exist.\nWe could remove missing values from our dataset.\nWe could impute the missing values (guess).\n\n\n\n\n\n💭 Check-in\n\n\nWhat do you think are the trade-offs of each approach?"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#the-problem-with-ignoring-na",
    "href": "lectures/week2-r-wrangling-slides.html#the-problem-with-ignoring-na",
    "title": "Data Wrangling in R",
    "section": "The problem with ignoring NA",
    "text": "The problem with ignoring NA\nUnsurprisingly, ignoring NA values doesn’t mean they go away.\n\nIn some cases, analyses or visualizations may “quietly” ignore them.\n\nThis is bad because it means you’re not fully aware of what goes into an analysis.\n\nIn other cases, it’ll interfere with your summary statistics.\n\n\n## Mean defaults to NA b/c of missing values\nmean(df_titanic$Age)\n\n\n\n[1] NA\n\n## Temporary fix is to remove them from this calculation...\nmean(df_titanic$Age, na.rm = TRUE)\n\n[1] 29.69912"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#removing-missing-values",
    "href": "lectures/week2-r-wrangling-slides.html#removing-missing-values",
    "title": "Data Wrangling in R",
    "section": "Removing missing values",
    "text": "Removing missing values\nAnother common approach to NA values is simply to remove them.\nWe can do this selectively in a given analysis, like below:\n\n## Temporary fix is to remove them from this calculation...\nmean(df_titanic$Age, na.rm = TRUE)\n\n\n\n[1] 29.69912\n\n\nOr we can actually filter our dataset to remove them entirely (or use drop_na).\n\ndf_titanic_subset = df_titanic %&gt;%\n  filter(!is.na(Age))\n\n\n\n\n\n\n\n\n💭 Check-in\n\n\nDo you think we should always just remove all NA values? Why or why not?"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#not-all-na-values-matter-equally",
    "href": "lectures/week2-r-wrangling-slides.html#not-all-na-values-matter-equally",
    "title": "Data Wrangling in R",
    "section": "Not all NA values matter equally",
    "text": "Not all NA values matter equally\nWhether or not we care that something is NA depends on our research questions!\n\nAge might be relevant for analyses of Survived.\nCabin may not really matter.\n\n\n### Cabin mostly NA, but maybe that's fine?\nsum(is.na(df_titanic$Cabin))\n\n\n\n[1] 687\n\ndf_titanic$Cabin[1:10]\n\n [1] NA     \"C85\"  NA     \"C123\" NA     NA     \"E46\"  NA     NA     NA"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#how-do-na-values-distribute",
    "href": "lectures/week2-r-wrangling-slides.html#how-do-na-values-distribute",
    "title": "Data Wrangling in R",
    "section": "How do NA values distribute?",
    "text": "How do NA values distribute?\nOne factor that might affect our approach is how NA values distribute.\n\nAre they distributed randomly with respect to some other variable (e.g., Survived)?\nOr are they systematic?\nEither way, it’s helpful to know.\n\n\n### Lower NA information about Cabins for people who survived\ndf_titanic %&gt;%\n  group_by(Survived) %&gt;%\n  summarise(proportion_na = mean(is.na(Cabin)))\n\n\n\n# A tibble: 2 × 2\n  Survived proportion_na\n     &lt;dbl&gt;         &lt;dbl&gt;\n1        0         0.876\n2        1         0.602\n\n\n\n\n\n\n💭 Check-in\n\n\nWhat is the relative rate of NA values across levels of Survived for Age?"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#imputing-missing-values",
    "href": "lectures/week2-r-wrangling-slides.html#imputing-missing-values",
    "title": "Data Wrangling in R",
    "section": "Imputing missing values",
    "text": "Imputing missing values\n\nImputing values means inferring them based on some other set of properties or set of assumption. I.e., an “educated guess”.\n\n\nCan set to mean or median value (e.g., mean(Age)).\nCan “guess” value based on some other property (e.g., mean(Age) for that Pclass).\n\n\ndf_titanic = df_titanic %&gt;%\n  group_by(Pclass) %&gt;% ### Group by passenger class\n  mutate(Age_imputed = ifelse(\n    is.na(Age), mean(Age, na.rm = TRUE),  ### Replace with mean for that Pclass\n    Age)) %&gt;% ### otherwise just use original age\n  ungroup()\n\n\ndf_titanic %&gt;%\n  group_by(Pclass) %&gt;%\n  summarise(mean_age = mean(Age_imputed))\n\n\n\n# A tibble: 3 × 2\n  Pclass mean_age\n   &lt;dbl&gt;    &lt;dbl&gt;\n1      1     38.2\n2      2     29.9\n3      3     25.1"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#missing-values-wrap-up",
    "href": "lectures/week2-r-wrangling-slides.html#missing-values-wrap-up",
    "title": "Data Wrangling in R",
    "section": "Missing values: wrap-up",
    "text": "Missing values: wrap-up\n\nReal-world data often has missing values.\nAt minimum, it’s important to identify and characterize NA values in your dataset.\nIdeally, you should develop a strategy for dealing with them with intentionality.\n\n\n\n\n\nNote\n\n\nSometimes missing values show up when we join datasets. We’ll discuss joining soon."
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#reshaping-data",
    "href": "lectures/week2-r-wrangling-slides.html#reshaping-data",
    "title": "Data Wrangling in R",
    "section": "Reshaping data",
    "text": "Reshaping data\n\nReshaping data means converting it to either a “longer” format (from a “wide” format) or the other way around.\n\n\nIn many cases, this means making data tidy (long format).\nThe tidyr package contains key functions for this, such as pivot_longer and pivot_wider.\ntidyr also includes functions to separate information from a single column across multiple columns (or unite, to do the opposite.)\n\n\n\n\n\nNote\n\n\nThe best way to understand this is to look at some examples!"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#pivot_longer-from-wide-to-long",
    "href": "lectures/week2-r-wrangling-slides.html#pivot_longer-from-wide-to-long",
    "title": "Data Wrangling in R",
    "section": "pivot_longer: from wide to long",
    "text": "pivot_longer: from wide to long\nLet’s start by creating a wide dataset, which might be considered “messy”.\n\ndf_messy_1 = data.frame(john = c(10, 11),\n                        mary = c(20, 25),\n                        jane = c(5, 10),\n                        treatment = c('a', 'b'))\ndf_messy_1\n\n\n\n  john mary jane treatment\n1   10   20    5         a\n2   11   25   10         b\n\n\n\n\n\n\n💭 Check-in\n\n\nWhy is this considered “messy”? And what would a “long” version look like?"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#pivot_longer-from-wide-to-long-1",
    "href": "lectures/week2-r-wrangling-slides.html#pivot_longer-from-wide-to-long-1",
    "title": "Data Wrangling in R",
    "section": "pivot_longer: from wide to long",
    "text": "pivot_longer: from wide to long\nWe can use the pivot_longer function to transform this into a long format.\n\ndf_messy_1 %&gt;%\n  pivot_longer(cols = c(john, mary, jane), ### which columns to pivot\n               names_to = 'name', ### what to call column with keys\n               values_to = 'result') ### what to call column with values\n\n\n\n# A tibble: 6 × 3\n  treatment name  result\n  &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;\n1 a         john      10\n2 a         mary      20\n3 a         jane       5\n4 b         john      11\n5 b         mary      25\n6 b         jane      10\n\n\n\nCrucial argument is cols (which columns to pivot).\nnames_to and values_to are helpful for making the resulting table more readable."
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#pivot_longer-from-wide-to-long-2",
    "href": "lectures/week2-r-wrangling-slides.html#pivot_longer-from-wide-to-long-2",
    "title": "Data Wrangling in R",
    "section": "pivot_longer: from wide to long",
    "text": "pivot_longer: from wide to long\nLet’s look at another example.\n\ndf_messy_2 = data.frame(name = c('john', 'mary', 'jane'),\n                        a = c(10, 20, 5),\n                        b = c(11, 25, 10))\n\ndf_messy_2\n\n\n\n  name  a  b\n1 john 10 11\n2 mary 20 25\n3 jane  5 10\n\ndf_tidy = df_messy_2 %&gt;%\n  pivot_longer(cols = c(a, b),\n               names_to = \"treatment\",\n                values_to = \"result\")\n\ndf_tidy\n\n# A tibble: 6 × 3\n  name  treatment result\n  &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n1 john  a             10\n2 john  b             11\n3 mary  a             20\n4 mary  b             25\n5 jane  a              5\n6 jane  b             10"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#pivot_wider-back-to-wide-format",
    "href": "lectures/week2-r-wrangling-slides.html#pivot_wider-back-to-wide-format",
    "title": "Data Wrangling in R",
    "section": "pivot_wider: back to wide format!",
    "text": "pivot_wider: back to wide format!\n\npivot_wider reshapes data into wide format.\n\n\n### To recreate first table\ndf_tidy %&gt;%\n  pivot_wider(names_from = \"name\",\n              values_from = \"result\")\n\n\n\n# A tibble: 2 × 4\n  treatment  john  mary  jane\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 a            10    20     5\n2 b            11    25    10\n\n### To recreate second table\ndf_tidy %&gt;%\n  pivot_wider(names_from = \"treatment\",\n              values_from = \"result\")\n\n# A tibble: 3 × 3\n  name      a     b\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 john     10    11\n2 mary     20    25\n3 jane      5    10"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#your-turn-pt.-1",
    "href": "lectures/week2-r-wrangling-slides.html#your-turn-pt.-1",
    "title": "Data Wrangling in R",
    "section": "Your turn! (pt. 1)",
    "text": "Your turn! (pt. 1)\nIf you’ve loaded tidyverse, you should have access to (at least) the following data tables:\n\ntable1\ntable2\n\n\n\n\n\n💭 Check-in\n\n\nHow would you pivot_wider table2 such that it resembles table1?"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#your-turn-pt.-2",
    "href": "lectures/week2-r-wrangling-slides.html#your-turn-pt.-2",
    "title": "Data Wrangling in R",
    "section": "Your turn! (pt. 2)",
    "text": "Your turn! (pt. 2)\nNow let’s try pivot_longer with a more complex example. Load the missing work example from this Full Stack Economics graph.\n\n### Part 1\ndf_work &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/wrangling/missing_work.csv\")\nhead(df_work, 3)\n\n\n\n# A tibble: 3 × 7\n   Year `Child care problems` Maternity or paternity le…¹ Other family or pers…²\n  &lt;dbl&gt;                 &lt;dbl&gt;                       &lt;dbl&gt;                  &lt;dbl&gt;\n1  2012                    18                         313                    246\n2  2012                    35                         278                    230\n3  2012                    13                         245                    246\n# ℹ abbreviated names: ¹​`Maternity or paternity leave`,\n#   ²​`Other family or personal obligations`\n# ℹ 3 more variables: `Illness or injury` &lt;dbl&gt;, Vacation &lt;dbl&gt;, Month &lt;dbl&gt;\n\n\n\n\n\n\n💭 Check-in\n\n\nRight now, the table stores different reasons for missing work as different columns.\nInstead, let’s pivot_longer so the table has a column for Reason (for missing) and a column for Count (number of days missed)."
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#other-handy-tidyr-functions",
    "href": "lectures/week2-r-wrangling-slides.html#other-handy-tidyr-functions",
    "title": "Data Wrangling in R",
    "section": "Other handy tidyr functions",
    "text": "Other handy tidyr functions\nAlthough pivoting is probably the main function enabled by tidyr, it also contains other valuable reshaping functions:\n\nseparate “splits” values within a given column into multiple columns.\nunite does the opposite.\n\n\ntable3 %&gt;%\n  separate(rate,  ### column to separate\n           into = c(\"cases\", \"population\"),  ### columns to split into\n           convert = TRUE, ### convert into numeric\n           sep = \"/\") ### separator currently separating values\n\n\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#joining-data",
    "href": "lectures/week2-r-wrangling-slides.html#joining-data",
    "title": "Data Wrangling in R",
    "section": "Joining data",
    "text": "Joining data\n\nJoining is an operation in which columns from data Y are added to data X, matching observations based on shared variables (or “keys”).\n\n\nIn real-world analysis, data is often stored across multiple sources.\nJoining brings together related information from separate tables.\nKey types: left_join(), right_join(), inner_join(), full_join()\n\n\nExample scenario:\n\nTable 1: State election results (state, biden_pct, turnout)\n\nTable 2: State demographics (state, median_income, college_pct)\n\nHere, we might join the tables based on the overlapping state variable.\nGoal: Analyze how median_income and college_pct relates to voting patterns."
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#types-of-joins",
    "href": "lectures/week2-r-wrangling-slides.html#types-of-joins",
    "title": "Data Wrangling in R",
    "section": "Types of joins",
    "text": "Types of joins\nGiven tables X and Y, you have several options for how to join them.\n\ninner_join: only keep observations from X with matching key in Y.\nleft_join: keep all observations from X, join matching ones from Y.\nright-join: keep all observations from Y, join matching ones from X.\nfull_join: keep all observations in both X and Y."
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#simple-join-example",
    "href": "lectures/week2-r-wrangling-slides.html#simple-join-example",
    "title": "Data Wrangling in R",
    "section": "Simple join example",
    "text": "Simple join example\nSuppose we have two tables: table4a contains information about the cases in each country (with different columns for different years), and table4b contains information about the population of each country (again, with different columns for different years).\nIdeally, we’d like to join these tables.\n\n\n\n\n💭 Check-in\n\n\nWhat steps will we need to take to join them?\n(Hint: It may involve more than joining.)"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#simple-join-example-1",
    "href": "lectures/week2-r-wrangling-slides.html#simple-join-example-1",
    "title": "Data Wrangling in R",
    "section": "Simple join example",
    "text": "Simple join example\n\n### First, tidy each table\ntidy4a = table4a %&gt;%\n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\ntidy4b = table4b %&gt;%\n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"population\")\n\n### Then, join\nleft_join(tidy4a, tidy4b)\n\n\n\n# A tibble: 6 × 4\n  country     year   cases population\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan 1999     745   19987071\n2 Afghanistan 2000    2666   20595360\n3 Brazil      1999   37737  172006362\n4 Brazil      2000   80488  174504898\n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583\n\n\n\n\n\n\n💭 Check-in\n\n\nIn this case, would it matter whether we use left_join or inner_join? If not, under what circumstances would this distinction matter?"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#a-more-complex-example",
    "href": "lectures/week2-r-wrangling-slides.html#a-more-complex-example",
    "title": "Data Wrangling in R",
    "section": "A more complex example",
    "text": "A more complex example\nWe’ve already loaded information about word concreteness. Now let’s load another dataset about the average age of acquisition at which words are learned, and combine that with the concreteness dataset.\n\ndf_aoa &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/wrangling/AoA.csv\")\nnrow(df_aoa)\n\n\n\n[1] 31124\n\nhead(df_aoa)\n\n# A tibble: 6 × 2\n  Word        AoA\n  &lt;chr&gt;     &lt;dbl&gt;\n1 a          2.89\n2 aardvark   9.89\n3 abacus     8.69\n4 abalone   12.2 \n5 abandon    8.32\n6 abandoner 11.9 \n\n\n\n\n\n\n💭 Check-in\n\n\nHow would you join this dataset with the concreteness dataset? What kind of join operation might you want to use?"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#a-more-complex-example-pt.-2",
    "href": "lectures/week2-r-wrangling-slides.html#a-more-complex-example-pt.-2",
    "title": "Data Wrangling in R",
    "section": "A more complex example (pt. 2)",
    "text": "A more complex example (pt. 2)\nThe concreteness and AoA datasets are not fully-overlapping. Concreteness contains words not in AoA and vice versa.\n\n### Inner join (keep only intersection)\ndf_inner = df_conc %&gt;%\n  inner_join(df_aoa)\nnrow(df_inner)\n\n\n\n[1] 23568\n\n### Left join (keep all concreteness)\ndf_left = df_conc %&gt;%\n  left_join(df_aoa)\nnrow(df_left)\n\n[1] 28612\n\n### Right join (keep all AoA)\ndf_right = df_conc %&gt;%\n  right_join(df_aoa)\nnrow(df_right)\n\n[1] 31124\n\n### full join (keep everything)\ndf_full = df_conc %&gt;%\n  full_join(df_aoa)\nnrow(df_full)\n\n[1] 36168"
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#joining-the-details",
    "href": "lectures/week2-r-wrangling-slides.html#joining-the-details",
    "title": "Data Wrangling in R",
    "section": "Joining: the details",
    "text": "Joining: the details\nWhen joining, there are a few other details we’ve glossed over:\n\nby: this specifies which keys to join on (will default to all overlapping).\nmultiple: how to handle cases where rows from X match multiple rows from Y.\nunmatched: how to handle cases where rows would be dropped."
  },
  {
    "objectID": "lectures/week2-r-wrangling-slides.html#putting-it-all-together",
    "href": "lectures/week2-r-wrangling-slides.html#putting-it-all-together",
    "title": "Data Wrangling in R",
    "section": "Putting it all together",
    "text": "Putting it all together\nThese lectures have covered the basics of data wrangling with the tidyverse.\nA typical workflow will involve:\n\nReading data (read_csv).\nExploring the data (is.na, summary, summarise).\nClean data (mutate, filter).\nReshape if needed (pivot_longer/wider).\nJoin if needed (inner_join, etc.).\n\n\n\n\n\nNote\n\n\nThe end result is a dataset ready for further analysis and visualization (coming up!)."
  },
  {
    "objectID": "lectures/week2-wrangling.html",
    "href": "lectures/week2-wrangling.html",
    "title": "Data Wrangling in R",
    "section": "",
    "text": "What is data wrangling and why should we learn it?\nPrinciples of tidy data.\nWrangling with the tidyverse."
  },
  {
    "objectID": "lectures/week2-wrangling.html#goals-of-the-lecture",
    "href": "lectures/week2-wrangling.html#goals-of-the-lecture",
    "title": "Data Wrangling in R",
    "section": "",
    "text": "What is data wrangling and why should we learn it?\nPrinciples of tidy data.\nWrangling with the tidyverse."
  },
  {
    "objectID": "lectures/week2-wrangling.html#what-is-data-wrangling",
    "href": "lectures/week2-wrangling.html#what-is-data-wrangling",
    "title": "Data Wrangling in R",
    "section": "What is data wrangling?",
    "text": "What is data wrangling?\n\nData wrangling refers to the processes of transforming or manipulating raw data into a useful format for downstream analysis and visualization.\n\nIn CSS, you’ll find yourself needing to:\n\n\nImport data from various sources, in various formats.\nClean messy variables or recast them to different types.\nFilter and transform data.\nAddress missing values.\nReshape data between different formats.\nJoin multiple datasets together.\n\n\nThese are all supported by the tidyverse! But what is tidy data?"
  },
  {
    "objectID": "lectures/week2-wrangling.html#tidy-data",
    "href": "lectures/week2-wrangling.html#tidy-data",
    "title": "Data Wrangling in R",
    "section": "Tidy data",
    "text": "Tidy data\nWickham (2014) defines tidy data as follows:\n\n“Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.”\n\nLet’s address each of these in turn:\n\n\nEach variable is a column.\nEach observation is a row.\nEach type of observational unit is a table.\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWith a partner, discuss what each of these criteria might mean."
  },
  {
    "objectID": "lectures/week2-wrangling.html#each-variable-is-a-column",
    "href": "lectures/week2-wrangling.html#each-variable-is-a-column",
    "title": "Data Wrangling in R",
    "section": "Each variable is a column",
    "text": "Each variable is a column\n\nA variable contains values that measure the same attribute across units.\n\n\n\nExamples: height, age, income, test_score, population.\nEach variable gets its own column.\n\n\n\nUntidy example:\n\n\n\ncountry\ngdp_2020\ngdp_2021\ngdp_2022\n\n\n\n\nUSA\n21.35\n23.368\n26.01\n\n\nChina\n14.72\n18.2\n18.32\n\n\nGermany\n3.94\n4.35\n4.16\n\n\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWhy is this not tidy? How would you make it tidy?"
  },
  {
    "objectID": "lectures/week2-wrangling.html#each-variable-is-a-column-1",
    "href": "lectures/week2-wrangling.html#each-variable-is-a-column-1",
    "title": "Data Wrangling in R",
    "section": "Each variable is a column",
    "text": "Each variable is a column\nNow, we have a column for country, year, and gdp.\n\nTidy example:\n\n\n\ncountry\nyear\ngdp_trillion\n\n\n\n\nUSA\n2020\n21.35\n\n\nUSA\n2021\n23.368\n\n\nUSA\n2022\n26.01\n\n\nChina\n2020\n14.72\n\n\nChina\n2021\n18.2\n\n\nChina\n2022\n18.32\n\n\nGermany\n2020\n3.94\n\n\nGermany\n2021\n4.35\n\n\nGermany\n2022\n4.16"
  },
  {
    "objectID": "lectures/week2-wrangling.html#each-observation-is-a-row",
    "href": "lectures/week2-wrangling.html#each-observation-is-a-row",
    "title": "Data Wrangling in R",
    "section": "Each observation is a row",
    "text": "Each observation is a row\n\nAn observation contains all values measured on the same unit across attributes.\n\n\n\nOne row = one complete case or measurement.\nNo splitting observations across multiple rows.\n\n\n\nUntidy example:\n\n\n\nstate\nmeasure\nvalue\n\n\n\n\nCalifornia\npopulation\n39.5\n\n\nCalifornia\nelectoral_votes\n54\n\n\nTexas\npopulation\n31\n\n\nTexas\nelectoral_votes\n40\n\n\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWhy is this not tidy? How would you make it tidy?"
  },
  {
    "objectID": "lectures/week2-wrangling.html#each-observation-is-a-row-1",
    "href": "lectures/week2-wrangling.html#each-observation-is-a-row-1",
    "title": "Data Wrangling in R",
    "section": "Each observation is a row",
    "text": "Each observation is a row\npopulation and electoral_votes are separate measures for the same case (observation), so should be in the same row.\n\nTidy version:\n\n\n\nstate\npopulation_millions\nelectoral_votes\n\n\n\n\nCalifornia\n39.5\n54\n\n\nTexas\n31.0\n40"
  },
  {
    "objectID": "lectures/week2-wrangling.html#each-type-of-observational-unit-is-a-table",
    "href": "lectures/week2-wrangling.html#each-type-of-observational-unit-is-a-table",
    "title": "Data Wrangling in R",
    "section": "Each type of observational unit is a table",
    "text": "Each type of observational unit is a table\n\nDifferent types of things being measured should be in separate tables.\n\n\n\nExample: Record student test scores in one table, and school-level information in another.\nExample 2: Record monthly temperatures for cities in one table, and city altitude in another.\nThis avoids repeating information (like school name for every student).\nOnce we’re ready, we can join these tables.\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nCan you think of other examples of “observational units” you would want to keep in separate tables?"
  },
  {
    "objectID": "lectures/week2-wrangling.html#tidy-data-revisited",
    "href": "lectures/week2-wrangling.html#tidy-data-revisited",
    "title": "Data Wrangling in R",
    "section": "Tidy data revisited",
    "text": "Tidy data revisited\n\n“Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.” (Wickham, 2014)\n\n\n\nThese guidelines aren’t arbitrary.\nA standard tidy structure is helpful for visualizing and analyzing data.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn fact, I often conceptualize “tidy” data in terms of:\n\nWhat are the axes in a plot I want to make? (These should be separate columns.)\nWhat are the terms in a regression formula I want to write? (These should be separate columns.)"
  },
  {
    "objectID": "lectures/week2-wrangling.html#welcome-to-the-tidyverse",
    "href": "lectures/week2-wrangling.html#welcome-to-the-tidyverse",
    "title": "Data Wrangling in R",
    "section": "Welcome to the tidyverse",
    "text": "Welcome to the tidyverse\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.”\n\n\n\nFundamentally, the tidyverse rests on the philosophy of tidy data.\nIncludes a range of functions, from importing data (read_csv) to merging datasets (inner_join).\n\n\n\n\n\n\n\n\n\nNote💭 Installation Help\n\n\n\n\nInstall the tidyverse using install.packages(\"tidyverse\").\nThen, load it using library(tidyverse)."
  },
  {
    "objectID": "lectures/week2-wrangling.html#loading-the-tidyverse",
    "href": "lectures/week2-wrangling.html#loading-the-tidyverse",
    "title": "Data Wrangling in R",
    "section": "Loading the tidyverse",
    "text": "Loading the tidyverse\n\n### Loading the tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntidyverse::tidyverse_packages()\n\n [1] \"broom\"         \"conflicted\"    \"cli\"           \"dbplyr\"       \n [5] \"dplyr\"         \"dtplyr\"        \"forcats\"       \"ggplot2\"      \n [9] \"googledrive\"   \"googlesheets4\" \"haven\"         \"hms\"          \n[13] \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n[17] \"modelr\"        \"pillar\"        \"purrr\"         \"ragg\"         \n[21] \"readr\"         \"readxl\"        \"reprex\"        \"rlang\"        \n[25] \"rstudioapi\"    \"rvest\"         \"stringr\"       \"tibble\"       \n[29] \"tidyr\"         \"xml2\"          \"tidyverse\"    \n\n\n\n\nWe’ll cover some of these packages later in this class, like broom and ggplot2.\nThis week, we’ll focus on functions from readr, magrittr, dplyr, and tidyr."
  },
  {
    "objectID": "lectures/week2-wrangling.html#the-pipe",
    "href": "lectures/week2-wrangling.html#the-pipe",
    "title": "Data Wrangling in R",
    "section": "The pipe (%>%)",
    "text": "The pipe (%&gt;%)\n\nThe pipe operator (%&gt;%) lets you chain functions together in a readable way.\n\n\n\nTakes the output from the left side and feeds it as the first argument to the right side.\nInstead of nesting functions, can pipe outputs into each other.\nMakes code more readable and easier to follow.\nWe’ll see examples coming up!"
  },
  {
    "objectID": "lectures/week2-wrangling.html#part-1-basic-data-manipulation",
    "href": "lectures/week2-wrangling.html#part-1-basic-data-manipulation",
    "title": "Data Wrangling in R",
    "section": "Part 1: Basic data manipulation",
    "text": "Part 1: Basic data manipulation\nImporting, filtering, mutating, and summarizing data."
  },
  {
    "objectID": "lectures/week2-wrangling.html#reading-in-a-dataset",
    "href": "lectures/week2-wrangling.html#reading-in-a-dataset",
    "title": "Data Wrangling in R",
    "section": "Reading in a dataset",
    "text": "Reading in a dataset\nOne of the most fundamental things you’ll need to do is import data, e.g., from a .csv file.\n\nread_csv takes as input a filepath (either locally, or a URL).\nOutput is a tibble.\n\n\ndf_conc &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/wrangling/concreteness.csv\")\n\nRows: 28612 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Word, Dom_Pos\ndbl (2): Concreteness, Frequency\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnrow(df_conc)\n\n[1] 28612\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nUse of the functions we’ve discussed (nrow, names, head, etc.) to explore the dataset."
  },
  {
    "objectID": "lectures/week2-wrangling.html#basics-of-dplyr",
    "href": "lectures/week2-wrangling.html#basics-of-dplyr",
    "title": "Data Wrangling in R",
    "section": "Basics of dplyr",
    "text": "Basics of dplyr\n\ndplyr is a package in the tidyverse that contains functions for transforming and manipulating dataframes.\n\nEach function is a “verb”, for instance:\n\n\nfilter relevant rows.\nmutate the dataset by adding new variables.\npull specific columns by name.\n\nrename specific columns.\ngroup_by specific values or factors.\nsummarize dataset with summary statistics."
  },
  {
    "objectID": "lectures/week2-wrangling.html#the-filter",
    "href": "lectures/week2-wrangling.html#the-filter",
    "title": "Data Wrangling in R",
    "section": "The filter",
    "text": "The filter\n\nfilter works by selecting a subset of rows based on some boolean condition.\n\n\ndf_conc %&gt;%  ### piping!\n  filter(Word == \"class\")\n\n# A tibble: 1 × 4\n  Word  Concreteness Frequency Dom_Pos\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  \n1 class         3.85      5985 Noun   \n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nTry using filter to return words that:\n\nHave a Concreteness &gt; 4.5.\nHave a Dom_Pos of Noun.\n\nHint: You can chain multiple filter statements together with a %&gt;% operator."
  },
  {
    "objectID": "lectures/week2-wrangling.html#mutate-your-data",
    "href": "lectures/week2-wrangling.html#mutate-your-data",
    "title": "Data Wrangling in R",
    "section": "mutate your data",
    "text": "mutate your data\n\nmutate works by creating a new variable (or overwriting an existing one) via applying some transformation to an existing variable or set of variables.\n\nLet’s create a log frequency variable, since Frequency is highly right-skewed.\n\ndf_conc = df_conc %&gt;%  ### piping!\n  mutate(log_freq = log10(Frequency))\n\nsummary(df_conc$log_freq)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   -Inf  0.8451  1.4624    -Inf  2.1004  6.3293 \n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWhy do you think the mininum log_freq value is -Inf?"
  },
  {
    "objectID": "lectures/week2-wrangling.html#mutate-pt.-2",
    "href": "lectures/week2-wrangling.html#mutate-pt.-2",
    "title": "Data Wrangling in R",
    "section": "mutate (pt. 2)",
    "text": "mutate (pt. 2)\nSome words in our dataset have a Frequency of 0. One strategy is to add one to those values before taking the log.\n\ndf_conc = df_conc %&gt;%  ### piping!\n  mutate(log_freq = log10(Frequency + 1))\n\nsummary(df_conc$log_freq)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.9031  1.4771  1.5793  2.1038  6.3293"
  },
  {
    "objectID": "lectures/week2-wrangling.html#group_by-specific-factors-and-summarise",
    "href": "lectures/week2-wrangling.html#group_by-specific-factors-and-summarise",
    "title": "Data Wrangling in R",
    "section": "group_by specific factors and summarise",
    "text": "group_by specific factors and summarise\n\nThe group_by function is extremely useful for grouping your data by specific factors.\nYou can %&gt;% this into summarise to calculate summary statistics for each group.\n\n\ndf_conc %&gt;%\n  group_by(Dom_Pos) %&gt;%\n  summarise(mean_conc = mean(Concreteness)) %&gt;%\n  head(2)\n\n# A tibble: 2 × 2\n  Dom_Pos   mean_conc\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Adjective      2.50\n2 Adverb         2.06\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nIn addition to calculating mean, try also calculating the sd; additionally, try calculating the mean and sd for log_freq by Dom_Pos."
  },
  {
    "objectID": "lectures/week2-wrangling.html#counting-the-number-of-observations",
    "href": "lectures/week2-wrangling.html#counting-the-number-of-observations",
    "title": "Data Wrangling in R",
    "section": "Counting the number of observations",
    "text": "Counting the number of observations\nOften relevant for a group_by calculation is the number of observations per group.\n\ndf_conc %&gt;%\n  group_by(Dom_Pos) %&gt;%\n  summarise(mean_conc = mean(Concreteness),\n            count = n()) %&gt;%\n  head(2)\n\n# A tibble: 2 × 3\n  Dom_Pos   mean_conc count\n  &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Adjective      2.50  6112\n2 Adverb         2.06  1876"
  },
  {
    "objectID": "lectures/week2-wrangling.html#hands-on-practice",
    "href": "lectures/week2-wrangling.html#hands-on-practice",
    "title": "Data Wrangling in R",
    "section": "Hands-on practice",
    "text": "Hands-on practice\nNow let’s get some hands-on practice applying these new techniques.\n\n\nRead in the pokemon.csv dataset from the ucsd_211_datasets GitHub using read_csv.\nUse group_by to count the number of Pokemon of each Type 1. Which Type 1 is most frequent?\nThen, filter the dataset to only include Pokemon of the most frequent type (based on what you learned).\nOf this filtered dataset, group_by legendary status and calculate the mean(Attack)"
  },
  {
    "objectID": "lectures/week2-wrangling.html#hands-on-practice-solution",
    "href": "lectures/week2-wrangling.html#hands-on-practice-solution",
    "title": "Data Wrangling in R",
    "section": "Hands-on practice (solution)",
    "text": "Hands-on practice (solution)\n\n### Part 1\ndf_pokemon &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/wrangling/pokemon.csv\")\n\nRows: 800 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Name, Type 1, Type 2\ndbl (9): #, Total, HP, Attack, Defense, Sp. Atk, Sp. Def, Speed, Generation\nlgl (1): Legendary\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n### Part 2\nmax_type = df_pokemon %&gt;% \n  group_by(`Type 1`) %&gt;% \n  summarise(count = n()) %&gt;%\n  slice_max(count) %&gt;%\n  pull(`Type 1`)\n\n### Parts 3-4\ndf_pokemon %&gt;%\n  filter(`Type 1` == max_type) %&gt;%\n  group_by(Legendary) %&gt;%\n  summarise(mean_attack = mean(Attack))\n\n# A tibble: 2 × 2\n  Legendary mean_attack\n  &lt;lgl&gt;           &lt;dbl&gt;\n1 FALSE            72.8\n2 TRUE            111. \n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nNotice any functions we haven’t discussed?"
  },
  {
    "objectID": "lectures/week2-wrangling.html#some-new-functions",
    "href": "lectures/week2-wrangling.html#some-new-functions",
    "title": "Data Wrangling in R",
    "section": "Some new functions",
    "text": "Some new functions\nAbove, we used a combination of read_csv, filter, group_by, and summarise. But we also used some other new functions:\n\n\nslice_max: returns rows with the max values of a given column.\n\nCan also use slice_head, slice_tail, slice_min, and slice_sample.\n\npull: returns the individual vector/value from a given column.\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nAny questions before we move on to more dplyr functions and more aspects of the tidyverse more generally?"
  },
  {
    "objectID": "lectures/week2-wrangling.html#select-specific-columns",
    "href": "lectures/week2-wrangling.html#select-specific-columns",
    "title": "Data Wrangling in R",
    "section": "select specific columns",
    "text": "select specific columns\n\nThe select function can be used to select certain columns for further analysis.\n\nThis is especially useful if you want to %&gt;% those columns into, say, a correlation matrix.\n\ndf_conc %&gt;%\n  select(log_freq, Concreteness, Frequency) %&gt;%\n  cor()\n\n              log_freq Concreteness   Frequency\nlog_freq     1.0000000   0.16983740  0.21158008\nConcreteness 0.1698374   1.00000000 -0.01501261\nFrequency    0.2115801  -0.01501261  1.00000000\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nTry building a correlation matrix of the numerical columns from the Pokemon dataset (e.g., Attack, Defense, etc.). Are there any conclusions you can draw?"
  },
  {
    "objectID": "lectures/week2-wrangling.html#part-2-more-complex-wrangling",
    "href": "lectures/week2-wrangling.html#part-2-more-complex-wrangling",
    "title": "Data Wrangling in R",
    "section": "Part 2: More complex wrangling",
    "text": "Part 2: More complex wrangling\nMissing data, reshaping, and joining."
  },
  {
    "objectID": "lectures/week2-wrangling.html#missing-values",
    "href": "lectures/week2-wrangling.html#missing-values",
    "title": "Data Wrangling in R",
    "section": "Missing values",
    "text": "Missing values\n\nReal-world data often has missing values, which are treated as NA (“Not Available”) by R.\n\nThere are a few relevant issues here:\n\n\nFirst, you need to identify the presence of missing values.\nSecond, you need to address the issue.\n\nIgnore them?\nRemove any rows with missing values?\nTry to impute their values?"
  },
  {
    "objectID": "lectures/week2-wrangling.html#identifying-missing-values",
    "href": "lectures/week2-wrangling.html#identifying-missing-values",
    "title": "Data Wrangling in R",
    "section": "Identifying missing values",
    "text": "Identifying missing values\nWe can use the is.na function to check for missing values.\n\n### Reading in dataset\ndf_titanic &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/wrangling/titanic.csv\")\n\nRows: 891 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Name, Sex, Ticket, Cabin, Embarked\ndbl (7): PassengerId, Survived, Pclass, Age, SibSp, Parch, Fare\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n### Check for missing values; returns a vector of boolean values\nis.na(df_titanic$Age)[0:5]\n\n[1] FALSE FALSE FALSE FALSE FALSE\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nHow many missing values are there total in the Age column? What about Survived or Cabin? (Hint: You can sum boolean values like TRUE and FALSE)."
  },
  {
    "objectID": "lectures/week2-wrangling.html#identify-missing-values-pt.-2",
    "href": "lectures/week2-wrangling.html#identify-missing-values-pt.-2",
    "title": "Data Wrangling in R",
    "section": "Identify missing values (pt. 2)",
    "text": "Identify missing values (pt. 2)\nWe can combine some handy functions (across and everything) to sum up the number of NA values per column.\n\n## Get number of NA across all columns\ndf_titanic %&gt;% \n  summarise(across(everything(), ~ sum(is.na(.))))\n\n# A tibble: 1 × 12\n  PassengerId Survived Pclass  Name   Sex   Age SibSp Parch Ticket  Fare Cabin\n        &lt;int&gt;    &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1           0        0      0     0     0   177     0     0      0     0   687\n# ℹ 1 more variable: Embarked &lt;int&gt;\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNA values seem disproportionately located in the Cabin column."
  },
  {
    "objectID": "lectures/week2-wrangling.html#addressing-missing-values",
    "href": "lectures/week2-wrangling.html#addressing-missing-values",
    "title": "Data Wrangling in R",
    "section": "Addressing missing values",
    "text": "Addressing missing values\nNow that we’ve identified missing values, we have a few options:\n\n\nWe could ignore them, i.e., perform analyses as if they don’t exist.\nWe could remove missing values from our dataset.\nWe could impute the missing values (guess).\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWhat do you think are the trade-offs of each approach?"
  },
  {
    "objectID": "lectures/week2-wrangling.html#the-problem-with-ignoring-na",
    "href": "lectures/week2-wrangling.html#the-problem-with-ignoring-na",
    "title": "Data Wrangling in R",
    "section": "The problem with ignoring NA",
    "text": "The problem with ignoring NA\nUnsurprisingly, ignoring NA values doesn’t mean they go away.\n\nIn some cases, analyses or visualizations may “quietly” ignore them.\n\nThis is bad because it means you’re not fully aware of what goes into an analysis.\n\nIn other cases, it’ll interfere with your summary statistics.\n\n\n## Mean defaults to NA b/c of missing values\nmean(df_titanic$Age)\n\n[1] NA\n\n## Temporary fix is to remove them from this calculation...\nmean(df_titanic$Age, na.rm = TRUE)\n\n[1] 29.69912"
  },
  {
    "objectID": "lectures/week2-wrangling.html#removing-missing-values",
    "href": "lectures/week2-wrangling.html#removing-missing-values",
    "title": "Data Wrangling in R",
    "section": "Removing missing values",
    "text": "Removing missing values\nAnother common approach to NA values is simply to remove them.\nWe can do this selectively in a given analysis, like below:\n\n## Temporary fix is to remove them from this calculation...\nmean(df_titanic$Age, na.rm = TRUE)\n\n[1] 29.69912\n\n\nOr we can actually filter our dataset to remove them entirely (or use drop_na).\n\ndf_titanic_subset = df_titanic %&gt;%\n  filter(!is.na(Age))\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nDo you think we should always just remove all NA values? Why or why not?"
  },
  {
    "objectID": "lectures/week2-wrangling.html#not-all-na-values-matter-equally",
    "href": "lectures/week2-wrangling.html#not-all-na-values-matter-equally",
    "title": "Data Wrangling in R",
    "section": "Not all NA values matter equally",
    "text": "Not all NA values matter equally\nWhether or not we care that something is NA depends on our research questions!\n\nAge might be relevant for analyses of Survived.\nCabin may not really matter.\n\n\n### Cabin mostly NA, but maybe that's fine?\nsum(is.na(df_titanic$Cabin))\n\n[1] 687\n\ndf_titanic$Cabin[1:10]\n\n [1] NA     \"C85\"  NA     \"C123\" NA     NA     \"E46\"  NA     NA     NA"
  },
  {
    "objectID": "lectures/week2-wrangling.html#how-do-na-values-distribute",
    "href": "lectures/week2-wrangling.html#how-do-na-values-distribute",
    "title": "Data Wrangling in R",
    "section": "How do NA values distribute?",
    "text": "How do NA values distribute?\nOne factor that might affect our approach is how NA values distribute.\n\nAre they distributed randomly with respect to some other variable (e.g., Survived)?\nOr are they systematic?\nEither way, it’s helpful to know.\n\n\n### Lower NA information about Cabins for people who survived\ndf_titanic %&gt;%\n  group_by(Survived) %&gt;%\n  summarise(proportion_na = mean(is.na(Cabin)))\n\n# A tibble: 2 × 2\n  Survived proportion_na\n     &lt;dbl&gt;         &lt;dbl&gt;\n1        0         0.876\n2        1         0.602\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWhat is the relative rate of NA values across levels of Survived for Age?"
  },
  {
    "objectID": "lectures/week2-wrangling.html#imputing-missing-values",
    "href": "lectures/week2-wrangling.html#imputing-missing-values",
    "title": "Data Wrangling in R",
    "section": "Imputing missing values",
    "text": "Imputing missing values\n\nImputing values means inferring them based on some other set of properties or set of assumption. I.e., an “educated guess”.\n\n\nCan set to mean or median value (e.g., mean(Age)).\nCan “guess” value based on some other property (e.g., mean(Age) for that Pclass).\n\n\ndf_titanic = df_titanic %&gt;%\n  group_by(Pclass) %&gt;% ### Group by passenger class\n  mutate(Age_imputed = ifelse(\n    is.na(Age), mean(Age, na.rm = TRUE),  ### Replace with mean for that Pclass\n    Age)) %&gt;% ### otherwise just use original age\n  ungroup()\n\n\ndf_titanic %&gt;%\n  group_by(Pclass) %&gt;%\n  summarise(mean_age = mean(Age_imputed))\n\n# A tibble: 3 × 2\n  Pclass mean_age\n   &lt;dbl&gt;    &lt;dbl&gt;\n1      1     38.2\n2      2     29.9\n3      3     25.1"
  },
  {
    "objectID": "lectures/week2-wrangling.html#missing-values-wrap-up",
    "href": "lectures/week2-wrangling.html#missing-values-wrap-up",
    "title": "Data Wrangling in R",
    "section": "Missing values: wrap-up",
    "text": "Missing values: wrap-up\n\n\nReal-world data often has missing values.\nAt minimum, it’s important to identify and characterize NA values in your dataset.\nIdeally, you should develop a strategy for dealing with them with intentionality.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSometimes missing values show up when we join datasets. We’ll discuss joining soon."
  },
  {
    "objectID": "lectures/week2-wrangling.html#reshaping-data",
    "href": "lectures/week2-wrangling.html#reshaping-data",
    "title": "Data Wrangling in R",
    "section": "Reshaping data",
    "text": "Reshaping data\n\nReshaping data means converting it to either a “longer” format (from a “wide” format) or the other way around.\n\n\n\nIn many cases, this means making data tidy (long format).\nThe tidyr package contains key functions for this, such as pivot_longer and pivot_wider.\ntidyr also includes functions to separate information from a single column across multiple columns (or unite, to do the opposite.)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe best way to understand this is to look at some examples!"
  },
  {
    "objectID": "lectures/week2-wrangling.html#pivot_longer-from-wide-to-long",
    "href": "lectures/week2-wrangling.html#pivot_longer-from-wide-to-long",
    "title": "Data Wrangling in R",
    "section": "pivot_longer: from wide to long",
    "text": "pivot_longer: from wide to long\nLet’s start by creating a wide dataset, which might be considered “messy”.\n\ndf_messy_1 = data.frame(john = c(10, 11),\n                        mary = c(20, 25),\n                        jane = c(5, 10),\n                        treatment = c('a', 'b'))\ndf_messy_1\n\n  john mary jane treatment\n1   10   20    5         a\n2   11   25   10         b\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWhy is this considered “messy”? And what would a “long” version look like?"
  },
  {
    "objectID": "lectures/week2-wrangling.html#pivot_longer-from-wide-to-long-1",
    "href": "lectures/week2-wrangling.html#pivot_longer-from-wide-to-long-1",
    "title": "Data Wrangling in R",
    "section": "pivot_longer: from wide to long",
    "text": "pivot_longer: from wide to long\nWe can use the pivot_longer function to transform this into a long format.\n\ndf_messy_1 %&gt;%\n  pivot_longer(cols = c(john, mary, jane), ### which columns to pivot\n               names_to = 'name', ### what to call column with keys\n               values_to = 'result') ### what to call column with values\n\n# A tibble: 6 × 3\n  treatment name  result\n  &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;\n1 a         john      10\n2 a         mary      20\n3 a         jane       5\n4 b         john      11\n5 b         mary      25\n6 b         jane      10\n\n\n\n\nCrucial argument is cols (which columns to pivot).\nnames_to and values_to are helpful for making the resulting table more readable."
  },
  {
    "objectID": "lectures/week2-wrangling.html#pivot_longer-from-wide-to-long-2",
    "href": "lectures/week2-wrangling.html#pivot_longer-from-wide-to-long-2",
    "title": "Data Wrangling in R",
    "section": "pivot_longer: from wide to long",
    "text": "pivot_longer: from wide to long\nLet’s look at another example.\n\ndf_messy_2 = data.frame(name = c('john', 'mary', 'jane'),\n                        a = c(10, 20, 5),\n                        b = c(11, 25, 10))\n\ndf_messy_2\n\n  name  a  b\n1 john 10 11\n2 mary 20 25\n3 jane  5 10\n\ndf_tidy = df_messy_2 %&gt;%\n  pivot_longer(cols = c(a, b),\n               names_to = \"treatment\",\n                values_to = \"result\")\n\ndf_tidy\n\n# A tibble: 6 × 3\n  name  treatment result\n  &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n1 john  a             10\n2 john  b             11\n3 mary  a             20\n4 mary  b             25\n5 jane  a              5\n6 jane  b             10"
  },
  {
    "objectID": "lectures/week2-wrangling.html#pivot_wider-back-to-wide-format",
    "href": "lectures/week2-wrangling.html#pivot_wider-back-to-wide-format",
    "title": "Data Wrangling in R",
    "section": "pivot_wider: back to wide format!",
    "text": "pivot_wider: back to wide format!\n\npivot_wider reshapes data into wide format.\n\n\n### To recreate first table\ndf_tidy %&gt;%\n  pivot_wider(names_from = \"name\",\n              values_from = \"result\")\n\n# A tibble: 2 × 4\n  treatment  john  mary  jane\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 a            10    20     5\n2 b            11    25    10\n\n### To recreate second table\ndf_tidy %&gt;%\n  pivot_wider(names_from = \"treatment\",\n              values_from = \"result\")\n\n# A tibble: 3 × 3\n  name      a     b\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 john     10    11\n2 mary     20    25\n3 jane      5    10"
  },
  {
    "objectID": "lectures/week2-wrangling.html#your-turn-pt.-1",
    "href": "lectures/week2-wrangling.html#your-turn-pt.-1",
    "title": "Data Wrangling in R",
    "section": "Your turn! (pt. 1)",
    "text": "Your turn! (pt. 1)\nIf you’ve loaded tidyverse, you should have access to (at least) the following data tables:\n\ntable1\ntable2\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nHow would you pivot_wider table2 such that it resembles table1?"
  },
  {
    "objectID": "lectures/week2-wrangling.html#your-turn-pt.-2",
    "href": "lectures/week2-wrangling.html#your-turn-pt.-2",
    "title": "Data Wrangling in R",
    "section": "Your turn! (pt. 2)",
    "text": "Your turn! (pt. 2)\nNow let’s try pivot_longer with a more complex example. Load the missing work example from this Full Stack Economics graph.\n\n### Part 1\ndf_work &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/wrangling/missing_work.csv\")\n\nRows: 110 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): Year, Child care problems, Maternity or paternity leave, Other fami...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(df_work, 3)\n\n# A tibble: 3 × 7\n   Year `Child care problems` Maternity or paternity le…¹ Other family or pers…²\n  &lt;dbl&gt;                 &lt;dbl&gt;                       &lt;dbl&gt;                  &lt;dbl&gt;\n1  2012                    18                         313                    246\n2  2012                    35                         278                    230\n3  2012                    13                         245                    246\n# ℹ abbreviated names: ¹​`Maternity or paternity leave`,\n#   ²​`Other family or personal obligations`\n# ℹ 3 more variables: `Illness or injury` &lt;dbl&gt;, Vacation &lt;dbl&gt;, Month &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nRight now, the table stores different reasons for missing work as different columns.\nInstead, let’s pivot_longer so the table has a column for Reason (for missing) and a column for Count (number of days missed)."
  },
  {
    "objectID": "lectures/week2-wrangling.html#other-handy-tidyr-functions",
    "href": "lectures/week2-wrangling.html#other-handy-tidyr-functions",
    "title": "Data Wrangling in R",
    "section": "Other handy tidyr functions",
    "text": "Other handy tidyr functions\nAlthough pivoting is probably the main function enabled by tidyr, it also contains other valuable reshaping functions:\n\n\nseparate “splits” values within a given column into multiple columns.\nunite does the opposite.\n\n\n\ntable3 %&gt;%\n  separate(rate,  ### column to separate\n           into = c(\"cases\", \"population\"),  ### columns to split into\n           convert = TRUE, ### convert into numeric\n           sep = \"/\") ### separator currently separating values\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583"
  },
  {
    "objectID": "lectures/week2-wrangling.html#joining-data",
    "href": "lectures/week2-wrangling.html#joining-data",
    "title": "Data Wrangling in R",
    "section": "Joining data",
    "text": "Joining data\n\nJoining is an operation in which columns from data Y are added to data X, matching observations based on shared variables (or “keys”).\n\n\n\nIn real-world analysis, data is often stored across multiple sources.\nJoining brings together related information from separate tables.\nKey types: left_join(), right_join(), inner_join(), full_join()\n\n\n\nExample scenario:\n\nTable 1: State election results (state, biden_pct, turnout)\n\nTable 2: State demographics (state, median_income, college_pct)\n\nHere, we might join the tables based on the overlapping state variable.\nGoal: Analyze how median_income and college_pct relates to voting patterns."
  },
  {
    "objectID": "lectures/week2-wrangling.html#types-of-joins",
    "href": "lectures/week2-wrangling.html#types-of-joins",
    "title": "Data Wrangling in R",
    "section": "Types of joins",
    "text": "Types of joins\nGiven tables X and Y, you have several options for how to join them.\n\n\ninner_join: only keep observations from X with matching key in Y.\nleft_join: keep all observations from X, join matching ones from Y.\nright-join: keep all observations from Y, join matching ones from X.\nfull_join: keep all observations in both X and Y."
  },
  {
    "objectID": "lectures/week2-wrangling.html#simple-join-example",
    "href": "lectures/week2-wrangling.html#simple-join-example",
    "title": "Data Wrangling in R",
    "section": "Simple join example",
    "text": "Simple join example\nSuppose we have two tables: table4a contains information about the cases in each country (with different columns for different years), and table4b contains information about the population of each country (again, with different columns for different years).\nIdeally, we’d like to join these tables.\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nWhat steps will we need to take to join them?\n(Hint: It may involve more than joining.)"
  },
  {
    "objectID": "lectures/week2-wrangling.html#simple-join-example-1",
    "href": "lectures/week2-wrangling.html#simple-join-example-1",
    "title": "Data Wrangling in R",
    "section": "Simple join example",
    "text": "Simple join example\n\n### First, tidy each table\ntidy4a = table4a %&gt;%\n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\ntidy4b = table4b %&gt;%\n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"population\")\n\n### Then, join\nleft_join(tidy4a, tidy4b)\n\nJoining with `by = join_by(country, year)`\n\n\n# A tibble: 6 × 4\n  country     year   cases population\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan 1999     745   19987071\n2 Afghanistan 2000    2666   20595360\n3 Brazil      1999   37737  172006362\n4 Brazil      2000   80488  174504898\n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583\n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nIn this case, would it matter whether we use left_join or inner_join? If not, under what circumstances would this distinction matter?"
  },
  {
    "objectID": "lectures/week2-wrangling.html#a-more-complex-example",
    "href": "lectures/week2-wrangling.html#a-more-complex-example",
    "title": "Data Wrangling in R",
    "section": "A more complex example",
    "text": "A more complex example\nWe’ve already loaded information about word concreteness. Now let’s load another dataset about the average age of acquisition at which words are learned, and combine that with the concreteness dataset.\n\ndf_aoa &lt;- read_csv(\"https://raw.githubusercontent.com/seantrott/ucsd_css211_datasets/main/main/wrangling/AoA.csv\")\n\nRows: 31124 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Word\ndbl (1): AoA\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnrow(df_aoa)\n\n[1] 31124\n\nhead(df_aoa)\n\n# A tibble: 6 × 2\n  Word        AoA\n  &lt;chr&gt;     &lt;dbl&gt;\n1 a          2.89\n2 aardvark   9.89\n3 abacus     8.69\n4 abalone   12.2 \n5 abandon    8.32\n6 abandoner 11.9 \n\n\n\n\n\n\n\n\n\nNote💭 Check-in\n\n\n\nHow would you join this dataset with the concreteness dataset? What kind of join operation might you want to use?"
  },
  {
    "objectID": "lectures/week2-wrangling.html#a-more-complex-example-pt.-2",
    "href": "lectures/week2-wrangling.html#a-more-complex-example-pt.-2",
    "title": "Data Wrangling in R",
    "section": "A more complex example (pt. 2)",
    "text": "A more complex example (pt. 2)\nThe concreteness and AoA datasets are not fully-overlapping. Concreteness contains words not in AoA and vice versa.\n\n### Inner join (keep only intersection)\ndf_inner = df_conc %&gt;%\n  inner_join(df_aoa)\n\nJoining with `by = join_by(Word)`\n\nnrow(df_inner)\n\n[1] 23568\n\n### Left join (keep all concreteness)\ndf_left = df_conc %&gt;%\n  left_join(df_aoa)\n\nJoining with `by = join_by(Word)`\n\nnrow(df_left)\n\n[1] 28612\n\n### Right join (keep all AoA)\ndf_right = df_conc %&gt;%\n  right_join(df_aoa)\n\nJoining with `by = join_by(Word)`\n\nnrow(df_right)\n\n[1] 31124\n\n### full join (keep everything)\ndf_full = df_conc %&gt;%\n  full_join(df_aoa)\n\nJoining with `by = join_by(Word)`\n\nnrow(df_full)\n\n[1] 36168"
  },
  {
    "objectID": "lectures/week2-wrangling.html#joining-the-details",
    "href": "lectures/week2-wrangling.html#joining-the-details",
    "title": "Data Wrangling in R",
    "section": "Joining: the details",
    "text": "Joining: the details\nWhen joining, there are a few other details we’ve glossed over:\n\nby: this specifies which keys to join on (will default to all overlapping).\nmultiple: how to handle cases where rows from X match multiple rows from Y.\nunmatched: how to handle cases where rows would be dropped."
  },
  {
    "objectID": "lectures/week2-wrangling.html#putting-it-all-together",
    "href": "lectures/week2-wrangling.html#putting-it-all-together",
    "title": "Data Wrangling in R",
    "section": "Putting it all together",
    "text": "Putting it all together\nThese lectures have covered the basics of data wrangling with the tidyverse.\nA typical workflow will involve:\n\n\nReading data (read_csv).\nExploring the data (is.na, summary, summarise).\nClean data (mutate, filter).\nReshape if needed (pivot_longer/wider).\nJoin if needed (inner_join, etc.).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe end result is a dataset ready for further analysis and visualization (coming up!)."
  }
]